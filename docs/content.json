{"pages":[{"title":"お探しのページは見つかりませんでした。","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/404.html"},{"title":"hexo-oembed demo page","text":"AboutThis is hexo-oembed plugin demo page. https://github.com/hinastory/hexo-oembed 1. oEmbed derivery compatible site demo1.1. YouTubeHexo tag1{% oembed https://www.youtube.com/watch?v=SX_ViT4Ra7k %} 1.2. TwitterHexo tag1{% oembed https://twitter.com/hinastory999/status/1089514744174632960 %} Kubernetes Meetup Tokyo #15 - KubeCon 2018 Recap に初参加 - cats cats cats https://t.co/O3ScDbSriz— hinastory (@hinastory999) January 27, 2019 1.3. Speaker DeckHexo tag1{% oembed https://speakerdeck.com/ladicle/recap-kubecon-plus-cloud-nativecon-north-america-2018-overview %} 1.4. SlideShareHexo tag1{% oembed https://www.slideshare.net/Odersky/preparing-for-scala-3 %} Preparing for Scala 3 from Martin Odersky 1.5. VimeoHexo tag1{% oembed https://vimeo.com/311121738 %} 1.6. TEDHexo tag1{% oembed https://www.ted.com/talks/susan_etlinger_what_do_we_do_with_all_this_big_data %} 1.7. Hatena BlogHexo tag1{% oembed https://rheb.hatenablog.com/entry/rhel8-python %} 2. oEmbed compatible (not support derivery) site demo2.1. flickrHexo tag1{% oembed https://www.flickr.com/photos/blueocean64/23831182193/in/photolist-CiT3cB-8Xbpns-quxdK6-cxB7vf-81BAtk-qC3LMs-bsndSq-81BAtt-wMHSBN-968ZuJ-7QdcHk-db2PFA-nZej15-7eQU7R-81BAti-egYg7p-dXL2hM-eUzjBM-81BAsx-2aVgqBo-4v2775-8DZJiq-aNfwDM-6M32zM-8HfqGw-mW86ZG-8DLjGC-4Yuujs-8Xbpa3-7FwpTe-52Vouq-5FT3Xr-cfXkk7-4YnhUK-9uhCgE-pTVRrq-aNJhs4-rYiKwt-7CjqHW-4jj4cZ-85EEX3-cr4wTW-ns4uE2-WjeQmQ-bJQyaM-fPTxwt-7TH4Dc-cr4wpG-oWgvSG-8X8oux %} endpoint configuration123flickr: match: flickr url: http://www.flickr.com/services/oembed/ 2.2. InstagramHexo tag1{% oembed https://www.instagram.com/p/BCOEogDOmpO/ %} endpoint configuration123instagram: match: instagram url: http://api.instagram.com/oembed/ View this post on Instagram The Famous Registan Square, the Heart of Gorgeous Samarkand City, Uzbekistan Winter 2014 🌙 (PHOTO By @hurshidnarimov ) ° ° ° ° ° #tourbeen_tour_and_adventures #withthebestguide @hurshidnarimov #greattour #friendstravel #travellingthroughtheworld #thetravellingfriends #withthebestguide #bestjourney #art #uzbekart #bestdestinations #uzbekculture #worldheritage #silkroad #friendstravel #travellingthroughtheworld #thetravellingfriends #bestjourney #art #uzbekart #bestdestinations #uzbekculture #worldheritage #silkroad #symbols #architecture #architecturephotography #bestblue #tourbeen_tour_and_adventures #bestphotographer #bestphotogram #bestphoto #gorgeousheritage #heritage #bestlandscape A post shared by 🧿TOURBEE’N VIAGGI🏆 (@tourbeen_silk_road_service) on Feb 25, 2016 at 10:48am PST 2.3. GyazoHexo tag1{% oembed https://gyazo.com/1fe1c370e1e82957b4f10b174fa02fef %} endpoint configuration123gyazo: match: gyazo url: https://api.gyazo.com/api/oembed/ 3. More infomationhexo-oembed related posts(Japanese) HexoのoEmbedプラグインをnpmに公開した話(前編) HexoのoEmbedプラグインをnpmに公開した話(後編) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/hexo-oembed-demo/index.html"},{"title":"about","text":"当サイトについてhinastoryのブログです。興味のある技術を中心に記録していきます。 1. hinastoryについて2. リンクについて3. アクセス解析について4. 商品リンクについて5. 免責事項6. 著作権について7. 匿名で質問を投げる8. お問い合わせ 1. hinastoryについて10の質問風に答えてみます。 好きな言語ベスト5 Scala(バランスがいい) Ruby(書いていてきもちいー) JavaScript(ES2015以降はわりとまともになったので好き) C(システムプログラミングの雄) Haskell(いろいろな発想を学べる) 嫌いな言語ベスト5 機械語/アセンブリ言語(特に命令長が変わるCISCはツライ) C(ポインタやメモリ管理が面倒) C++(ごちゃ混ぜ感がある。テンプレートが吐くエラーで更に嫌いになった) BASIC(GOSUB,GOTOでスパゲッティ)(VBやVBAも好きではない・・・) Python(バージョン非互換がなければ・・・、あと特殊属性や特殊メソッドのアンダースコアも違和感) 5KL以上のソフトウェアを書いたことがある言語 C, Ruby, Scala, JavaScript, C++, C#, BASIC(N88 BASIC, Visual Basic, VBA), (HTML/CSS1) 1KL以上のソフトウェアを書いたことのある言語 Go, Python, lua, Lisp(Common Lisp/Emacs Lisp), Scheme, Java, シェルスクリプト, Haskell, アセンブリ(GAS) 写経したことがある言語 Perl, Tcl, Swift, R, AWK, OCaml, D, Delphi, Elixir, Erlang, TypeScript, Rust, Idris, Julia, Elm, Clojure, PowerShell 好きなエディタ・開発環境ベスト5 Emacs(キーバインドが忘れられない) IntelliJ IDEA(Scalaを書くにはこれ一択。が重い・・・) VisualStudio Code(そこそこ軽くて万能) VisualStudio(重いが機能は優秀) BoostNote(Markdownでメモを取るときのお供) 好きなOSベスト5 MacOS(毎日お世話になっております) iOS(あなたがいないと生活できません) Linux(Linux Mintが好き。もともとはVine Linux派だった・・・) Windows(一応実用的。プログラマーからすれば地獄。セキュリティ周りとか、UACとかどうしてこうなった・・・) 自作OS(昔某書籍の影響でチャレンジした覚えがある。いい思い出になった) 持っているキーボード メイン ErgoDash mini Iris サブ Ergo42 Planck Light 布教用/観賞用 Mint60 お世話になったもの FKB8579 ProgresTouch RETRO TINY 行ったことがある勉強会/カンファレンス/イベント RubyKaigi, 大江戸Ruby会議, ScalaMatsuri, GTC Japan, AWS Summit Tokyo, Developers Summit, Erlang & Elixir Fest, JapanContainerDays, Kubernetes Meetup Tokyo, プログラマのための圏論勉強会, 天下一キーボードわいわい会, 技術書典, Maker Faire Tokyo, コミケ…2 最近の興味、力を入れている分野 クラウドネイティブ, 機械学習, 圏論, 量子コンピューティング、自作キーボード 2. リンクについて当ブログはリンクフリーです。正規に公開されている箇所であればどこにリンクを貼って頂いても構いません。 3. アクセス解析について当ブログでは、アクセス解析ツールとしてGoogle アナリティクスおよびGoogle Search Consoleを利用しています。 これらはトラフィックデータの収集のためにCookieを使用しています。このトラフィックデータは匿名で収集されており、個人を特定するものではありません。この機能はCookieを無効にすることで収集を拒否することが出来ますので、お使いのブラウザの設定をご確認ください。 4. 商品リンクについて当ブログは、商品リンクをクリックすることで本ブログが紹介料を獲得できる手段を提供することを目的としたアフィリエイトサービス(もしもアフィリエイト)を利用しています。商品画像とともに設置された「Amazonで見る」や「楽天市場で見る」といったリンク画像が対象です。 5. 免責事項当ブログの記事は、記事公開時点で正しい情報を記載するように努めております。ただし、この記事が提供する情報、リンク先などによりいかなる損失、被害が発生したとしても責任は負いかねますので、ご了承ください。また、当ブログの記事は予告なく修正、追加、削除されることがあります。 また、本ブログの運営者である「hinastory」の「hinastory」名義の活動は全て個人の趣味としての活動であり、所属する組織および団体とは関係ありません。 6. 著作権について本ブログの著作権は、引用元が他所と分かる文章と画像を除いて、hinastoryが保持しています。本ブログ内のhinastoryが著作権を有する記事と画像(cats cats catのロゴ画像を除く)はCC BY 4.0()で公開されています。 cats cats catsのロゴ画像は本ブログのシンボルですので、本ブログを引用元として明示したい場合にご利用できます。それ以外の用途での利用はご遠慮ください。 7. 匿名で質問を投げる本ブログは記事に関する匿名の質問をマシュマロで受け付けています。 記事に対する質問や指摘があれば、気軽にマシュマロをお投げください3。 8. お問い合わせ本ブログに関するお問い合わせは以下へのメールでお願いします。 1.一応入れましたがプログラミング言語かと言われれば微妙です。 ↩2.ここ2、3年で行ったものです。小さいものは他にもあります。 ↩3.マシュマロの仕様により「ネガティブな内容、性的な内容、スパム等はAIがこっそり削除しちゃうので届きません」ということなので、該当するものは届いていないかもしれません。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/about/index.html"}],"posts":[{"title":"GitHubのプライベートリポジトリに移行した話","text":"新年そうそうビッグニュースが流れてきました。GitHubがプライベートリポジトリをタダで使わせてくれるってよ! ITmedia NEWSGitHub、無料ユーザーもプライベートリポジトリを使い放題にhttp://www.itmedia.co.jp/news/articles/1901/08/news051.htmlGitHubが、無料ユーザーもプライベートリポジトリを無制限に使えるようにした（共有は3人まで）。料金体系は変わらないが、有料プランの名称も少し変わった。 目次1. これまで2. なんでそんな運用にしていたのか？3. 現在3.1. 移行方法3.2. 注意点4. まとめ 1. これまでこれまではパブリックなリポジトリはGitHubにおいて、プライベートなリポジトリは自宅のKubernetes上にGitBucketを立てて、そこに置いていました。 GitHubgitbucket/gitbuckethttps://github.com/gitbucket/gitbucketA Git platform powered by Scala with easy installation, high extensibility & GitHub API compatibility - gitbucket/gitbucket 2. なんでそんな運用にしていたのか？Gitを使っているとなんでもGit管理したくなってきました。しかし、パブリックにおいても良いデータはGitHubで問題ありませんでしたが、どうしてもパグリックにしたくないデータもGit管理しようとすると困りました。GitHubは当時プライベートリポジトリを使うために月$7も必要だったのです。払えないわけではないですが、流石にちょっとしたデータを管理するためには高すぎるなぁと感じて躊躇していました。それではGitLab等のプライベートリポジトリが無料で使える所にしようかとも考えましたが、さすがにパブリックなリポジトリをGitLabに移行すると色々とリンクが切れて面倒なので諦めました。 それではプライベートなリポジトリだけGitLab管理にすればいいかというと.gitconfigに登録してあるメールアドレスの切り替えが面倒でした。GitHub上に公開しているコミットログに実運用しているメールアドレスを残すのは嫌だったのでGitHubのnoreplyメールアドレス(1696779+hinastory＠users.noreply.github.com)を設定していましたが、それをGitLabではそれを切り替える必要があります1。最初はローカルのgitconfigで対応しようとしましたがリポジトリ毎に設定しければならず面倒すぎました。 そして結局、自宅にGitBucketを立てて運用するに至ったわけで、自宅のファイル管理の大体のニーズは満たせました。しかし、それでもやっぱりプライベートでリポジトリを作成して落ち着いてからパブリックにしたいとかはやりたいわけです。そんなこと考えながらここ数年もやもやしていましたが、ここに来てやっと素晴らしいニュースに巡り会いました。 3. 現在GitBucketにあったリポジトリを全てGitHubのプライベートリポジトリに移行しました2。 3.1. 移行方法移行については簡単で、GitHubの画面右上の+ボタンからNew Repositoryを選択するとリポジトリの作成画面が表示されます。そしてその画面でPrivateを選択します。このときInitialize this repository with a READMEを選択しないでください。これを選択すると既存のレポジトリのpushができなくなります。 上記の画面でCreate repositoryを実行すると以下の画面が表示されるので、あとは既存のリポジトリ上で赤枠で囲った手順を実行するだけで移行は完了です。 3.2. 注意点一応注意点を上げるとすれば、プライベートなデータを上げる場合、最低限2段階認証は有効にしておくべきです3。2段階認証用のアプリには自分はGoogle Authenticatorを利用しています。あとFreeプランでは「Protected branches」、「Code ownedrs」、「Pages and wikis」、「Repository insights」の機能がプライベートリポジトリで使えません。プライベートリポジトリのコラボレータも３名までとなっています4。これらが使いたくなったら素直にProプランに移行すべきですね。 4. まとめFreeプランでプライベートリポジトリが使えるようになり、GitHubにリポジトリを統一できて幸せになれました。Gitbucketからの移行も元がGitなので非常に楽です。GitHubがMicrosoftに買収されたときはどうなることやらと思いましたが、この決断はGood Jobと言わざるを得ません。恐らくGitLabへの対抗処置だとは思いますが、今後も競い合ってより良いサービスになってくれることを願って止みません5。 1.プライベートリポジトリだけなら気にしないという手もありますが、やはりGitLabにGitHubのnorelyメールアドレスが入るのは抵抗があるので、切り替える必要がありました・・・ ↩2.ついでにローカルのリポジトリも整理して、GitHubからクローンしたものをお行儀悪く直接修正したりしていたリポジトリを、ちゃんとフォークとリベースをしてGitHubにpushしました。こちらの作業の方が実は大変でした・・・ ↩3.プライベートなデータを上げなくてもセキュリティのために2段階認証はオススメです。 ↩4.パブリックリポジトリでは今までどおりの機能が使えてコラボレータも無制限です。詳細はGitHubの料金プランをご確認ください。 ↩5.いいサービスなら月500円までなら払いますが、$7はちょっと・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/01/13/github/"},{"title":"ようこそキーマップ沼へ","text":"以下のErgodash miniの記事では書ききれなかったキーマップについて紹介します。 サイレントでホットスワップなErgoDash miniを制作(前編) サイレントでホットスワップなErgoDash miniを制作(後編) 目次1. そうだ、キーマップをいじろう！2. QMK Firmware辛い問題3. 僕が考えた最強のキーマップ 1. そうだ、キーマップをいじろう！自作キーボードを始めた理由の一つがキーマップを自由にいじりたいという動機からでした。もともと英語キーボードが好きでしかも「A」の横がCtrlしか許せない派だったので色々と苦労していました。キー配置を自由にできたらと何度夢見たことか・・・ しかし、自作キーボードを手に入れたことで状況は一変しました。QMK Firmwareというものを使えばキーマップを自由にイジり放題だと言うのです。思えばCtrl以外にもいろいろと入れ替えたいキーがあるのでぜひ色々と試してみたいとQMK Firmwareを触りはじめました・・・ツライ・・・ 2. QMK Firmware辛い問題まず、簡単にキーマップを書き換えるために何をすればいいか簡単に説明すると、前述のQMK Firmwareの主にkeymap.cを書き換えて、ビルドと転送を行います。これだけだと簡単に聞こえるかもしれませんが、修正しなければイケナイのはC言語ファイルなんですよね・・・ちょっと記述を間違えると謎のコンパイルエラーに出くわして頭を抱えたりします。あと、最初はQMK Firmware独自の概念に悩まされたりします。レイヤーとかレイヤーとかレイヤーとか・・・ 例えばErgodash miniで光っているLEDを消したいとしましょう。どうすればいいのかというとQMK Firmwareには特殊キーが定義してあってそのキーを押すとシフトキーみたいにキー全体の定義を変更させる機能があります。これがレイヤーと呼ばれていてこのレイヤーは複数定義できるので単純なキー数の何倍もの定義が必要になったりします。Ergodash miniには初期設定でQwertyとRaiseとLowerとAdjustの4つが定義されていて、デフォルトのレイヤーはQwertyになります。話を元に戻すと、LEDを消すためににはAdjustレイヤーの「RGB ON」とコメント書かれているキーを押さないといけないのですが、このレイヤーを出現させるためには何とLowerキーとRaiseキーの同時押しが必要になります。完全に初見殺しですね。しかも、これはQMK Firmware標準機能というわけではなく一部のキーボートで採用されている方式です。keymap.cの後ろの方にさり気なくC言語の関数で実現されています。 もう何というか、何とかして欲しいですね・・・自作キーボードなんだから自分で何とかするのが筋なのは分かっていますが、ちょこっとキーを修正したい場合にいちいちCファイルを修正してビルドと転送を繰り返すのはかなり面倒です。Ergodash miniの場合は自分が見てきたkeymap.cの中では比較的見やすい方でしたが、レイヤーを自分好みにするために全面的に書き換えたのでかなり大変でした・・・一応QMK ConfiguratorというWebベースのツールもあるみたいですが、まだまだ機能が足りていないというかそもそもErgodashのような物理キーの変更はどうするのかとか課題がありそうです。自分はレイアウトの確認のみに利用しています。 3. 僕が考えた最強のキーマップ前置きが長くなってしまって申し訳ございません。以下が「僕が考えた最強のキーマップ」です(笑)。 以下がキーマップのポイントです レイヤーは４つ Qwertyレイヤーは通常配列 Lowerレイヤーはカーソル移動 RaiseレイヤーはNum Padと記号 AdjustレイヤーはBacklight調整とファンクションキー スペース、エンター、バックスペース、シフトを親指で押す 左手親指でレイヤー切り替えて、右手でキー入力 日本語入力切り替えはMacスタイル(英数キーでIMEオフ/かなキーでIMEオン) Adjustレイヤーは同時押しが嫌だったので専用キーを割り当てて、ここにファンクションキーも押し込んでいます。LowerとRaiseの使い方特殊で基本的に左手親指でレイヤー切り替えのキーを押して右手でキーを打つスタイルです。Lowerレイヤーの出し方も特殊でスペースキーの長押しかEnterキーの長押しで出すようにしています。Enterキーにも割り当てた理由は右手だけでカーソル移動ができるようにするためです。そして自作キーボード界隈では親指酷使スタイルです。このスタイルになれると小指の負担が軽減されてキータイプしやすくなったと感じています。 自分は他にもErgo42やIrisやPlanckキーボードも持っていますが、直行配列のキーボードは大体こんな感じのレイアウトで統一しています。以下にキーマップを公開しているので気になる人は参考にしてみてください。 GitHubhinastory/qmk_firmwarehttps://github.com/hinastory/qmk_firmware/blob/ergodash-mini/keyboards/ergodash/mini/keymaps/hinastory/keymap.ckeyboard controller firmware for Atmel AVR and ARM USB families - hinastory/qmk_firmware document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/01/12/keymap/"},{"title":"hexo-oembedを公開しました","text":"HexoにYouTubeなどのサイトを記事に埋め込むためのプラグインを作成して、npmに公開しました。この記事はそのプラグインの紹介記事です。 npmhexo-oembedhttps://www.npmjs.com/package/hexo-oembedembed oEmbed item on your Hexo article. プラグイン制作記事は以下に書きました。興味があればそちらも読んで頂けると嬉しいです。 HexoのoEmbedプラグインをnpmに公開した話(前編) HexoのoEmbedプラグインをnpmに公開した話(後編) 目次1. hexo-oembedについて2. インストール3. 使い方4. デモ4.1. YouTube4.2. Twitter4.3. Speaker Deck5. 設定5.1. className5.2. endpoints5.3. embedlyKey5.4. 設定サンプル6. リポジトリ 1. hexo-oembedについてhexo-oembedはブログ構築ツールHexoのプラグインで、投稿にYouTubeやSlideShareやSpeaker Deckやインスタグラム等さまざまなメディアをパーマリンクを指定するだけで埋め込むためのタグを提供します。埋め込みのための業界標準としてoEmbedというものがありますが、この仕様を利用しているためhexo-oembedという名前になっています。特徴は以下です。 oEmbed Discoveryのサポート YouTube, Vimeo, Twitter, SlideShare, Speaker Deck, CodePen, TED, pixiv等、oEmbed Discoveryに対応している様々なメディアを埋め込むことができます パーマリンクがoEmbed Discoveryに対応しているかどうかは、oEmbed Testerでしらべることができます oEmbedのエンドポイントを設定ファイルで指定可能 oEmbed Discoveryに対応していないサイトでも、oEmbedに対応していればoEmbedのエンドポイントを個別に設定ファイル(_config.yml)に指定することで埋め込めます Instagram, Gyazo, FlickrなどはoEmbedに対応しています エンドポイントはoEmbedのサイトから入手可能です 2. インストールnpmコマンドでインストールしてください。 1$ npm install hexo-oembed --save 3. 使い方埋め込みたいページのパーマリンクを指定するだけです。[maxwidth]と[maxheight]は埋め込みのオブジェクトに期待する最大幅と最大高を数値(ピクセル)で指定します。サイトが対応していればこの値を超えない埋め込みコンテンツを返してくれます。 1{% oembed permlink [maxwidth] [maxheight] %} 4. デモ貼ろうと思えば楽しくていくらでも貼れる感じですが重くなるので3つにしておきます。もっと見たい方は デモページをご覧ください。 4.1. YouTubeちょうどこれを書いているときに3億再生いきました。マトリョシカの頃から応援してたけどまさかここまで人気になるとは・・・ 1{% oembed https://www.youtube.com/watch?v=SX_ViT4Ra7k %} 4.2. Twitter1{% oembed https://twitter.com/hinastory999/status/1089514744174632960 %} Kubernetes Meetup Tokyo #15 - KubeCon 2018 Recap に初参加 - cats cats cats https://t.co/O3ScDbSriz— hinastory (@hinastory999) January 27, 2019 4.3. Speaker Deck1{% oembed https://speakerdeck.com/ladicle/recap-kubecon-plus-cloud-nativecon-north-america-2018-overview %} 5. 設定Hexoの設定ファイル(_config.yml)に以下の設定が可能です。 5.1. className埋め込まれたコンテンツに付与されるCSSクラスのベース名を指定できます。(デフォルト: oembed) 5.2. endpointsoEmbedプロバイダのエンドポイントを指定できます。(デフォルト: なし) oEmbedプロバイダは以下から取得できます。 https://oembed.com/#section7 oEmbedプロバイダのエンドポイントとしてmatchとurlが定義できます。パーマリンクのホスト名がmatchを含んでいた場合、そのエンドポイントが選択され,urlのアドレスでプロバイダに問い合わせます。 もしエンドポイントの定義にマッチしなかった場合は oEmbed Discoveryを利用してエンドポイントを探します。 例えばYouTubeはoEmbed Discoveryに対応しているのでendpointsにYouTubeの定義は必要ありません。 5.3. embedlyKeyもし、embedlyKeyを指定すればEmbed.lyにフォールバックします。 Embed.lyはoEmbedに対応していないサイトのoEmbed情報の提供も行っています。利用したい場合はサインアップして、APIキーの設定してください。 5.4. 設定サンプル_config.yml:12345678910111213oembed: className: oembed embedlyKey: endpoints: instagram: match: instagram url: http://api.instagram.com/oembed/ gyazo: match: gyazo url: https://api.gyazo.com/api/oembed/ flickr: match: flickr url: http://www.flickr.com/services/oembed/ 6. リポジトリ以下のリポジトリで開発しています。バグ報告、ご要望はIssuesへどうぞ。プルリクエストも歓迎です。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/02/07/hexo-oembed/"},{"title":"hexo-tag-detailsを公開しました","text":"HexoにHTML5の detailsタグ1を記事に埋め込むためのプラグインを作成して、npmに公開しました。この記事はそのプラグインの紹介記事です。 npmhexo-tag-detailshttps://www.npmjs.com/package/hexo-tag-detailsHTML5 details tag on your Hexo article 目次1. hexo-tag-detailsについて2. インストール3. 使い方4. 設定4.1. className4.2. open5. リポジトリ6. 最後に 1. hexo-tag-detailsについてhexo-tag-detailsはブログ構築ツールHexoのプラグインで、投稿にHTML5のdetailsタグを埋め込むためのタグを提供します。detailsタグを使うと以下のようなUIを追加できます。 猫は好きですか？ 好き 大好き 愛している 2. インストールnpmコマンドでインストールしてください。 1$ npm install hexo-tag-details --save 3. 使い方要約文と詳細文を書くだけです。ページを表示したしたときに詳細文を開いておく場合には、mode:openを指定します。指定しない場合は詳細文は閉じて表示されます。mode:closeは後で説明する設定でデフォルトを詳細文を開くにした場合に利用します。 123{% details [mode:open/close] summary text %}detail text{% enddetails %} 例1: 123{% details あなたの出身はどこですか? %}私は地球出身です。 水の惑星です!{% enddetails %} 上の例は以下のようなHTMLを生成します。 1234 あなたの出身はどこですか? 私は地球出身です。 水の惑星です! 表示は以下のようになります。 あなたの出身はどこですか?私は地球出身です。 水の惑星です! 例2(mode:openを指定した場合): 12345{% details mode:open あなたの好きな食べ物は何ですか? %}1. お寿司2. 天ぷら3. すき焼き{% enddetails %} 上の例は以下のようなHTMLを生成します。 12345678 あなたの好きな食べ物は何ですか? お寿司 天ぷら すき焼き 表示は以下のようになります。 あなたの好きな食べ物は何ですか? お寿司 天ぷら すき焼き 4. 設定4.1. classNameclassName はCSSクラス名を指定します。 (デフォルト: なし) 4.2. openopenは最初から詳細文を開いておくかどうかを指定します(デフォルト: false) _config.yml123tag_details: className: open: false 5. リポジトリ以下のリポジトリで開発しています。バグ報告、ご要望はIssuesへどうぞ。プルリクエストも歓迎です。 6. 最後にこのタグを作ったきっかけは「10の質問」をこのサイトでやってみようと思ったことです。 10の質問をやろうと思うと質問の解答が質問よりボリュームが多くて全体的に見づらくなることが分かりました。そこで質問の解答部分を折り畳めるようにしようとおもったのですが、HTML5の`details`タグを直接HexoのMarkdownに埋め込むとうまく動作しないことが分かりました。改行を有効にしていたので途中で``が入ってしまったのです。 もしかしたら、すでにdetailsのタグプラグインを作っている人がいるかも知れないと思って探して見ましたが、なかったので自分で作ってしまいました。大したことないタグですが、使って頂ければ幸いです。 1.流石に今どきHTML5のタグをサポートしていないブラウザなんてないと思っていましたが、IEとEdgeはだめなようです・・・もちろん表示されないわけではなく、タグが無視されて表示されるので見れないわけではないです。誰もIEやEdgeなんて使っていないですよね？少なくともこのブログのアクセス解析の結果、賢明な読者様の中にはそういう方はいらっしゃらなかったようなので遠慮なくdetailsタグを使えました。まぁ、IEはすでに非推奨ですしEdgeもChromeベースになるらしいので将来的にはdetailsタグの互換性を気にする必要はなくなるはずです。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/03/09/hexo-tag-details/"},{"title":"技術書典6に一般参加した話","text":"技術書典に一般参加してきました。なにげに第1回から皆勤賞です。天候に恵まれない印象が強いですが、今日はなんとか雨に降られずに済みました。そのおかげもあってか1万人以上の参加者が来場した模様です。 技術書典：技術書オンリーイベント技術書典6https://techbookfest.org/event/tbf06技術書の祭典が2019年04月14日（日）＠池袋サンシャインシティ2F 展示ホールD（文化会館ビル2F）で開催決定！ 目次1. 日時と場所2. 開場前3. いざ戦場へ4. 戦利品5. 感想 1. 日時と場所日時と場所は以下のとおりです。池袋駅から徒歩で10分ほどで到着しました。 日時 2019年4月14日 (日) 11:00〜17:00 場所 池袋サンシャインシティ2F 展示ホールD （文化会館ビル2F） 2. 開場前サンシャインシティの展示ホールには10時少し前に着きましたが、すでにかなり長い待機列ができていました。待っている間に かんたん後払いアプリを導入しておきます。導入時に注意しておくべきことは認証用のSMSの確認コードが「海外の電話番号」から届くことです。自分は海外の電話番号のSMSを拒否する設定になっていたので通知が届かず少し焦りましたが、なんとかこのページを見て問題は解決しました。 後払いアプリは以下のような感じで、「購入する」ボタンを押すとカメラが立ち上がります。そしてブースに表示されているQRコードを読み込むと物販の選択肢が表示されて購入ボタンを押すと購入ができます。購入後に表示された画面を販売者に見せると購入した物が渡されます。右側のキャプチャ画像は購入したもの一覧です。これは非常に便利で以前にやらかした2度買いを防げそうです・・・ このようにキャッシュレスは非常に楽でいい感じです。これがコミケだと常に1000円札や500円玉の残りを意識しなければいけないので、購入者も販売者も面倒くさいことこの上ないです1。ただ一つの難点は想定以上に買いすぎてしまうことです。なので、買った合計金額の表示と予算設定ができるようになるともっと使いやすくなるのではと思いました。 また、今回から「一般参加は11時〜13時の2時間のみ有料（雨天時は無料)」というルールに変わったようで、待機列で待っている間に1000円でチケットを購入しました。そして入場時にはチケットを掲げて入るという、「伝統の確認方法」で入場しました(笑)。 #技術書典 一般参加の待機列にこれからお並びの方は、入口向かって左側の通路を突き当たりまでお進みください。一般入場券は待機列のそばで販売しております。入場の際は入場券を高くかかげてお進みください。また会場前の入口付近では公式ファンブック「技術季報」も販売しています。 pic.twitter.com/D9cllWiI2u— 技術書典公式アカウント (@techbookfest) April 14, 2019 3. いざ戦場へとにかく会場の込み具合は半端ない感じでした。まぁ、1万人も詰めかければそうなるでしょうね・・・ #技術書典 にお越しの皆さまにお知らせします。ただいま会場内が大変混雑しております。場内に入られましたらゆっくりと歩いてお進みください。 pic.twitter.com/fb4Q8suHOZ— 技術書典公式アカウント (@techbookfest) April 14, 2019 自分は目一杯楽しんで13時過ぎに離脱しました。というか体力の限界です。看板を横目に買い忘れたものがあるのではないかという不安を抱えながら立ち去りました・・・ #技術書典 一般入場は予定どおり11時開始となります。お時間になりましたら待機列の皆さんを順次ご案内してまいりますので、どうぞ走らずゆっくりとお進みください。 pic.twitter.com/BVF4PjJK1V— 技術書典公式アカウント (@techbookfest) April 14, 2019 4. 戦利品さて、早速ですが戦利品のご紹介です。といってもほとんど読めていないので書影だけで失礼いたします2。中には Boothで購入できるものもあるので、気になるカバーがあれば調べてみてください。 ネタ枠は２つあって一つはオライリーです。かるたとコーディングマグネットを手が滑って購入してしまいました。 もう一つは自戒の意味を込めて購入しました。Fat Projectというカードゲームです3。胃が痛くなりそうなゲームです。過労死はヤバイ・・・ 5. 感想今回はあまり買わなかったな・・・とか思っていましたが、広げてみると結構ありました(笑)。薄い本でもこんなにたくさんの人が執筆するなんて本当に凄いと思います。自分は興味がとっちらかっているのでなかなか集中して一冊の本を書くのは難しそうだなと感じていますが、それでもいつかはチャレンジしてみたいと思いました。 技術書典はエンジニアのモチベーションを高めてくれる素晴らしいイベントだと思うので、まだ参加されていないエンジニアの方はぜひ一度現場に足を運んでみてください。エンジニアの熱量や一体感、新しい技術の波が肌で感じられるはずです。 記事は以上になりますが、最後に主催者、参加者の皆様、本当にお疲れ様でした！ 次回もあればぜひ参加して、あの熱気、あの息吹をまた感じたいと思います。1.販売者になったことはないので想像です・・・ ↩2.書影は自分がカメラで撮影したものです。光の関係で実物と印象が異なるものもあるかもしれませんがご容赦ください。 ↩3.写真は拡張パックのカードも含みます。俺たちは魔法使いじゃない！ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/04/14/tech-book-fest6/"},{"title":"あの頃の自分に読ませたい「怖くないQiita」をQiitaに書きました","text":"Qiitaって怖い・・・そう思っていたあの頃の自分に読ませたい最高のQiita入門記事をQiitaに書きました。恐らくこのクオリティならあの頃の自分に満足してもらえると思っています1。 Qiita怖くないQiita (2019年度版) - Qiitahttps://qiita.com/hinastory/items/07e62b9b42f09c2be04a誰しもが最初は初心者です。分からないことに怯え、知らないことに恐怖します。**「Qiitaが怖い」**と思うのはQiitaに対して何かしらの**「壁」**を感じているからであり、その壁の正体を知り、正しく理解することが**「怖くないQ... ただ一つの懸念点としてはこの記事の内容が若干Qiita向けではないことですね・・・まぁ読んで頂ければ理由はわかりますが。ただ、初心者向けの記事なのでなるべく検索流入に期待できるQiitaに書いたほうがいいだろうという判断です。当然、文句を言われれば記事はこのブログに移しますが、内容自体はQiita自体のためになると思うので大目にみてもらえるのではないかと思っています。 1.「自分の中では」文句なく最高の入門記事ですが、当然全てのQiita初心者を満足させることができないことは理解しております。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/07/17/unscared_qiita/"},{"title":"Hexoでブログに再入門","text":"Advent Calendar用に久々にブログを立てて見ました。以前にパブリックに書いたのは多分10年以上前のような気がします。レンタルーサーバ借りてドメイン取ってRubyで動く何かを使って立てた気がするけど、よく思い出せません・・・ まぁ、過去のことは置いておいてせっかく立てたので、何か役立ちそうなことをメモしていきたいと思います。 目次1. Hexoとは2. 静的サイトジェネレータ3. 静的サイトジェネレータの長所4. 静的サイトジェネレータの短所5. なぜHexoを選んだのか？6. まとめ7. 補足 1. Hexoとはとりあえず今回このブログは、Hexo使って立ててみました1。Hexoはブログを書くためのフレームワークですが、WordPressのような従来型のブログサービスとの一番の違いは、静的にブログを生成するか、動的に生成するかです。 2. 静的サイトジェネレータHexoは静的サイトジェネレータというツールに分類されていて、あらかじめ用意したブログのテキストや画像等から、ブログの表示に必要なHTML,CSS,JavaScriptなどの静的ファイルを生成してくれます。Hexoがやってくれるのはここまでで、あとはHexoが生成したファイルをお好みのWebサーバに公開すればブログが表示されると言う仕組みです。WordPressはこれとは違い、Webにアクセスが来るごとにWebページをサーバサイドのプログラムが動的に生成してブラウザに返しています。 3. 静的サイトジェネレータの長所どちらにも一長一短があって、静的サイトジェネレータの良いところはローカルで確認してから、チェックが完了してからWeb公開できる点です。このローカルで確認できると言うのはすごい強みで、画像のアップロードも不要だし、テキストやCSSの修正も手元で直してすぐ確認できると言うのが非常に心地いいです。これが動的サイトだと一度サーバにアップロードして確認と言う手間が一つ入るため、色々と面倒臭くなります。 また、静的サイトジェネレータの場合、サイトの生成に必要なファイルは必然的に手元で管理することになるので、Gitなどのバージョン管理システムと相性が良いです。これは非常に安心感があって、こまめにコミットしておけば様々な理由による『ブログの消失』を未然に防ぐことができます。動的サイトの場合も手元にファイル等の形で残しておけば、バージョン管理することができますが、ちょこっとした修正をサーバにアップロードして、バージョン管理用ファイルに反映し忘れるという事故が起き易いです。 セキュリティ的に安心というのも魅力の一つですね。サーバサイドの処理がないので攻撃パターンが限られますし、特にWebサーバにホスティングサービスを使っている場合、自分ではほぼ何もセキュリティについて考えることはないです。少ない懸念点の一つとしてあげるならばJavaScriptを使ってる場合です。まぁ最近は大抵知らずのうちに使われてると思いますが、特に怪しいライブラリを使っている場合に勝手にマイニングされたり、踏み台にされる場合もあるので、気をつけた方が良いかもしれません2。 4. 静的サイトジェネレータの短所このようにHexoのような静的サイトジェネレータ型のブログツールには利点がたくさんあるのですが、もちろん欠点もあります。一番目立つのは、非エンジニアにとって利用するのにハードルが高いということです。例えば、Hexoのトップページには「Simple」と謳いながら、以下の導入時のコマンドが載っています。 12345$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo server これを見て「簡単そうだな、やってみようかな」と思える人は世の中に1%もいないと思います。もちろん現役のWebフロントエンドエンジニアなら、これを見てお湯を注いだカップラーメンができるより早くブログサイトを構築できるでしょう3。つまり、現在のところ静的サイトジェネレータはGitが分かるフロントエンドエンジニアにとって、非常に使い易いツールだけど、それ以外の人は、まぁ覚悟して取り組んでくださいとしか言いようがないですね。。。もちろん将来的にはシンプル詐欺などではなく本当に非エンジニアでも無理なく使えるツールが出てくるかもしれないので、この点は静的サイトジェネレータ固有の短所というよりもまだ成熟途中の技術の特徴ということになります。 あと当然ですが、静的サイトジェネレータ単体ではサーバサイドという概念がないのでRDBと連携して、大量データの検索とかテーブルの結合とかそういった処理も基本できません。カウンタやトラックバックやコメント機能などのブログにおなじみの機能もデフォルトではありません。このような場合はWebサービスと連携するのが常套手段ですが4、複雑になりそうなら最初から動的サイトやSPAやSSR等の構成5を検討した方が良いでしょう。逆に考えると静的サイトジェネレータの一番の使い道はこのような処理が基本的に不要な場合のプライベートなブログで、まさにHexoはこのサイトにうってつけなのでしょう。 5. なぜHexoを選んだのか？まさしく、前掲のコマンドを見て簡単そうに思えたからです(笑)。つまりそれをコピペして実行するだけで試せる環境と知識をすでに有していたからということになります。あとはHexoと言うより静的ジェネレータの利点の方が大きいですね。最初はVPSやパブリッククラウド上に、以前のように動的サイトを構築しようと考えていましたが、構築コストやセキュリティのことを考えるとだんだん気が重くなってきました。ブログサービスも検討しましたが、ソースは全てGit管理したかったのと手元で表示確認がすぐにできないので却下しました。つまりGit管理と親和性があって手元ですぐにサイトの表示が確認できて、セキュリティーの心配をあまり心配しなくてもいい記事投稿が簡単にできるブログツールを探していたということになります。その中で一番最初に目に付いたのがHexoだったというだけです。記事がMarkdownでかけるのもいいですね。なのでもっといいツールが見つかれば移行するかもしれませんが、しばらくはHexoと仲良くやって行こうと思っています。目下の悩みはテーマをどうするかです。デフォルトのlandscapeも拡張しやすくて悪くはなかったのですが、せっかくこれだけのテーマが公開されているので色々と試したいと思っています。6。 6. まとめまとめです。Hexoの箇所は静的サイトジェネレータに置き換えても大体合ってます。３行を目指しましたが、なかなか難しいですね。 Hexoは静的サイトジェネレータ型のブログツール Hexoはローカルで編集して、すぐに実際の表示が確認ができるので、心地よい Hexoはバージョン管理と相性が良く、こまめにコミットで安心できる Hexoはセキュリティ的に動的サイトに比べて安心 Hexoは非エンジニアに基本向かない Hexoではカウンタやコメント機能等、サーバサイドの処理が必要なものは基本使えない 7. 補足前述の通りでHexoを最初に選んだのは私にとって簡単そうに見えたからです。だけどそう見えたのは私の好みや経験という色眼鏡があったからなので、申し訳ないですがこのブログを読んでくれた方に当てはまるかどうかは分かりません。もちろん世の中にはHexo以外にも山のように静的サイトジェネレータがあるので、余力のある人にはぜひStaticGenを参考に自分にあった運命の静的サイトジェネレータを探し出して欲しいです。。。7 1.Hexoは「ヘキソ」と発音しています。正しいかどうかは分かりません。 ↩2.つい先日もnpmパッケージの「flatmap-stream」に仮想通貨を盗み出すコードが混入していて話題になりました。近年JavaScriptのセキュリティインシデントは増加傾向にあるので楽観視はできません。 ↩3.テーマを弄りはじめるとエンジニアでも頭を抱えそうですが・・・ ↩4.例えばコメント機能はDisqusと連携すれば実現できます。 ↩5.SPA = Single Page Application, SSR = Server Side Rendering ↩6.現在このブログで利用しているテーマはフッターを参照してください。テーマは今後変わる予定があるのであえて本文には記しません。 ↩7.願わくばこの記事を読んだ人が静的サイトジェネレータの旅に出て、ブログを書いてくれてそれが、私の目に止まりますように・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2018/12/02/start-blog-hexo/"},{"title":"JapanContainerDays v18.12 に初参加","text":"12/4(火)にJapanContainerDays v18.12に初参加しました。このイベントは春に1回目を開催して今回で2回目の開催だそうですが1、会場には800人以上の人が詰め掛けたようで非常に盛況でした。 JapanContainerDays v18.12JapanContainerDays v18.12https://containerdays.jp/[12/3-5開催]コンテナ活用/クラウドネイティブの現状をひとまとめにした開発者のためのイベント - JapanContainerDays 目次1. 会場の様子2. キーノート3. 参加セッション一覧3.1. 24日参加セッション3.2. 25日参加セッション4. 食事5. 戦利品6. 感想とまとめ 1. 会場の様子会場は御茶ノ水のソラシティカンファレンスセンターでした。ここには以前にデブサミ夏や大江戸Ruby会議06に参加した時に来たことがあったので特に迷うことなく到着しました2。9:40開始だったのですが念の為30分早く会場入りしたのでキーノートの会場はまだ半分近く空いていました。 撮影：JapanContainerDays実行委員会引用元 撮影：JapanContainerDays実行委員会引用元 2. キーノートさすがにキーノート開始直前には満席近くになっていました。キーノートは会社やコミュニティの代表が順番に登壇する形式で2時間行われました。その中でなんといっても目玉はCNCFのCOOであるChris Aniszczykさんの発表だと思います。 撮影：JapanContainerDays実行委員会引用元 CNCFの歩みや今後の展望が語られていました。気になったのはIoTやEdgeにもKubernetesが進出するという話題でしょうか。KubeEdgeには注目していきたいと思います。あとはどんどんServerlessやNodelessに進んでいく方向性が強調されていました。virtual-kubeletの紹介もあり今後が非常に楽しみです。 GitHubkubeedge/kubeedgehttps://github.com/kubeedge/kubeedgeKubernetes Native Edge Computing Framework (project under CNCF) - kubeedge/kubeedge GitHubvirtual-kubelet/virtual-kubelethttps://github.com/virtual-kubelet/virtual-kubeletVirtual Kubelet is an open source Kubernetes kubelet implementation. - virtual-kubelet/virtual-kubelet あと、クラウドネイティブの定義も紹介されていました。今までクラウドネィティブとは何ぞやと思っていたので少しはもやもやが晴れたような気がします。 クラウドネイティブテクノロジは、パブリック、プライベート、およびハイブリッドクラウドなどの最新のダイナミックな環境でスケーラブルなアプリケーションを構築および実行するための組織の能力を強化します。コンテナ、サービスメッシュ、マイクロサービス、不変インフラストラクチャ、および宣言型APIは、このアプローチの例です。 これらの技術は、復元力があり、管理しやすく、観測可能な疎結合システムを可能にします。堅牢な自動化と組み合わせることで、エンジニアは頻繁に、そして予想通りに影響の少ない変更を最小限の労力で行うことができます。 クラウドネイティブコンピューティング基盤は、ベンダーに依存しないオープンソースのプロジェクトのエコシステムを促進し維持することによって、このパラダイムの採用を推進しようとしています。最先端のパターンを民主化し、これらのイノベーションを誰もが利用できるようにします。 クラウドネイティブの定義(Google翻訳を利用)引用元 あとはCNNFの歩き方や全体像も紹介されていました。 CNCF Cloud Native Interactive LandscapeCNCF Cloud Native Interactive Landscapehttps://landscape.cncf.io/format=landscapeFilter and sort by GitHub stars, funding, commits, contributors, hq location, and tweets. Updated: 2020-02-01 04:30:11Z Trail MapはともかくLandscapeは圧巻ですね・・・ まぁその中でもCNNFが直接ホスティングしているのはごく僅かです。これを見ると全部で30程ありますが、現時点では成熟度が最高のGraduatedになっているのはKubernetes、Prometheus、Envoyのわずか３つです3。これからクラウドネイティブが浸透していく中でこれらのプロジェクトがどの様になっていくのか注視していきたいと思います。 キーノートには他にも面白い発表がいくつもあったのですが、印象に残ったのはメルカリの発表でした。なぜコンテナオーケストレーションにKubernetesなのかという問いに対して拡張性とエコシステムを挙げていましたが、これと同じ答えはこの後に続くセッションでも繰り返し聞くことになりました。そしてこの答えがOSSで勝ち抜くための到達点なんだという気がしました。シンプルで直交的で整合性のとれた拡張性で素早くエコシステムを構築すること。言葉にすると短いですがこの状況をわずか数年で作り上げて決着までほぼつけてしまうとは、改めてKubernetesの成し遂げたことの凄さを実感しました。 撮影：JapanContainerDays実行委員会引用元 3. 参加セッション一覧とりあえず参加セッションを列挙してみます。メモったキーワードを記載していますが、あやふやな部分も多く含むためさらっと流していただけると幸いです。 3.1. 24日参加セッション keynote CNCF Microservices on Kubernetes at Mercari（Mercari） Kubernetesによる機械学習基盤への挑戦（Preferred Networks） LINEエンジニアを支えるCaaS基盤の今とこれから（LINE） Cloud Nativeの未来とIBMの取組み （IBM） クラウドネイティブで作る、新しいクルマの世界（デンソー） ZOZOTOWNシステムリプレイスにおけるKubernetes活用（ZOZOテクノロジーズ） コンテナネットワーキング（CNI）最前線 コンテナネットワーキングは闇が深い、動きが早い、スケーラビリティに注意 Kubernetes ネットワーキングのすべて Services, Ingress, NetworkPolicy 40 topic of Kubernetes in 40 minutes serviceでなくingressを使え、latestタグは使うな、DockerHubのイメージは信用するな、バージョンアップは容易にするな、ボリュームは使うな、全機能を使う必要はない、ingressはGAではない Ansible、Terraform、Packerで作るSelf-Hosted Kubernetes Kubernetesから始めるクラウドネイティブエンジニアへの道 〜 Kubernetesトレーニングと、CKA/CKAD資格取得に向けて 〜 構築ツールを使わずにやってみた(kubernetes the hard way)、k8sはCloud Nativeにおいてはスターティングポイント 2019年はコンテナよりもクラウドネイティブ!? Knativeのすべて コンテナ辛い！そこでknative, カナリアリリースが楽、riffもあるよ After Party & Booth Crawl !! 3.2. 25日参加セッション 2020年のコンテナはどうなる!? コンテナプラットフォームのこれまでとこれから マイクロサービスは組織論に行き着くので難しい k8sは拡張性の高いOSようなもの K8sは開発やPOCまではうまくいくが本番運用に課題あり istioがここまでくると思っていなかった。サービスメッシュはエンタープライズにこそ必要 cloudfoudry,istio,mesoss,open shift,rancher,rio, core os,kubeflow, knative マイクロサービスの高可用性：Ingress, サービスメッシュの世界におけるロードバランシング nginx社は実在する plusもあるよ 複雑になったWebを簡素化したい fablic modelではすべてのappにサイドカーとしてnginxが入る Dockerセキュリティ: 今すぐ役に立つテクニックから，次世代技術まで 鍵をイメージに入れるな、--ssh,--seacretを使え。使えない場合はスカッシュやマルチステージというワークアラウンドを使え イメージスキャナ - clair, aqua microscanner 誤検知もあるので信用しすぎるな dockerソケットに気をつけろ。dockerグループに入れるのは危険 コンテナ is namespace + capabilities + cgroups SELinux無効化するときは「石川さん、Walshさんごめんなさい 」と唱えてから実行 Kubernetesと暮らすRancherな生活 rancherはk8sのオーケストレーション層ではなくその上のストラテジー層 2.0でk8s一本になり、Pure Goになり、マネージドもOKになりカタログがHelmになった rioはおもしろそうだがまだかなり柔らかい。k8sを意識しない時代に・・・ MeetUpを活性化せよ！最強のリアルタイムQA投稿アプリをCloud_Nativeにつくってみた Qicoo 自演が可能です eks,ec2,elastic cache, rds,cloud flront, route 53,cert manager, elb, travis ci, kustomize, hepioak, spinnaker,kanyeta,prometheousOperator, Halyard Cloud Native Developers JP Cloudcraft – Draw AWS diagrams コンテナ時代のOpenStack Kubernetes Meetup Tokyo 2年間の振り返りと未来 参加者の規模数がすごい 登壇者限定で濃い話をする会がある 登壇してください 自分が参加したセッション以外は以下にまとめがあるのであとで読みたいと思います。 MediumJapanContainerDays v18.12 matomehttps://medium.com/@containerdaysjp/jkd1812-ab186974d52dJapanContainerDays v18.12 公式資料まとめ 4. 食事昼食はランチセッション中に配布されたサンドイッチやまい泉ヒレかつサンドを食しました。 撮影：JapanContainerDays実行委員会引用元 お菓子やドリンクも無料で楽しむ事ができました4。ContinerDaysのロゴが入ったマカロンが可愛かったです。 撮影：JapanContainerDays実行委員会引用元 After Partyではビールやワインや焼酎をの呑みながらワイガヤしてました。急遽決まったLT大会では個性あふれる様々な会社がHiringトークをしていました。 撮影：JapanContainerDays実行委員会引用元 5. 戦利品勢い余って電子書籍3冊も買ってしまいました。面白かったのはNGINXで配られたUSBケーブルです。正直そのタコ足のような発想はありませんでした(笑) 6. 感想とまとめ参加したことでCloudNativeがどこまできているか肌で実感できた気がしました。自宅ではminikubeからGPUを触れる環境を構築していますが、いまいち有効利用できていない気がしていたので、このイベントを機会に手を動かしながらいろいろとCloudNativeに近づけて見たいと思います。 Kubernetesの一人勝ち 今後はKubernetesのエコシステムの何処を占めるかの戦い サービスメッシュ(Istio)がかなり熱い Helmもデファクトになりそう knative, virtual-kubeは注視したい 機械学習のインフラとしても面白そう Kubernetesは本番運用に課題あり knative, OpenShift, Rancher等さまざまなアプローチで解決しようとしている 最終的にはNoOpsやサーバーレスやFaaSな未来に行き着くのだろう・・・（時期未定） 来年はCloudNative Days1.JapanContainerDaysは2回目にして、今回で最終回だそうです。次回からはCloudNative Daysに変わるそうですがCloud Native Days Tokyoとの関係性はよくわかりません・・・ ↩2.駅から徒歩1分なので、そもそも迷うほうが難しい気もします・・・ ↩3.というかPrometheusはCNNFだったとは初めて知りました・・・ ↩4.当日は本当に無料だと喜んだのですが、冷静に考えると参加費を払っているので無料でもなんでもなかったです・・・ しかし、チケットは最後までEarly Bird価格のまま安売りされていたけど主催者の想定した人数が集まらなかったから値下げしたのか、それとも早めに予定金額を調達できたから値下げしたのか真相がわからない ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2018/12/06/container-days/"},{"title":"気が付いたら本棚がキーボード棚に変わっていた話","text":"この記事は自作キーボード #2 Advent Calendar 2018 12日目の記事です。自分はゆるふわでキーボードを楽しみたい勢なので、カジュアルにキーボードを楽しむ方法について書いてみたいと思います。ガチなモノ作りの記事はすでに多くの人が書かれていると思うので、その途中で箸休め的に読んでいただけると幸いです。 Adventar自作キーボード #2 Advent Calendar 2018 - Adventarhttps://adventar.org/calendars/2964書きたいのに枠がないという方のための2枚目です。 [その1](https://adventar.org/calendars/2954) [その3](https://adventar.org/calendars/3208) 「皆さん、EndGameしていますか？」 日本の自作… 目次1. 自作キーボード以前2. 自作キーボードに興味を持つ3. 発狂する4. 自作キーボードの門を叩く5. キーボード棚の作り方6. デコキーのすすめ7. 最後に 1. 自作キーボード以前キーボードにこだわりを持ち始めたのはコンパクトなキーボードを探し始めてからでした。もう15年以上昔の話ですが、その時出会ったのがぷらっとホームが販売していた以下のFKB8579です。 気持ち良い打鍵感で10年以上愛用していました。右側の画像はキートップを外したところですが、キースイッチはメンブレンシートです。最近はCherry MX互換やロープロのスイッチしか触っていなかったのでこういうのを見るとほっこりします。大切に使ってきたのでまだ使えますが、さすがに引退させてあげたいと思ったので次に飛びついたのがARCHISSのProgresTouch RETRO TINYです。メカニカルキーボードでコンパクトなタイプを探していたらちょうどこのキーボードが発売直後だったので購入しました1。 2. 自作キーボードに興味を持つProgresTouch RETRO TINYは打ち心地がよく気に入っていたのです、去年あたりから自作キーボードが盛り上がっていると言う噂を聞くようになりました。ただ、はんだ付けが面倒そうだったので様子見していましたが、キー配置のカスタマイズが自由にできるとういうのはやってみたいと思っていました。 しかしその瞬間は唐突に訪れました。去年の11月ごろMassdropからPlanck Lightが買えるという情報が流れてきたのです。もちろんこのときMassdropもPlanckキーボードのことも欠片も知りませんでしたが、以下の動画を見て一発でやられてしまいました。 やっぱり光るのずるいですね（笑）。あと完成品も販売してくれるとのことだったので、懸念だったはんだ付けも必要がないことも大きかったです。これで念願のキー配置を自由にいじれるということで即座にDropに参加しました。ただこれが過ちの第一歩だとは知らずに・・・発送予定は2018/5/18となっていて、このときまだ半年先だったので気長に待つことにしました・・・待つことにしたのですが、Massdropのやつは毎日のようにダイレクトメールを送りつけてきます。無視すればいいだけのことなのですが、ときどき GMK Red Samurai のような耐え難い誘惑があり、Dropに参加させられました・・・ 3. 発狂する5月初旬にMassdropから一通のメールが来ました。PCBの生産でトラブっていて発送が7月初旬になるということでした。楽しみにしていただけに落ち込みましたが、既に半年近く待っていたのであまり気にしない方向で精神を落ち着けました。ちなみにこのときはまだ自分がはんだ付けをすることになるとはまだ考えていませんでした。このときまでにQMK firmwareが動作する完成品を販売している場面を何回か見ていたのでこれが気に入らなかったら別のを買えばいいと考えていたからです。時が流れて6月に入り大分そわそわしてきたので、GitHubからQMK firmwareのリポジトリをフォークしてキーマップをいじり始めました。このキーマップを考えている瞬間は最高に幸せでした。。。 そしてとうとう7月に入り我が家にPlanck Lightがやってきました。Macにつないで打ってみて動作は問題なさそうでした。そして色々と光らせてこのときは大分テンションが上がりました。そして念願のQMK firmwareを書き込むときが来ました。しかし、不思議なことにfirmwareが書き込まれてくれないのです。何回も試しました。何回も、何回も・・・困り果ててMassdropのDiscussionを覗いて見ると、なんとブートローダを書き込むのを忘れたからキー配置をいじれないという旨が公式からアナウンスされていました・・・ 発狂しました 4. 自作キーボードの門を叩く選択肢はいくつかありました。幸いというか当然というか公式はISPフラッシャーを設計して送ると言っていました。しかしながらこのときは完全に疑心暗鬼になっていて楽観的に待つ気にはなれませんでした。実際に公式の対応は非道いもので、一週間何の音沙汰もないこともしばしばで毎週Discussionで誰かが進捗を尋ねていましたが無応答がほとんどで、罵声が飛ぶような殺伐としてた雰囲気でした2。２つ目の選択肢は自力でブートローダを書き込むことです。一応成功した人もいるようなので試してみる価値はありましたが3、いきなり壊したら立ち直れそうになかったので、諦めて今まで避けてきた第三の選択肢を取ることにしました。 ここに来てようやく自作キーボードの門を叩く決心をしました。本当に長くなってすいません・・・決心したら行動は早かったです。 Ergo42 を注文して4、秋葉原のマルツではんだごてやテスター等必要な道具を揃えて、届いたらすぐに組み立てはじめました。しかし結局動かなかったのでDiscordの自作キーボードのサーバーの初心者チャンネルで色々な人に助けてもらいながら何とか動作させることができました。このときの感動は何とも言い難いですね。あのとき善意で助けいて頂いた方々には本当に感謝しています。そして辛かったけど本当に乗り越えられてよかったです。 この後は数字キーが欲しくなってIrisに手をだしました。 上のキーボードがIrisで下のキーボードがErgo42です。両方共46キーだったのでこの写真は「46キー姉妹」というタイトルでDiscordに投稿しました。しかし何というかあれだけ欲しかった数字キーも慣れてくるといらなくなりました・・・結局手首をなるべくリストレストに固定して打つようになるため指が届きにくいキーは不要みたいな感じになりました。 このあとに作ったのはMint60です。Maker Faire Tokyoで買い損ねたので、コミケに始発で乗り込んでゲットしてきました。 オシャカワなキーボードが手に入ってとても嬉しかったのですが、残念ながらこの頃には指がOrtholinearに慣れてしまっていて、Row-StaggerdなMint60は既に打ちづらくなっていました・・・もう少し出会いが早ければ最高の相棒になったかもしれませんが、現在は布教担当として働いてもらっています。 だいたいここまでが自作キーボードを作り始めて1ヶ月くらいですが、一気にモノを増やしすぎたため置き場所に困るようになってきました。もちろんただ積み重ねるだけならできるのですが、キーキャップやスイッチ等は探しやすく見比べやすいようにしたい欲求が高まってきたのでキーボード棚を作ることにしました。 5. キーボード棚の作り方まずは本棚をご用意ください(笑)。いきなり落ちからはじめて申し訳ございません。実際に自分のキーボード棚は本棚の流用なのでこれ以上何も言うことはないのですが問題が一つありまして、もともとこの本棚には捨てられない本5がぎっしり詰まっていたのです。そこでひたすら 自炊を頑張りました。 一気に100冊程自炊したので大分消耗しました。上の写真は自炊道具で、どこのご家庭にもあると思わるごく普通の裁断機とスキャナです6 。ブログ先頭の写真は自作キーボードをはじめて2ヶ月経った頃の写真でキーボード棚を作った当初の写真です。現在は以下のようになっています。 下から2段目は道具を置いています。道具は他にもあるのですが収まりが悪いハンダゴテや腕を増やすツールとかを置いています。一番下の棚は非常に心苦しいのですがいわゆる積みキーボードというものです。今年中に何とか進捗を出そうと思っています。最初は実用重視で作ったキーボード棚ですが、実際に作ってみるとショーケースみたいな感じで毎日眺めていて楽しいので、キーボードの楽しみ方が一つ増えました。これはおすすめです。ちなみに本棚はあと6台あるので徐々にキーボード棚に変えて行こうと思っています。 6. デコキーのすすめこの記事の冒頭でカジュアルにキーボードを楽しむ方法について書きたいと述べましたが、その一つがデコキーです。デコキーとは「デコレーションされたキーボード」の略で、ここではシールや紙・布等を両面テープとでキーボードに貼り付けて飾ることを指しています。メイクと同じで盛りキーボードと言ってもよいかもしれません。 EndGameという言葉をよく耳にしますが、実際に打ち心地やキー配置はEndGameに近づけられても見た目は必ず飽きがくると思われます。もちろん際限なくキーキャップを変えたり、キーキャップやケーブルやケースやPCB等を自作できる時間と金銭と能力に余裕がある人はいいのですが、世の中の大半の人はそうでないと思われます。そこでデコキーでカジュアルにキーボードの見た目を変えてみようというのがここでの提案です。もちろんこのデコキーがしやすいキーボードには前提があってシールを貼る余白が必要です。現在の自作キーボードキットにはデコキーに必要な余白を持つものが非常に少ないのですが、Ergo42はその中では珍しくデコキーと相性が良いと思われます。 Ergo42の上部の台形部分にシールを貼るだけで印象はガラッと変わります。この写真では木目調のシールを貼っていますが。この手のシールや紙・布は100均で安く手に入り種類も豊富です。自分もちょっとした気分転換にキーキャップに合わせて気軽に盛り直していますが、簡単な割には満足度は高いです。 上のキーボードはRed Samuraiのキーキャップに合わせて台形部分と巻取り式ケーブルに畳縁を貼っており、ケース側面に畳シールを貼っています。一つ注意点を上げるとすればErgo42にデフォルトで付属しているのは鍋ネジなので、そのままだとシールが浮いてしまうので皿ネジに変える必要があります。また皿ネジにしてもそのままだとまだ少し浮くので適当なドリルでネジが平らになるようにアクリル板を少し凹ませる必要があります。 7. 最後に長々と書いてしまいましたが、自作キーボードの世界に飛び込んで後悔はしていません。最初は面倒だったり怖かったりしたものですが、すぐに助けてくれる気の良いコミュニティがあるので、余計な心配をせずにまずは手を動かしてみることが重要だと気づきました。そして新しく開けた扉の先には、モノを作る楽しさと感動がダイレクトに待っています。何より普段何気なく使っているキーボードに対して理解が増し、不便も解消され、愛着も得られると思います。ただ無理をしすぎると反動が来るので、自分にあったペースで自分にあった楽しみ方を見つけて行きたいという思いでこの記事を書きました。この記事がキーボードをカジュアルに楽しみたい方や自作キーボードに興味があるけどまだ手を出せていない方の一助になれば幸いです。 おいでよ、自作キーボードの世界へ この記事はErgo42 with DSA COFFEE HOUSE\" KEYSET & Cherry MX Redで書きました。1.軸は悩みましたがまずは無難に赤軸にしました。 ↩2.結局ISPフラッシャーはさらに2ヶ月も経って、諦めかけた頃に送られてきましが、firmwareの書き込みは無事成功しました。 ↩3.Quick and dirty guide to flashing your Planck Light bootloader : olkb ↩4.現在は後継のErgo42 Towelが販売されています。 ↩5.決して怪しい本ではないです。 ↩6.スキャナはScanSnap S1500Mです。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2018/12/12/keyboard/"},{"title":"サイレントでホットスワップなErgoDash miniを制作(前編)","text":"2018年のキーボード納めとして、年末にErgoDash miniを組みました。ただ普通に作ったのでは成長がないので、以前よりやってみたかった静音化とキースイッチとProMicroのホットスワップ化に挑みます。 GitHubomkbd/ErgoDashhttps://github.com/omkbd/ErgoDash/tree/master/minikeyboard. Contribute to omkbd/ErgoDash development by creating an account on GitHub. 目次1. はじめに2. ErgoDash miniとは3. 制作工程3.1. 潤滑剤の塗布3.2. MogeMicro対策3.3. ダイオードの取り付け3.4. TRRSジャックとリセットスイッチの取り付け3.5. Undergrow LEDの取り付け3.6. 後編へと続く・・・ 1. はじめにこの記事は自作キーボード初心者から中級者向けの記事です。ただ「初めての自作キーボード制作」の場合、静音化とホットスワップ化は難易度が高い箇所があるので、まずはシンプルに一台目を組み上げて自信をつけてから挑戦することをオススメします1。実際の制作には公式のビルドガイドを熟読した上で進めてください。この記事は静音化とホットスワップ以外にもビルドガイドの行間とも言える一般的に躓きやすい箇所も説明を入れるように心がけました。これから作業する方の参考に少しでもなれば幸いです。 2. ErgoDash miniとは国内で購入可能なキーボードキットです。もともとErgoDashというものがあってそれよりもキーが一行少ないバージョンが今回制作したminiです。キーボードの特徴としては左右分離型で、一部の物理的なキー配置をカスタマイズすることができます。キー数は52～56キー（最下段をカスタマイズ可能、最大100パターン）です。ケースの関係上、親指あり/なしは購入時に選択する必要があります。 ErgoDash miniドキュメントより引用元 販売元 だっしゅきぃぼぉど - BOOTH ビルドガイド ErgoDash mini Build Guide 3. 制作工程実際に自分が行なった手順通りに書いていきます。前述のとおり公式のビルドガイドに含まれていない手順もあります。 3.1. 潤滑剤の塗布キーボードの静音化は主に以下の３つの対策があるようです。 静音用キースイッチを使う キースイッチ自体に静音化対策が施されています 潤滑剤を使う キースイッチを分解して、擦れる箇所に潤滑剤を塗布します スタビライザーを利用する場合は、スタビライザーにも塗布します O-ringを使う キーを強く打ったときの底打ちの音を抑えるものです 今回は上記の3つとも実施しました。まずはキースイッチを買うところからですが、今回はGateron Silent スイッチの赤軸を利用しました。Cherry MX互換のサイレントスイッチは他にも色々ありますが、とりあえず手っ取り早く入手できるものを選びました。今回キースイッチをホットスワップ化するので、まずは手持ちのキースイッチで初めて後で交換でも良いです。 次にキースイッチに潤滑剤を塗布します。lubeするとも言います。潤滑剤にも色々ありますが無色透明でプラスチックと金属に塗布可能なものから選択すれば良いです。どの潤滑剤がいいかは正直好みになります。音もそうですが打ち心地にも影響します。塗布箇所や塗布量によっても変わってくるのでいろいろなパターンで研究してみるのが一番いいかと思われます。今回は潤滑剤にKrytox GPL 105を使用しました。実際の塗布作業ですが、一つ一つキースイッチを分解して潤滑剤を筆で塗る作業です・・・長く・辛い作業です・・・しかしこの苦痛を軽減するために以下の素晴らしい治具の数々があります。キースイッチオープナーはほぼ必須ですが他も揃えておいたほうが断然楽です。自分の場合、以下の治具を使って2時間かかりました2。 キースイッチオープナー スイッチステムホルダー キースイッチオープナー用オプション スイッチ Lube Station 以下の写真は実際にlube作業中の風景です。キースイッチオープナーで必要なキースイッチの個数を分解して、パーツごとに選り分けておきます。その後スイッチの下側部分をLubeステーションにはめ込みます。あとは下側部分に一気に塗布した後、バネと軸を順番に塗布して下側部分に乗せて、最後に上部を戻して完成です。 3.2. MogeMicro対策これも公式ビルドガイドには書いていませんが、大事なことなので記載しておきます。キットに付属しているPro microはMogeMicroという蔑称を授けられるほど、USB端子部分が破損しやすいです。何も対策をしないと何回かケーブルを抜き差ししているうちにポロリと逝ってしまわれます。対策は自分の観測範囲で以下の4つがありました。今回は1.を実施しておきます。2.も合わせて購入しておけばさらに安心です。3.はお金に余裕がある人向けです。4.はここを読まれている方にはとりあえず無視してもらって構いません・・・気になる方は調べてみてください。 エポキシ系樹脂で固める 磁石で着脱可能なmicro USBケーブルにする Pro micro スイッチサイエンス版を買う BLE Micro Proを使って無線化する エポキシ系樹脂は２つの液剤を混ぜて使うタイプのものです。よく混ぜたらUSB端子の側面に盛って行きましょう。注意点としてはスルーホールに液剤が流れ込まないようにすることと、USB端子の上部に液剤を盛るとケースに干渉するので側面だけにしておくことです。もちろんUSB端子も塞いではいけません(笑)。液剤が固まるまでそこそこ時間がかかるのでMogeMicro対策は組み立て工程の最初に行っておくのがオススメです。 3.3. ダイオードの取り付けダイオードはキットに標準で付属するスルーホールに挿して使うタイプのものを使用します。表面実装タイプ(SMD)のものも利用できそうですが、はんだ付けの難易度がぐっと上がるので今回は見送りました。ダイオードを取り付ける前にダイオードの足を曲げる必要がありますが、正しい長さで曲げないとスルーホールにきちんと嵌らないので楽をしたいなら治具(リードベンダ ー)を使うことをオススメします。 ダイオードの足を曲げ終えたら一度PCBに全て挿してダイオードの向きが間違えていないか、挿し忘れがないか確認します。ここは本当に何回も見直したほうが良いです。はんだ付けしたあと間違いを見つけると心が折れてしまいます・・・あとErgoDashではダイオードの隣に抵抗のスルーホールがある箇所があるので差し間違いが容易に起こりえます。公式のビルドガイドの写真で色分けされているのでしっかりと確認しましょう3。ダイオードの確認時はツールクリッパーという治具が役立ちます。下の写真のように基盤をツールクリッパーで固定すればダイオードを挿す工程と確認工程が非常に楽です。電子工作をしていると時々腕が足りなくなるので、とりあえず持っておいて損はないと思います。 ダイオードの確認が終わったらマスキングテープでダイオードを仮止めしてからはんだ付けに入ります。上の写真ではマスキングテープの代わりにカプトンテープを使っています4。ここからはようやくはんだ付けの作業です。今回は有鉛はんだを用いました。鉛は有害なので作業中は煙をなるべく吸わないようにしてこまめに換気しながら作業しました。はんだ付けは温度が命と言われていますので温度調整機能付きのはんだごてを使用します。ただし標準のの小手先だと母材に熱が伝わりにくいため小手先をC型のものに変えました。はんだ付けのコツは母材を温めてからハンダをそっと近づけて溶かして流し込む感じなそうですが、言葉だけでは伝わりにくいので一度YouTube等ではんだ付けの動画を検索してみることをオススメします。 3.4. TRRSジャックとリセットスイッチの取り付けTRRSジャックとリセットスイッチの取り付けは公式のビルドガイドの通りに実施すれば問題ないはずです。基盤の裏側に取り付けることだけを気をつけていれば十分です5。公式のビルドガイドではこの手順の前にオプションとしてBacklight LED用のパーツ取り付けがありますが、今回はBacklight LEDは取り付けないのでスキップします。というのも、Backlight LEDはスイッチの上から挿さないといけないため、スイッチをホットスワップにするとLEDもホットスワップにしないといけませんが今回はそこまで手が回りませんでした6。 3.5. Undergrow LEDの取り付けBacklight LEDに関しては一旦保留にしましたが、やはり光らせたいという欲求はあるのでUndergrow LEDを実装します。今までテープ型や砲弾型のLEDは実装したことがありましたが、チップ単体での実装は初チャレンジです。チップは公式で案内されているとおりWS2812Bを電子工作の強い味方、秋月電子さんで購入しました。初期不良や取り付け途中で壊してしまうことも考慮して5セット分余分に購入しました。汎用LEDなので余った分は他の電子工作で使う予定です。 公式のビルドログのとおりUndergrowを有効にするにはマスター(USBを挿す方)を決めて、はんだでジャンパする必要があります。このはんだだけでジャンパさせる方法も最初は慣れませんでしたが、ショートさせたいランドの双方にはんだを盛った後(予備ハンダ)、はんだを供給しながら小手先で盛ったはんだを溶かして繋ぐように動かせばうまくいくようになりました。そしていよいよLEDのはんだ付けですが、意外とLEDが小さいので思ったよりは面倒でした。取り付けの際に注意したほうがいいと思ったのは以下の3点です。 LEDの方向に注意する LEDが浮かないように注意する LEDを熱で壊さないようにする LEDには方向があります。公式のビルドガイドを見て方向を間違わないようにしましょう。LEDは直列につながっていて、ErgoDash miniではマスターの内側から外側に向けてぐるっと回って、反対側の基盤の外側から内側に向けて繋がっている感じです。LEDの方向がわかったらマスキングテープでチップの片側を固定します。このときチップが浮かないように確認してください。チップが浮いてしまうと接続不良になる恐れがあります。マスキングテープは片側のはんだが終われば剥がして、もう片側をはんだします。また、LEDは非常に熱に弱いみたいです。有鉛はんだではんだ付けする場合340℃から360℃に温度を調整するのが一般的ですが、今回は260℃でなるべく短時間ではんだ付けするように意識しました7。はんだの方法はまず軽くランドを温めてはんだを盛った後、素早くチップの電極に触れてはんだを接着させました。電極にふれる時間はほんの一瞬です。 3.6. 後編へと続く・・・後編ではいよいよPro Microとキースイッチのホットスワップ化に挑みます。 サイレントでホットスワップなErgoDash miniを制作(後編)1.1台目はシンプルなものでも、いろいろ反省点や改善点が目に付きます。また、シンプルなものを知らないと何が良くなったのがわからないので、お楽しみは２台目以降にとっておきましょう。自分はこれで4台目ですが腕はまだまだです・・・ ↩2.アニメを見ながらだったので実際はもっと短く済むはずです・・・ ↩3.実際私は再確認で合計3箇所の間違いを発見しました・・・過信は禁物です。 ↩4.カプトンテープは絶縁にも使えるので持っておいて損はないです。 ↩5.基盤は右手側と左手側で裏表が逆になるので注意が必要です。 ↩6.公式のビルドガイドにありませんが、チップLEDにする方法もあるのでBacklight LEDに関しては後で検討します。。。 ↩7.ErgoDash miniのビルドガイドには、はんだ付けの推奨温度は書いてありませんでしたが、Helixのビルドガイドでは220℃を推奨していました。高い温度で行うとはんだが溶けやすく作業がしやすいのですが、LEDを熱で壊すリスクがあります。あくまで温度の調整は自己責任でお願いします。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/01/06/ergodash-mini-1/"},{"title":"Kubernetes Meetup Tokyo　#15 - KubeCon 2018 Recap に初参加","text":"Kubernetes Meetup Tokyoに初参加しました。会場は六本木ヒルズのGoogle東京オフィスです。倍率2倍の抽選を潜り抜けて参加できることになりました。 connpassKubernetes Meetup Tokyo #15 - KubeCon 2018 Recap (2019/01/10 19:00〜)https://k8sjp.connpass.com/event/112661/?utm_campaign=&utm_source=notifications&utm_medium=email&utm_content=title_linkKubernetes Meetup Tokyo #15を開催します！ コンテナをデプロイできる強力なシステム Kubernetes のことを詳しく聞く会です！ # 行動規範 (Code of Conduct) について Kubernetes Meetup Tokyoは、Kub… 目次1. Kubernetes Meetup Tokyo #15 - KubeCon 2018 Recapとは？2. 会場の様子3. 発表内容3.1. KubeCon + CloudNativeCon North America 2018 Overview - @ladicle3.2. Lightning Talk: Introduction to GitOps Deployment to Kubernetes - @sakajunquality3.3. Running VM Workloads Side by Side with Container Workloads - @amsy8103.4. Keynote: Developing Kubernetes Services at Airbnb Scale - @jyoshise3.5. Fly Your Containerized Environments by Joint Work of Harbor and Dragonfly - @capsmalt3.6. Securing Kubernetes With Admission Controllers - @tkusumi4. 食事5. 感想とまとめ 1. Kubernetes Meetup Tokyo #15 - KubeCon 2018 Recapとは？Kubernetes1 Meetup Tokyoは、東京都内で不定期に開催されるKubernetesに関する情報交換や交流を行うための勉強会で今回で15回目の開催です。Kubernetesに熱い情熱を捧げる有志の方々が運営されています。参加登録はconnpassから気軽に行えますが、ここ最近は人気が高く定員オーバーの状態が続いています。一般枠は激戦で参加できるかどうかは抽選なので運次第ですが、遠方枠や女性/LGBT枠は競争率が低いので条件を満たす人はオススメです。 今回のMeetupは「KubeCon 2018 Recap」という副題が示すとおり、昨年12月10〜13日にかけてシアトルで行われたKubeCon + CloudNativeConのおさらいです。KubeConは世界最大規模のKubernetesのカンファレンスで、前回は8000人を超える参加者と100を超えるスポンサーが集結しました。 2. 会場の様子実は六本木ヒルズ自体も初めてだったので、開場の10分前くらいに到着して周囲を散策していまいた。場所はその名の通り六本木駅でコンコースで直結されていたので、東京メトロの地下からエスカレータで地上に出ると眼前に六本木ヒルズがそびえ立っていました。 六本木ヒルズ1階入り口の入ってすぐ左に臨時受付があり、そこで入館証を貰ってからエスカレータで43階のGoogleオフィスにお邪魔しました。 開場は食堂だったようで中央にオープンキッチンがあってその周囲にテーブルや椅子が配置されていました。さすがGoogleだけあってスペースも広く、ガラス張りの展望も素晴らしいものでした。Google東京オフィスは今年中に渋谷ストリームに移転するらしいので、その前にこれて良かったです。 ITmedia NEWSGoogle日本法人、19年に大規模移転 「渋谷ストリーム」22フロア占拠http://www.itmedia.co.jp/news/articles/1711/17/news082.htmlGoogle日本法人は、2019年に六本木ヒルズから渋谷にオフィスを移転する。 3. 発表内容今回の発表はKubeCon参加者が参加したセッションの中から一つ選んで、その概要、なぜそのセッションが面白いのか、自身の業務課題との関わりについて話すというものでした。 3.1. KubeCon + CloudNativeCon North America 2018 Overview - @ladicle最初の発表は今回のKubeKonの概要を説明するものでした。開場の雰囲気や注目ポイントを発表されていました。印象に残ったのはKubernetesが退屈(Boring)なものになったというものですね。Kubernetesのコアの技術は十分に成熟してメインストリームで使われるようになり、Kubernetes本体よりもその周辺技術や応用例、運用関連に話題の方向性がシフトしてきている感じです。 3.2. Lightning Talk: Introduction to GitOps Deployment to Kubernetes - @sakajunquality次の発表は実際にKubeConでLightning Talkをした方の発表でした。テーマはGitOpsでDevOpsを一歩先に進めた感じのものでした。簡単に言うとGit上のコード変更やプルリクエストをトリガーにしてKubernetesの操作やCI/CDを自動化するというもので、その取り組みに関して解説されていました。プルリクエストとも連動させるとなるといろいろと考えることがありそうでした。しかし、テーマ自体も面白かったのですが、やはり実際にKubeConで話すための一連の裏話的な内容がとてもおもしろかったです。実際にCFPを出す所から話されていたので、今後発表しようと言う方にオススメの内容でした。 3.3. Running VM Workloads Side by Side with Container Workloads - @amsy810VMもKubeVirtを利用してコンテナと同じようにKubernetesで管理するという話。これができると今までOpenStack上にKubernetesを構築してたものが、Kubernetes上でVMを立ててその上に本番用のKubernetesを立てる運用も可能になりそうでした。正直OpenStackは構築も運用も想像以上にツライのでこれは非常に魅力ある話です。ただもちろん現在ではまだそんなに簡単ではなくて少なくともストレージとネットワーキングには頭を悩ませそうな感じでした。 3.4. Keynote: Developing Kubernetes Services at Airbnb Scale - @jyoshiseAirbnbが「Kubernetesツライ」をどのように対処したのかの説明でした。Kubernetesの何がツライかと言うと、マイクロサービス一つ作るだけでも、開発、ステージング、本番環境の３つにデプロイが必要で、さらに一つの商用サービスをつくるためにはそのマイクロサービスを大量に作らないといけないため、そのたびに大量の設定(yamlファイル)を書き、大量のコマンド(kubectl)を打たなければならいからということでした。前者のことは「YAMLの壁」と呼ばれていました。 YAMLの壁をAirbnbがどう解決したかと言うと既存のツールはいまいちだったので独自の生成ツールを作ったとのことでした。また発表者の方がshowKsというサービスを作ったときには、テンプレートを作って変数を置換するようにしていたそうです。helm、kustomize、kapitan等のツールはAirbnbでは却下されていましたが、一般的にYAMLに押しつぶされそうになったらまずはこれらから検討するのが良さそうです。 後者のkubectlの問題は独自のラッパーを作って対処したそうです。ktoolと呼ばれていてk一文字のコマンド名ですが、これだけでも腱鞘炎が減りそうです(笑)。 3.5. Fly Your Containerized Environments by Joint Work of Harbor and Dragonfly - @capsmaltサイズが大きいコンテナイメージを大規模な分散環境で効率よく、スピーディーに配信するために、HarborとDragonflyがあるよという話。Harbarがイメージ管理で、Dragonflyがイメージ配布のOSSです。どちらもAlibabaで採用されていて中国を中心に流行っているみたいです。 3.6. Securing Kubernetes With Admission Controllers - @tkusumiKubernetesのセキュリティーポリシーをどうやって定義、管理するかという話でした。KubernetesにはデフォルトでAdmission Controllerというセキュリティの仕組みがあって、その拡張ポイントとしてValidatingAdmissionWebhookがあるけど独自でポリシーを作り込むのはツライから、Open Porlicy Agent(OPA)という汎用ポリシーエージェントを使っていこうというものです。OPAはRegoというDSLでポリシーを書けるので作り込みは楽そうでした2。実際にk8sと連携させるためにはkubernetes-policy-controllerが必要とのことです。 4. 食事鉄板のピザとビール。ノンアルもあるよ! ナポリの窯は初めて食べましたが、評判通り美味しかったです。 5. 感想とまとめ運用のツラさで感じることはみんな大体同じで、様々な取り組みやツールがエコシステムとして整備されつつあると感じました。まさしく「Kubernetesは退屈」だからこそ安心して取り組める、そう感じたMeetupでした。非常に有意義なMeetupだったのでまた機会があれば参加してみたいと思います。 Kubernetesは退屈 成熟してメインストリームで使われ始めた 運用面や周辺技術に関心がシフト GitOpsをk8sに適用 GitOps VM管理をk8sで行う KubeVirt YAMLの壁、kubectlの壁に立ち向かう helm, kustomize, kapitan, ktool イメージ管理とイメージ配信 Harbor, Dragonfly セキュリティ Open Porlicy Agent(OPA), kubernetes-policy-controller 1.Kubernetesはご存知かもしれませんが一応説明しておくと、もともとはGoogleが開発して2014年に公開して、現在はCloud Native Computing Foundationの管轄下にあるコンテナオーケストレーションシステムです。よくk8sと略されます。去年の段階で同様のツールやシステムを押さえてほぼ一強状態で、昨今の盛り上がりはすごく、これから到来すると予想されるクラウドネイティブ時代の中心的な役割を担っています。 ↩2.話を聞いた限り、Regoはいろいろとクセが強そうな印象を受けました。=が比較だったり代入だったりするなど・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/01/20/k8s-meetup-tokyo-15/"},{"title":"Go Modulesとマルチモジュール構成でGo Homeする方法","text":"Go Modulesでマルチモジュールにする方法がわからなくて調べました。発端は単にgo.modがある別モジュールのパッケージをインポートしようとしても出来なかったことです。そこで、Go Modulesでマルチモジュールを実現するためのシナリオを説明してみたいと思います。 TL;DR Go Modulesは便利なので使っていこう Go Modulesでマルチモジュール構成にする場合はgo.modファイルでreplaceディレクティブを使おう マルチモジュール構成の採用には慎重になろう 1. 目次1. 目次2. Go Modulesとは3. Go Modulesで管理を始める3.1. まずは基本のおはようの挨拶から3.2. おはようの挨拶をパッケージにしてみる3.3. Go Homeしようとして失敗する3.4. replaceのおかげでGo Homeに成功する4. なぜ、マルチモジュール化したかったのか？5. まとめ6. 参考文献 2. Go Modulesとはとりあえず、Go Modules is 何？という方の為に簡単に説明します。ご存知の方はこの節を飛ばしてください。 Go ModulesはGo 1.11から試験的に導入され、Go 1.13からデフォルトで有効になる予定の新しいパッケージ依存関係の管理方法です。使ってみた実感としてはすでに充分実用的なので新規にプロジェクトを作成する場合はGo Modulesを使って作成することをオススメします。Go 1.12でGo Modulesを有効にするためには、GOPATH以外のパスで作業をするか以下で環境変数を設定します。 1export GO111MODULE=on この記事ではGo 1.12の前提で解説します。 3. Go Modulesで管理を始める基本はディレクトリを作成してgo mod init で始められます。 123$ mkdir go-multi-modules$ cd go-multi-modules$ go mod init go-multi-modules go.modというファイルが作成されています。これが依存関係を管理するファイルになります。 go.mod123module go-multi-modulesgo 1.12 3.1. まずは基本のおはようの挨拶から早速ですが、基本どおりHello Worldから初めて見ます。ただし、依存関係を入れるために go-figureを利用して挨拶をしてみます。ソースコードは以下の通りです。 main.go12345678910package mainimport( \"github.com/common-nighthawk/go-figure\")func main() { myFigure := figure.NewFigure(\"Hello World\", \"\", true) myFigure.Print()} go buildをすると依存関係があるパッケージがダウンロードされて、ビルドされます。事前にgo getする必要がないので、これだけでもGo Modulesの良さがわかります。./go-multi-modulesで実行して無事挨拶ができれば成功です。 12345678$ go buildgo: finding github.com/common-nighthawk/go-figure latest$ ./go-multi-modules _ _ _ _ __ __ _ _ | | | | ___ | | | | ___ \\ \\ / / ___ _ __ | | __| | | |_| | / _ \\ | | | | / _ \\ \\ \\ /\\ / / / _ \\ | '__| | | / _` | | _ | | __/ | | | | | (_) | \\ V V / | (_) | | | | | | (_| | |_| |_| \\___| |_| |_| \\___/ \\_/\\_/ \\___/ |_| |_| \\__,_| go.modファイルを見てみるとrequireの行が追加されて依存関係が追跡されているのが分かります。 go.mod12345module go-multi-modulesgo 1.12require github.com/common-nighthawk/go-figure v0.0.0-20190529165535-67e0ed34491a また、go.sumというファイルも生成されます。依存関係の管理はgo.modだけでもできますが、go.sumは検査用に必要なようです。詳しくは ここを参照してください。 go.sum12github.com/common-nighthawk/go-figure v0.0.0-20190529165535-67e0ed34491a h1:kTv7wPomOuRf17BKQKO5Y6GrKsYC52XHrjf26H6FdQU=github.com/common-nighthawk/go-figure v0.0.0-20190529165535-67e0ed34491a/go.mod h1:mk5IQ+Y0ZeO87b858TlA645sVcEcbiX6YqP98kt+7+w= 3.2. おはようの挨拶をパッケージにしてみるさて、挨拶は毎日するものです。せっかくなので再利用可能なようにパッケージとして分離してみます。pkgディレクトリを作成し1、その下にhello-worldディレクトリを作成して、その下にhello-world.goファイルを作成します。ディレクトリ構成は以下の通りです。今回はhelloworldというパッケージを作成します。 ディレクトリ構成12345678.├── go-multi-modules // `go build`で生成された実行ファイル├── go.mod // `go mod init` で生成されたモジュール管理ファイル├── go.sum // `go build`で生成されたモジュール管理ファイル（検査用）├── main.go // メインファイル└── pkg └── hello-world └── hello-world.go // 新規追加 hello-world.goファイルの中身は以下の通りです。 hello-world.go12345678910package helloworldimport( \"github.com/common-nighthawk/go-figure\")func HelloWorld() { myFigure := figure.NewFigure(\"Hello World\", \"\", true) myFigure.Print()} main.goファイルは以下のように書き換えます。 hello-world.go123456789package mainimport ( \"go-multi-modules/pkg/hello-world\")func main() { helloworld.HelloWorld()} go buildでビルドして./go-multi-modulesで実行して同じように挨拶ができたら成功です。 3.3. Go Homeしようとして失敗するさて、挨拶も済んだのでもう用はありません。帰宅したくなってきたとします。ただし、帰宅時間まで細かく管理されたくないので別モジュールで管理することを考えます。この場合、pkgディレクトリ配下にgo-homeディレクトリを作成して、go-homeディレクトリに移動してからgo mod init gohomeを実行します。ディレクトリ構成は以下のようになります。 ディレクトリ構成1234567891011.├── go-multi-modules // `go build`で生成された実行ファイル├── go.mod // `go mod init` で生成されたモジュール管理ファイル├── go.sum // `go build`で生成されたモジュール管理ファイル（検査用）├── main.go // メインファイル└── pkg ├── go-home // このディレクトリ配下は別モジュールになる │ ├── go.mod // `go mod init`で生成される │ └── home.go // 新規追加 └── hello-world └── hello-world.go // 挨拶パッケージ go-home配下のgo.modは以下のようになります。初期化しただけなのでrequireはありません。 go.mod123module gohomego 1.12 home.goは以下のようになります。 home.go1234567package gohomeimport(\"github.com/common-nighthawk/go-figure\")func GoHome() { figure.NewFigure(\"Go Home!\", \"basic\", true).Scroll(30000, 400, \"right\")} main.goは以下のように書き換えます。 main.go1234567891011package mainimport ( \"go-multi-modules/pkg/hello-world\" \"go-multi-modules/pkg/go-home\")func main() { helloworld.HelloWorld() gohome.GoHome()} これをトップディレクトリ(go-multi-modulesディレクトリ)でgo buildでビルドしようとしたところ以下のようなエラーが出てうまくいきませんでした。どうやらモジュールの読み込みに失敗したようです。 12$ go buildbuild go-multi-modules: cannot load go-multi-modules/pkg/go-home: cannot find module providing package go-multi-modules/pkg/go-home 3.4. replaceのおかげでGo Homeに成功する解決方法は簡単で親のgo.modに以下のreplaceディレクティブを記述することでした。 replace go-multi-modules/pkg/go-home => ./pkg/go-home replaceディレクティブを記述してgo buildをするとビルドが成功します。以下はgo build後のgo.modです。依存関係(require)が追加されています。 go.mod12345678910module go-multi-modulesgo 1.12require ( github.com/common-nighthawk/go-figure v0.0.0-20190529165535-67e0ed34491a go-multi-modules/pkg/go-home v0.0.0-00010101000000-000000000000)replace go-multi-modules/pkg/go-home => ./pkg/go-home //追加 さて、ビルドできたら./go-multi-modulesで実行してみましょう。一瞬Hello Worldが表示されてその後Go Home!が実行されます。 我々はようやく成し遂げたのです（笑）。 4. なぜ、マルチモジュール化したかったのか？さて、ここまででマルチモジュール化の方法が分かったわけですが、問題の発端のなぜ自分がマルチプロジェクトにしたかったのかをまだ説明していませんでした。 理由としてはC言語のライブラリをビルドしてcgoで呼び出すモジュールを書いたのですが、makeでビルドする必要があったのでgitのサブモジュールでローカルに取り込もうとして、必然的にマルチモジュール構成になりました。ただ本家のFAQでは一つのリポジトリに一つのモジュールをススメているので、一般的にはマルチモジュールの採用には慎重になったほうがいいと思われます。 5. まとめ本記事ではGo Modulesにおける「マルチモジュール構成」に焦点を当てて、以下についてストーリ仕立てで解説しました。 Go Modulesを使って依存関係を管理する方法 パッケージに分割して呼び出す方法 マルチモジュール構成にする方法 ただし安易にマルチモジュール構成にしないほうがよい Go Home! する方法2 またこの記事を書くために作成したコードは以下に置きました。 この記事がGo Modulesを使ってモジュール管理を始めようという方、マルチモジュールで躓いた方の参考になれば幸いです。 6. 参考文献 Using Go Modules - The Go Blog (和訳) Modules · golang/go Wiki · GitHub Go Modules1.pkgディレクトリはGo Modulesを使う上で必須ではありませんが、ここではGoの標準レイアウトを採用しています。 ↩2.Go Homeはネタなので優しくスルーして頂けると幸いです。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/07/07/go-multimodules/"},{"title":"長年育て続けた秘蔵の一括ファイル変換ライブラリを晒してみる","text":"時々、複数のCSVファイルやJSONファイルを変換したい場面に遭遇します。そういうときはよくRubyで使い捨てのスクリプトを書いて済ませていました。しかし、複数のファイルを読み込む処理や行を分割して改行コードを取り除いたり等、同じ処理を何回も書いていることに気づきました。そして次第に「変換処理」だけに集中したいと思うようになり、自前のライブラリfileconvを作り始めました・・・ 目次1. はじめに2. インストール3. 使い方3.1. コンバータのフック3.2. コンバータの変数3.3. デフォルトのメタコンバータ3.4. メタコンバータを作ってみる4. リポジトリ5. fileconvの失敗談6. 最後に 1. はじめに本記事は拡張可能な一括ファイル変換ライブラリfileconvの紹介記事です。fileconvを使えばファイルのオープンや読み書きに手を煩わせることなく、簡単に複数ファイルの変換処理を実装することができます。またデフォルトでCSVやJSONフォーマットにも対応しており、他のフォーマットに対応するのも簡単です。 fileconv - RubyGems fileconv - GitHub また、記事の最後の方でこのライブラリを育てる途中の失敗談も紹介したいと思います。 2. インストール以下の行をGemfileに加えてください。 1gem 'fileconv' それから以下を実行してください。 1$ bundle もしくは以下のようにgemコマンドで直接インストールしてください。 1$ gem install fileconv 3. 使い方「コンバータ」を作成するには以下の2つをする必要があります。 MetaConvertor(例: Fileconv::Line)をincludeする 必要に応じていくつかのフック(e.g. input_ext)を定義する まずは簡単な例から紹介します。以下はテキストファイル(拡張子がtxtのもの)を選択して行番号を付加するコンバータです。 1234567891011121314151617require 'fileconv'class AddLinenoConvertor include Fileconv::Line def input_ext \"txt\" end def init_acc(acc) acc[:lineno] = 0 end def convert_line(line, acc) acc[:lineno] += 1 \"#{acc[:lineno]}: #{line}\" endend あとは以下のようにインスタンスを生成して#convメソッドを実行するだけです。このスクリプトを実行するとカレントディレクトリのテキストファイル(拡張子がtxtのもの)を選択して、カレントディレクトリの”output”ディレクトリ配下にファイルの変換結果(行番号を付加したもの)を同じファイル名で出力します。 12convertor = AddLinenoConvertor.newconvertor.conv つまり以下の2つのファイルがあるとすると、 aaa.txt123aaabbbccc bbb.txt123111222333 コンバータの実行後には以下のような2つのファイルが変換結果として生成されます。 output/aaa.txt1231: aaa2: bbb3: ccc output/bbb.txt1231: 1112: 2223: 333 3.1. コンバータのフック前出の例では最低限のフックしかオーバーライドしていませんでしたが、必要に応じて様々なフックをオーバーライドできます。フックを一つもオーバーライドしない場合のデフォルトのアクションでは、カレントディレクトリのすべてのファイルを”output”ディレクトリにコピーします1。 フック デフォルト 説明 input_dir “.”(カレントディレクトリ) 入力元ディレクトリ input_ext nil (全ファイル) 入力ファイルの拡張子 output_dir “output” 出力先ディレクトリ input_files(files) files 入力ファイル init_conv nil コンバータの初期化用フック init_acc(acc) nil アキュームレータ(acc)の初期化用フック read_file(filename, acc) nil (デフォルトのリーダ) ファイル読み込み用フック convert_line(line, acc) line 行変換用フック convert_file(file, acc) file ファイル変換用フック output_filename(filename, acc) filename 出力ファイル名変更用フック result_filename “result.txt” 結果ファイル変更用フック conv_result nil 変換結果出力用フック よく使われるフックは以下のとおりです。 #input_ext #convert_line #convert_file #conv_result #input_extは対象ファイルの拡張子を返します。オーバーライドしない場合のデフォルトはnilでこの場合は「全ファイル」が対象となります。ディレクトリは対象外です。このフックをオーバーライドして”csv”を返すと拡張子がcsvのファイルが選択されます。自分で選択ファイルを直接したい場合はinput_filesフックで上書きでききます。 #convert_lineフックは前述の例でも利用されていましたが、基本的に引数のlineをそのまま返せば「コピー」と同じ動作になります。そして行を変更したければlineを加工して戻り値として返せば行が変更されます。もし行を削除したければnilを返すか空の配列([])を返してください。行を増やしたい場合は必要な行を配列で返してください。 #convert_fileフックにはファイル全体\bに対する処理を記述します。引数のfileには読み込んだ一つのファイル全体のデータが入っているので2、これを加工して戻り値にします。 #conv_resultフックは複数のファイルを処理した後の一番最後に呼ばれるフックです。デフォルトではnilを返して何も出力しませんが、このフックをオーバーライドして文字列を返すと、それが”output/result.txt”に出力されます。出力先ディレクトリと出力結果のファイル名はそれぞれ#output_dirフックと#result_filenameフックで変更可能です。 3.2. コンバータの変数コンバータ内で利用できる主な変数はacc,@meta,@optsの３つで、全てHash型です。これらの変数はスコープを持っています。accはいくつかのフックの引数として渡されますが、スコープとしては一つのファイルを処理する間の共通の変数として使えます。そして一つのファイルの処理が終わると初期化されます。@optsと@metaはコンバータ全体で有効な変数です。 @optsはオプション引数を保持するのに使われます。オプションは#convメソッドの引数として渡されます。@metaはどんな目的にも使える変数として用意しています。一般的にはファイル処理全体に関わる情報を保持しておいて#conv_resultフックの出力用に利用します。 変数 スコープ 説明 acc ファイル 単一ファイル用の変数 @meta コンバータ 多目的変数 @opts コンバータ オプション用変数 3.3. デフォルトのメタコンバータメタコンバータは主にコンバータにincludeして利用されることを目的にしたコンバータです。fileconvにデフォルトで用意されているメタコンバータは以下のとおりです。 メタコンバータ モード 説明 Line 行 行の生データを取得 CSV 行 CSVの1行を取得(ArrayまたはRow) Data ファイル ファイルの生データを取得 File ファイル Fileオブジェクトを取得 Stat ファイル Statオブジェクトを取得 JSON ファイル JSONオブジェクトを取得 コンバータ(メタコンバータも含む)は「モード」を持っており、主に2つに分けられます。 ラインモード #convert_lineフックが呼ばれる #convert_fileフックが呼ばれる 例) Line, CSV ファイルモード #convert_lineフックが呼ばれない #convert_fileフックが呼ばれる 例) Data, File, JSON Lineコンバータはファイルを読み込んで改行コードで区切って#convert_lineに渡してくれるコンバータです。改行コードは指定がなければ維持されます3。@opts[:new_line]を指定することで明示的に改行コードを変換することもできます。 CSVコンバータはその名の通りCSV形式のファイルを扱います。Ruby標準のCSVモジュールを用いておりオプションもそのまま使えます。#convert_lineにはCSVモジュールでパースされたCSVの各行が渡ってくるので行単位で処理を行いたい場合はこのフックを利用してください。#convert_fileにはパースされたファイル全体が渡されるのでファイル単位で処理したい場合はこちらに処理を書いてください。 Dataコンバータはファイルの中身を全て読み込んで処理したい場合に利用します。データの中身は直接#convert_fileに渡されるのでここに変換処理を書くことができます。バイナリファイルの処理をしたい場合やファイルを自前でパースしたい場合に用います。 Fileコンバータはファイルは読み込まれず#convert_fileにFileオブジェクトが渡ってくるので直接ファイルを読み込めます。大きなファイルを分割して読み込んで処理したい場合などに用います。 Statコンバータもファイルは読み込まず、代わりにStatオブジェクトが#convert_fileに渡されます。ファイルの更新日時やサイズ等のメタ情報だけが必要な場合に用います。 一番最初の例はラインモードの例だったので次はファイルモードであるJSONメタコンバータの利用例を紹介します。 123456789101112131415161718require 'fileconv'class ModifyJSONConvertor include Fileconv::JSON def input_ext \"json\" end def convert_file(data, acc) data.map do |e| e[\"country\"] = \"USA\" e end endendModifyJSONConvertor.new.conv オリジナルファイル (address.json) : 1[{\"name\": \"Mike\", \"Age\": \"21\"}, {\"name\": \"Jon\", \"Age\": \"33\"}] 変換後のファイル (output/address.json) : 1[{\"name\":\"Mike\",\"Age\":\"21\",\"country\":\"USA\"},{\"name\":\"Jon\",\"Age\":\"33\",\"country\":\"USA\"}] さらに多くのサンプルはここで見ることが可能です。 3.4. メタコンバータを作ってみるメタコンバータは簡単に作ることができます。以下はfileconvジェムのJSONメタコンバータの例です。 12345678910111213141516171819202122232425require \"json\"module Fileconv module JSON include Fileconv::Base def pre_init_conv @opts[:read_json_opts] ||= {} @opts[:write_json_opts] ||= {} end def pre_convert_file(data, acc) ::JSON.parse(data, @opts[:read_json_opts]) end def post_convert_file(obj, acc) return unless obj if @opts[:pretty_json] ::JSON.pretty_generate(obj, @opts[:write_json_opts]) else ::JSON.generate(obj, @opts[:write_json_opts]) end end endend メタコンバータを作成するには「Fileconv::Base」をincludeしてコンバータで呼び出されるフックの事前(pre_)もしくは事後(post_)に呼び出される以下のフックを必要に応じてオーバーライドするだけです。 pre_init_conv post_init_conv pre_input_files post_input_files pre_init_acc post_init_acc pre_convert_file pre_convert_line post_convert_line post_convert_file pre_conv_result post_conv_result 4. リポジトリ以下のリポジトリで開発しています。バグ報告、ご要望はIssuesへどうぞ。プルリクエストも歓迎です。 このライブラリの今後についてですが、最近は複雑なコンバータを書くのをやめて単純なコンバータをつなげて処理を書くことが多くなっています。従ってそのうちその知見を生かしてfileconvにコンバータの「合成」を実装するかもしれません。 5. fileconvの失敗談fileconvを長年使い続ける中で色々と失敗を重ね、少しずつ改良していきました。以下は主な失敗した点です。 継承ベースにする メタコンバータを本当に「メタプログラミング」で実装する 例外を途中でキャッチする 1.は分かりやすいですが、最初は継承ベースで設計していたため、別の親クラスを持つクラスと一緒に使うことができなかったためMixinベースに変更しました。 2.に関してはメタコンバータをeval系やdefile_methodやsendやmethod_missingを使いまくって実装していた時期がありました・・・ あの頃は若かった・・・。 当時は「かっこいい」と本気で思っていましたが、普段使いのライブラリにとっては地獄以外の何物でもなかったです。どこでエラーが起こったのか分かり辛く、処理も追い辛いので実装して半年で全て現在のpre, postのフック形式に書換えました。メタコンバータの「メタ」はメタプログラミングで実装していた頃の残滓であり、自分への戒めのために残してあります。まぁ、一見すると「pre-」はダサいですがフックのライフサイクルが分かりやすくなったのと、コンバータを簡単にメタコンバータに昇格できるようになりました。つまり、最初はコンバータで書いたものを、ちょっと汎用的に使いたければフックをpreかpostに移すだけでメタコンバータに変換できるので非常に使い勝手がよくなりました。 3.に関してはファイル変換の途中で失敗したら例外をレスキューして続きのファイル変換を継続するような処理を入れたこともありました。しかし、fileconvは「コンバータ」や「メタコンバータ」を作る基盤なので薄いレイヤーに徹して例外処理は上位に委譲するのが正解だと使い続けて気づきました。 他にも色々失敗した点はありますが、上記の3つは似たような基盤ライブラリを作る上でも参考になるのではと思っています。 6. 最後にfileconvを作ってからファイルを開いたりファイルを書き戻したりする「余計な一手間」を考えずに済むようになり、書きたいと思った変換処理をすぐに書けるようになりました。ここ数年はライブラリのインターフェースも安定しており、シンプルに実装するのが一番だと実感しています。メタプログラミングコワイ・・・ この手の薄いユーティリティ系のライブラリにどれほどニーズがあるかは謎ですが、せっかく育てたので晒してみました。 ファイル変換の際に試して頂けると幸いです。 1.カレントディレクトリに\"output\"ディレクトリが無かった場合には作成します。\"output\"ディレクトリに同名のファイルが存在した場合は上書きします。 ↩2.fileに格納されるものは後述するメタコンバータの種類によって異なります。 ↩3.最初に読み込んだファイルの最初の改行コードが採用されます。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/09/11/fileconv/"},{"title":"ベン図を使って、インタプリタとコンパイラの違いを説明してみる","text":"長年インタプリタとコンパイラの違いの説明に苦慮してきました。その原因には言語と処理系を混同してしまったために起こる誤解や、処理系の内部と外部から見た差異に起因するものや、歴史的な背景によるものなど様々なものが含まれています。そのためインタプリタとコンパイラの定義を簡単に述べることは難しく、誤解が誤解を生む構造が出来上がっているものと考えられます。そこで本記事ではその状況を少しでも改善すべく、ベン図を使ってインタプリタとコンパイラの違いを説明してみたいと思います。 (本記事は「数式言語を使って、インタプリタとコンパイラの違いを説明してみる - Qiita」に触発されて書かれたものです。) 目次1. 前提知識2. ベン図による説明3. コンパイラ4. インタプリタ5. 視点、視座、視野による見え方の違い6. まとめ7. 参考文献 1. 前提知識まずは「インタプリタ」と「コンパイラ」をざっくり説明します。これらはプログラミング言語の「処理系」に紐付いた概念であり、簡単に言えばプログラミング言語の実装方式の違いになります。インタプリタは対象となるプログラミング言語を「解釈」して「実行」します。ここで言う解釈とはプログラミング言語の意味論に基づいて実行方法を決めることです。コンパイラはインタプリタとは異なり「実行」を伴わず、対象となる言語を別の言語に変換だけを行います。どのプログラミング言語も基本的には「インタプリタ」としても「コンパイラ」としても実装することができます1。 これだけ書くと簡単で明確な定義のように思えますが、実際には上記は「広義」の定義であり、現実を踏まえた分類にはもう少し工夫が必要になります。 2. ベン図による説明最初に図で説明しようとしたときにどのように表現しようか非常に悩みましたが、最終的には以下のようなベン図になりました。 なるべくわかりやすく書いたつもりですが、意図が読み取りにくい箇所もあると思うので簡単に説明したいと思います。 3. コンパイラまず、広義のコンパイラは単に異なるプログラミング言語の変換を意味しますが、一般的なコンパイラのイメージはCやJavaのコンパイラのように高級言語から低レベル言語(機械語もしくは機械語に近いバイトコード)への変換を行うものです。従ってこのイメージを「狭義のコンパイラ」の定義としています。 次に「トランスレータ」は同レベルのプログラミング言語に変換する処理系です。「同レベル」というのが曖昧ですが、区分としては機械語、バイトコード、それ以外の高水準言語の3つだけを考えれば大体あっています。アセンブラはアセンブリ言語(ニーモニック)を機械語に変換しますが、これはほぼ一対一の同じレベルの変換なのでトランスレータとみなすことができます2。パーサはこの図ではプログラミング言語を抽象構文木(AST)に変換する処理のことを指していますが、ASTをある種の形式言語と見なせばパーサもトランスレータに分類することができると考えています3。AltJS(TypeScript, CoffeeScript, Dart, PureScript, Elm, Scala.js, Opal・・・)の処理系もトランスレータの一種で「トランスパイラ」と呼ばれることも多いです。これらの処理系は変換先の言語がJavaScriptに固定されているためJavaScriptの代替言語(AltJSの由来)となっています。 4. インタプリタインタプリタは元々は初期のBASIC言語の処理系のように、言語を直接解釈して実行していました。従ってこのイメージのインタプリタを「狭義のインタプリタ」としています。現代の高級言語でこの方式で実装されている処理系はほとんどありません。なぜなら現代の高級言語では人間が扱いやすいように様々な「工夫」がされており、コンピュータが直接実行するのに向いていないからです。逆に高級言語ではなくJavaのバイトコードのような低レベル言語の言語処理系(JVM)は「バイトコードインタプリタ」とも呼ばれており、「狭義のインタプリタ」に当てはまります。 現代の高級言語の処理系をインタプリタとして実装しようと思った場合、ほぼ間違いなく間接解釈のインタプリタを実装することになると思います。「間接解釈」のインタプリタとは元のソースコードを抽象構文木(AST)やバイトコード等の実行しやすい中間表現に一旦変換して実行する方式です。PythonやRubyやJavaScript等のいわゆる軽量プログラミング言語(Lightwight Language/LL)の代表的な処理系はこれに該当します。すでにお気づきの方もいると思いますが、「中間表現」への変換には潜在的にコンパイラ相当の処理がインタプリタに実装されていることを意味します。それがベン図で広義のインタプリタや間接解釈の円がコンパイラと重なっている理由です。特に最近のインタプリタ処理系では「中間表現」に最適化の研究が進んでいるバイトコード形式が採用されることが多く、コンパイラ用に開発された技術をフルに活かして開発されています。 5. 視点、視座、視野による見え方の違いコンパイラ、インタプリタという区分は実は視点、視座、視野等のコンテキストが変われば見え方が変わります。例えば最近のRuby処理系(C言語で書かれたRubyVM)で説明すると、Ruby言語のユーザ視点からはCRuby処理系はインタプリタとして認識されます。それはCRuby処理系がRuby言語を解釈して実行しているように見えるからです。しかしCRuby処理系の内部に視点を移すと見える景色が変わってきます。CRuby処理系の中ではまず、Ruby言語を抽象構文木(AST)に変換する処理が行われており、そこからCRuby処理系独自のバイトコードに変換されます。その後CRuby処理系独自のバイトコードインタプリタが動作して生成されたバイトコードを実行します。このように内部から詳細に見ればRuby言語をASTに変換する「トランスレータ」とASTをCRuby処理系のバイトコードに変換する「狭義のコンパイラ」とCRuby処理系のバイトコードを実行する「狭義のインタプリタ」が動作していることがわかります。さらにバイトコードインタプリタの内部に視点を移すと、そこではJITコンパイラが動作していることも分かります4。今度は視野を広げてみるとRuby言語にはJVM上で動作するJRubyという処理系もあります。JRubyには事前コンパイラ(Ahead-Of-Timeコンパイラ)が搭載されており、Rubyスクリプトを事前にJavaのバイトコードに「コンパイル」しておくこともできます。 Javaの例も挙げておくと、Java 9からJshellが導入されており、これはいわゆるREPL(Read-Eval-Print Loop)なので、ユーザ視点からはインタプリタの処理系として見ることができます。またJava 11からjavaファイルを即時実行できるようになっているので、これは正しくインタプリタの挙動であり、今までRubyのような軽量プログラミング言語が得意としてきた実行方式です。 このように現在はプログラミング言語およびその処理系は大きく進化し複雑化、多様化しています。その結果インタプリタやコンパイラが入れ子になっていたり、多段構成になったり、複数の処理系をもっていたりするので、どのコンテキストから「コンパイラ」や「インタプリタ」を見ているかが非常に重要になってきます。つまり自分の視点からはインタプリタに見えていたものが他の視点からは別の見え方になることもあるので、話題に出す場合にはコンテキストに十分注意を払ったほうが良いと思われます。 6. まとめ本記事ではインタプリタとコンパイラの違いをベン図を使って説明してみました。まとめは以下のとおりです。 「インタプリタ」と「コンパイラ」とは言語処理系の実装方式のことであり、プログラミング言語とは紐付いていない プログラミング言語の処理系は基本的にインタプリタとしてもコンパイラとしても実装できる インタプリタは対象となるプログラミング言語を「解釈」して「実行」する コンパイラはインタプリタとは異なり「実行」は伴わず、対象となる言語を別の言語に変換することだけを行う インタプリタとコンパイラには歴史的な経緯により狭義と広義の定義が存在する ベン図参照 現代のプログラミング言語およびその処理系は大きく進化し、複雑化、多様化した結果、コンパイラとインタプリタが非常に複雑に絡み合っている コンテキストが違うと見え方が異なるので話題に出す際には十分注意すること またベン図は現代的なインタプリタとコンパイラを合理的かつ直感的に理解しやすいように整理しつつ、歴史的解釈も尊重して書いて見ました。円の重なりにも工夫をしていて上位層は下位層を含む場合があることを表現しています。 本記事が皆様のインタプリタおよびコンパイラのよりよい理解および、新たな気付きの一助になれば幸いです。 7. 参考文献 数式言語を使って、インタプリタとコンパイラの違いを説明してみる - Qiita インタプリタ - Wikipedia コンパイラ - Wikipedia トランスコンパイラ - Wikipedia Ruby 2.6.0 Released1.本記事ではノイマン型コンピュータで実行可能なプログラミング言語を想定しています。量子コンピューティングを考え出すとややこしくなるので本記事では触れません。 ↩2.概念的にそのように分類できるというだけで、実際にアセンブラを「トランスレータ」や「コンパイラ」と呼称することはありません。 ↩3.抽象構文木への変換は元のプログラミング言語の意味論を変更しないので、コンパイラよりもトランスレータに近いと感じましたが一般的な解釈とまでは言えません。 ↩4.Ruby 2.6からオプションでJITコンパイラ(Experimental)が利用できるようになっています。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/09/23/interpreter-compiler/"},{"title":"天下一キーボードわいわい会 Vol.3 に参加してきました。(フォトギャラリーもあるよ)","text":"この記事は キーボード #2 Advent Calendar 2019の２日目の記事です。天下一キーボードわいわい会 Vol.3に参加してきたのでその熱が冷めやらぬ内にレポートしようと思います。というかほぼフォトギャラリーです。写真を見て楽しんで頂ければと思います。 Adventarキーボード #2 Advent Calendar 2019 - Adventarhttps://adventar.org/calendars/4332キーボード Advent Calendar 2019 の2枚目です。 1枚目は[こちら](https://adventar.org/calendars/4117) こんにちは 2019年、遊舎工房のリアル店舗のオープンに始まり、キーボード・キーキャップ・スイッチのGBが頻繁に… 目次1. 日時・場所2. 天下一キーボードわいわい会とは3. フォトギャラリー4. 購入品5. 感想とか 1. 日時・場所日時と場所は以下のとおりです。南北線「六本木一丁目駅」より直結しています。開場は12:30からだったので1時間前に余裕を持って到着したのですが、すでに3列に折り返された長い待機列ができていました。 日時 2019年11月30日(土) 12:30～17:30 場所 六本木グランドタワー24F DMM.comグループ セミナールーム 参加費 500円 参加人数 270人程 待機列に並ぶ前にスマホのconnpassページで参加者であることをスタッフさんに確認してもらって、以下の参加証を受け取りました。 待機列に並んでいたのは5分程でその後誘導されて会場に向かいました。参加証のQRコード1をかざしてゲートを潜ってエレベータで24階まで移動です。エレベーターホールに着いたら、会場に入っていいのかと思いきや参加証の裏面の書かれた番号順に案内されました。エレベータは一気にたくさん人が乗れるので順番を崩さないために工夫のようです。ちなみに早く会場に入ったほうがキーボードの展示場所の確保や買い物に有利なので順番は大事です。 2. 天下一キーボードわいわい会とは天下一キーボードわいわい会はキーボード好きが集まる日本最大級の交流会です。いわゆる「自作キーボード」界隈の人たちが集まって来ますが、初心者やエンジョイ勢でも比較的入りやすいゆるふわな感じが特徴の交流会です。 一番の醍醐味は自慢のキーボードを好きなだけ持ち込んで展示できることです。特に申請とかは要りません。展示場所は早いもの勝ちですが、持ってきた人はどこかに必ず置けるだけのスペースはあります。そしていくつかのキーボードはキットとして販売もされているのでその場で買うこともできます。 connpass天下一キーボードわいわい会 Vol.3 (2019/11/30 12:30〜)https://tenkey.connpass.com/event/150626/# イベント概要 最近大きく盛り上がっている自作キーボード！ 天下一キーボードわいわい会（略称：天キー）はキーボード好きによるゆるふわ交流会です。 自慢のキーボードを持ってきて見せたり、キーボード好き同士で交流したり、国内のキーボード開発者から話を聞いたり、その場で自作キーボ… 創意工夫に溢れたキーボードをじっくり眺めて気に入ったものがあれば購入もできる、至福の体験をすることができます。その他にもそれはそうの公開収録や製作者寄りのセッション等もあり、入門者から上級者まで幅広い層が楽しめるイベントになっています。すでに「ほぼ週刊キーボードニュース #40 天下一キーボードわいわい会Vol.3 レポート！」がYouTubeで配信されているのでそちらを見るともっと雰囲気がつかめると思います。 お土産としてはTai-Haoから参加者に一つずつ6個入のキーキャプが配られた他に、イベントの最後に抽選(ゆかりさんとじゃんけん)で40人?くらいの方にキーキャップが当たりました。残念ながら自分はすぐに負けてしまいました。 3. フォトギャラリー参加レポートとして何を書くか迷いましたが、やはり言葉よりも見てもらった方が伝わると思ったので頑張ってフォトギャラリー(78枚)を作りました。回線スピードが遅い場合には読み込みにそこそこ時間がかかるのでご了承ください。クリックして写真を開くかマウスオーバーをすると一言コメントが読めます2。それではお楽しみください 😺 4. 購入品３万円が沼に消えました・・・ ほぼ一瞬です。 もちろん分かる人には分かると思うのですが、キットが２セットありそれ以外にも諸々の部品代がかかるので、予算を決めていくことをオススメします。 5. 感想とか実は自分は1年前の天下一キーボードわいわい会に参加していてこれで２回目です。1回目は頑張ってErgo42とIrisとPlanck Lightの３台を持ち込んで展示したのですが、今回は色々と忙しくて進捗が出ず何も持っていけませんでした。本当は何か設計して作ったものを持って行きたかったのですが、また今度頑張ってみたいと思います。 とりあえず天下一キーボードわいわい会は第1回目と変わらずゆるふわな雰囲気で安心しました。主催者のゆかりさん曰く、自作キーボードをまだ作ったことがない人が主役なので、少しでもキーボードに興味がある方は参加してみてはいかがでしょうか？ 本記事がキーボードに興味がある方の一助になれば幸いです。 (この記事はErgo42 with DSA COFFEE HOUSE KEYSET & Cherry MX Redで書きました。)1.QRコードは入場と退場が関連付けられているそうなので、参加証は無くさないようにする必要があります。 ↩2.一言コメントは、自分がノリと勢いで書いたものです。軽く読み流して頂ければと・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/12/02/tenkaichi-keybord/"},{"title":"Google好きの開発者が集結！GDG DevFest Tokyo 2019に参加してきました","text":"検索エンジン、Google Map、Android、Go言語、Kubernetes、GCP・・・ 派生技術やプロダクトを含めるとおびただしい量のGoogleの技術が私達の生活に入り込んでいます。こうなるともうGoogleの技術が好きかどうかに関わらずエンジニアとしてはGoogleとうまく付き合っていくしかないというお気持ちです。 ・・・というのは前置きで、自分はGoogleが好き🥰なので、Google好きな開発者たちが集まる祭典、GDG GevFest Tokyoに参加してきました。 tokyo.gdgjapan.orgGDG Tokyo | DevFest Tokyo 2019https://tokyo.gdgjapan.org/devfest2019DevFest は、Google Developer Group (GDG) コミュニティによって世界各地で開かれるデベロッパー向けイベントです。東京では、Android、Google Cloud Platform（GCP）、Web、Firebase、Machine Learni… 目次1. 日時・場所2. GDG DevFestとは3. セッションとホール4. 参加セッション(午前)5. ランチ5.1. GKEを本番運用する時に最低限考えるべきこととその方法(ランチセッション) @ 富永裕貴(クラウドエース)6. 参加セッション(午後)6.1. TensorFlow の使い方 〜 TF2.x とエコシステム 〜 @ 太田 満久(ブレインパッド)6.2. Goの10年の道のりとその変遷 @ 山口 能迪(Google)6.3. 令和元年Google機械学習技術総決算 @ 足立 昌彦(カブク)6.4. CloudNative 時代における GKE/Kubernetes ではじめる開発 @ 青山 真也(サイバーエージェント)6.5. BigQueryの今年の多すぎるアップデートを振り返ってみる @ なかむら さとる7. まとめ8. おまけ 1. 日時・場所日時と場所は以下のとおりです。参加したのは12/14日のセッションで、会場の電気通信大学は京王線調布駅から歩いて5分程でした。セッション開始より20分前の10:40頃に着いて受付を済ましました。 日時 2019年12月14日(土) 11:00～18:00 セッションと懇親会 2019年12月15日(日) 14:00～17:00 抽選で100名が参加可能なハンズオンおよびGoogleオフィス見学 場所 国立大学法人 電気通信大学 参加費 無料（ランチは500円、懇親会は3000円のオプション） 参加人数 1700人程 2. GDG DevFestとはGDGはGoogle Developer Groupsのことで、Googleの技術好きが集まったコミュニティで世界各地にあります。GDG DevFestはその各地のGDGが独自に主催する開発者のためのイベントで世界で年間400以上のDevFestが開催されています1。 DevFest Eventsより引用 今回の「GDG DevFest Tokyo 2019」はGDG Tokyoが主催をしており今年で4回目の開催になります。ちなみにDevDestはあくまでコミュニティが主体のイベントであり、Googleはサポーターとしての位置づけになっています。そのせいかビジネスよりのセッションやガツガツした宣伝多めのセッションは殆どなく、技術愛に溢れた温かみのあるイベントとなっています。 GDG DevFest Tokyo 2019のハイライトがすでに公開されていますので興味がある方はご覧ください。。 3. セッションとホールホールは３つに別れており、7つのトラックで並行してセッションが走っていました。面白そうなセッションやハンズオンがたくさんあったので、本当に分身したい気持ちでいっぱいでした（笑）。 B棟1階の出展ブースでは協賛企業が様々な展示をしたりノベリティを配ったりしていました2。面白い試みとしては出展ブースを見学したりセッションに参加するとシールが貰えて、シールを５枚集めるとノベリティと交換するというシステムでした。今まで出展ブースを回ってハンコを集めするようななものは見たことあったのですが、セッションも含めてシールを配っていたのは斬新でした。 4. 参加セッション(午前)午前の参加セッションは以下のキーノート２つです。 Keynote1: 世界に広がるGDGコミュニティとDevFest 鈴木 拓生(Google) Keynote2: 円周率世界記録への道 @ 岩尾 エマ はるか(Google) Keynote1はDevFestの由来や歴史とその盛り上がりについて解説されていました。DevFestは数年前から継続しているムーブメントでここ最近は爆発的な勢いで開催されているようです。さすがGoogle。 Keynote2は円周率世界記録(31.4兆桁)をクラウド(GCP)を利用して121日間かけて達成したお話でした。円周率の計算には既存のプログラムを利用したそうですが、数ヶ月に及ぶ計算が必要なため信頼性の確保や検算やモニタリングにいろいろと工夫をされていました。結果的に8ペタバイトの読み書きが発生して利用料金に換算すると20万ドルかかったことになったそうです3。ちなみに現在は利用料金が低下したので300万円ほど実行できるそうです。「後から見ると簡単に見える」という言葉が印象に残っていて、やはり世界一になる大変さと凄さとインパクトが伝わるエピソードでした。 あと、キャリアの話も興味深くて、Googleの採用に何度も落ちたけど受けるのタダだから毎年受けた話とか、英語の学び方とか色々と参考になりました。 5. ランチランチは事前に500円のチケットをネットで購入しておけばゲットできました(限定800食)。B棟前の広場には4台のキッチンカーが並んでおり、自分は一番人気の熟成ハラミステーキ丼の列に迷わず並びました。とにかく一番人気だけあってすごい行列で多分一番早く売り切れたんじゃないかと思います4。 熟成ハラミステーキ丼は本当に柔らかく美味しくて、500円で大丈夫か？と思ったくらいです。ただ、ちょうど一緒にランチをした同僚のバジルチキンセットも美味しそうだったので、願わくば全て味見をしてみたかったです・・・ 5.1. GKEを本番運用する時に最低限考えるべきこととその方法(ランチセッション) @ 富永裕貴(クラウドエース)ゆっくりランチを楽しみたかったのですがランチセッションも見たかったので、食べ終わったらすぐにセッション会場に向かいました。話された内容はKubernetes(GKE)のノードのアップグレード戦略でした。結論から言えばPodDisruptionBudgetリソースはほぼ必須でmaxUnavailableを適切に設定することでサービスの継続性を保ったまま安全にノードのアップグレードができるというものです。以外だったのはそのPDBをGKEが無視する場合があるということで、そういったエッジケースの検証の話が聞けてとても興味深かったです。 6. 参加セッション(午後)さて、昼食後しばらくしたら必ず眠くなってします体質の自分にとってはここからが修羅場です。フリスク片手に歯を食いしばって参加しました。 6.1. TensorFlow の使い方 〜 TF2.x とエコシステム 〜 @ 太田 満久(ブレインパッド)午後一のセッションは楽しみにしていたTensorFlowです。TensorFlowはGoogle発の機械学習のプラットフォームで、最初はディープラーニング向けのライブラリとして注目されましたが、最近は推論フェーズにも力を入れており機械学習の総合プラットフォームに進化しています。このセッションではTensorFlowの詳細というよりは、TensorFlowの地図を見ながら全体を俯瞰する内容でした。 まず印象に残ったのは機械学習には訓練フェーズと推論フェーズがありエンジニアには訓練フェーズに注目が行きがちですが、実際の機械学習の利用シーンでは推論フェーズが重要だという話でした。そしてTensorFlowはまさにその領域にも十分力を入れていて、「TensorFlow Serving」、「TensorFlow Lite」、「TensorFlow.js」の3つが担っています。 機能 説明 TensorFlow Serving サーバサイドで推論。gRPCやREST APIを簡単に作成できる TensorFlow Lite モバイル上で推論。閉域や遅延に厳しい場面で利用 TensorFlow.js ブラウザ上で推論。見た目の良いモデルが多い 特に面白そうだと思ったのがTensorFlow.jsで、ブラウザでぬるぬる動くようなモデルもあるので試してみたいと思います。また、実際に機械学習を始める際にアルゴリズムや統計学から入る人も多いと思いますが5、TensorFlow Hubで既存のモデルを試してみたり、Papers With Codeで最新の研究成果でどこまで実現できるかを確認したりすることがオススメだそうです。またAutoMLは特に機械学習の知識がなくても簡単に画像分類等ができるので試してみたいと思います。 6.2. Goの10年の道のりとその変遷 @ 山口 能迪(Google)Go言語の進化の歴史を丁寧にまとめたセッションでした。やはり一番のターニングポイントは目立たないですがセルフホスティングをするようになった1.5だと個人的には思っています。セルフホスティングによってコンパイラ開発自体が言語の利便性と信頼性のベンチマークになるのでインフラ周りに使われることが多いGo言語にとっては重要な資質だと考えています。 またGo言語は最近教育や普及にも力を入れて以下のようなサイトが公開されています。 go.devgo.devhttps://go.devGo is an open source programming language that makes it easy to build simple, reliable, and efficient software. 6.3. 令和元年Google機械学習技術総決算 @ 足立 昌彦(カブク)このセッションはタイトル通り、この一年のGoogle機械学習の振り返りでした。主なトピックとして紹介されていたのはTF Lite、AutoML、TF 2.0、TensorFlow Hubです。 このトピックのなかで個人的に一番印象的だった出来事はやはりTF 2.0のリリースでした。Keras APIの統合と既存APIの整理によって初学者にも十分オススメできる環境になったと思います。TF 2.0に関しては自分も「MinikubeとTensorFlow 2.0でGPUを使ってCIFAR-10」という記事を書いているので興味がある方はご覧ください。 6.4. CloudNative 時代における GKE/Kubernetes ではじめる開発 @ 青山 真也(サイバーエージェント)Kubernetesで有名な青山さんのセッションです。CKAD(Certified Kubernetes Application Developer)の資格を世界で2番目に取った方です。自己紹介の後いきなり「インスタ映えする手乗りクラスタ持ってますよね？」的な煽りで始まって苦笑しました。 話の内容はCloud NativeとKubernetesの総合的な解説でした。「Kubernetes != Cloud Native 」の式は重要でKubernetesを使うだけでCloud Nativeになるわけではなく、VMでもCloud Nativeは可能なので以下のCloud Nativeの本質を理解することが大切だそうです。 OpenかつScalableなシステムの実現 疎結合なシステム 回復性がある 管理しやすい 可観測である 堅牢な自動化により、頻繁かつ期待通りに最小限の労力で大きな変更が可能 Kubernetesについては青山さんの著書である以下がおすすめです。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"Kubernetes完全ガイド (impress top gear)\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/m.media-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/51bfBL9XdyL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/dp\\/4295004804\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"eirB2\",\"s\":\"s\"});リンク 6.5. BigQueryの今年の多すぎるアップデートを振り返ってみる @ なかむら さとるBigQueryは簡単に言えばGoogle版のDWHです。Googleのクラウドサービスの最初期からあるサービスですが、まだまだアップデートはたくさんあり成長過程にある活発なサービスのようです。 普段はAWSを使っていてもBigQueryだけは使うという事例もよく聞くのでBigQueryには今後も注目していきたいと思います。 7. まとめGDG DevFestは初参加でしたが、セッションの内容も会場の雰囲気もとても良く素晴らしい一日でした。 クラウドでいうとAWSが抜き出ており、ビジネスではAzureが追い上げている状況でGoogleはなんとなく存在感が薄かった印象ですが、言語やフレームワークのトータルでみるとまさしくGoogleの技術はいたるところで社会を支えているということをあらためて実感できました。そして、そのGoogleの技術の取り巻きには熱狂的なファンを中心にGDGが形成され、さらにファンを増やしていく好循環が世界中でムーブメントが発生しているということはGoogleの技術はこれからも支持され、社会を支えていくという何よりの証左だと思います。その中で自分はこの巨人の肩に乗っていかに価値を提供していくのかを考え続けなければいけないという思いを胸にしました。 最後になりましたが関係者、参加者の皆様、本当にお疲れさまでした。本記事がGDGの活動およびDevFestとGoogleの技術に興味がある方の一助になれば幸いです。 【GDG Devfest Tokyo 2019】これにて全てのプログラムが終了しました！ご参加頂きました皆様、本当にありがとうございました🙏また来年お会いできる事を楽しみにしています。今後も #gdgtokyo のイベントに是非ご参加下さい😊良いお年を！ #DevFest19 #devfest #GDG pic.twitter.com/6fQyMNWkLC— GDG Tokyo (@gdgtokyo) December 15, 2019 8. おまけ以下が戦利品です。中央のお米が異彩を放っています(笑)。 1.DevFestの詳細はここ ↩2.お米がノベリティになる日が来るとは・・・ ↩3.もちろんこれはGoogleの社内プロジェクトなのでタダです。 ↩4.「熟成ハラミステーキ丼」以外には「南インドカレー」、「ビーフストロガノフ 温泉卵添え」、「バジルチキンセット」のキッチンカーが並んでおりビーガンメニューを用意している所もありました。 ↩5.自分も最初アルゴリズムや線形代数、微積分、統計学から初めて挫折しかけました・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/12/20/devfest_tokyo/"},{"title":"ブログカードを支える技術","text":"ブログカードとは以下のようにリンクをちょっとリッチに表示してくれる機能のことです。以前はこれってどうやって実現しているんだろうと不思議に思っていました。 上記のようなブログカードは「はてなブログ」や「WordPress」等のブログサービスでよく見かけますが、基本的にこれらのブログカードはリンク先のURLを指定するだけで自動的に生成されています。本記事では上記のようなブログカードを支える技術について解説します。 目次1. はじめに2. ブログカードの要素技術2.1. ブログカードの構成2.2. ブログカードの主な情報源2.3. Open Graph Protocol2.4. favicon2.5. favicon取得用API2.6. ソーシャルカウント3. ブログカードの実装3.1. HTMLの骨組み3.2. JavaScriptにおける実装3.3. 実行結果4. まとめundefined1. おまけ（1/24追記）1.1. 参考文献 1. はじめに本記事では、ブログカード1の表示に使われる一般的な技術の解説およびJavaScriptによる実装を行います。普段何気なく見たり使ったりしているブログカードの技術に興味がある人におすすめします。 2. ブログカードの要素技術まずはブログカードを実現するための要素技術について解説します。 2.1. ブログカードの構成ブログカードは主に「タイトル」、「説明」、「画像」から成ります。オプションで「favicon」、「サイト名」、「ソーシャルカウント」を表示する場合もあります。 2.2. ブログカードの主な情報源ブログカードの主な情報源は以下の３つになりますが、メインの「タイトル」、「説明」、「画像」といった情報を提供しているのはOpen Graph Protocolになります。 Open Graph Protocol(OGP) favicon ソーシャルカウント 2.3. Open Graph ProtocolOpen Graph Protocolの説明の前提知識として、まずはソーシャルグラフについて説明します。ソーシャルグラフとはFacebookやTwitter等のSNS（ソーシャル・ネットワーク・サービス）において、人と人の繋がりである「ソーシャル・ネットワーク」を点と線で可視化したものです。「Graph」の由来は、点と点の結びつきに関する数学理論であるグラフ理論から来ています。 上記のようなソーシャルグラフの構築には点と点をつないで線にする仕組みが必要です。そのために考案されたのがOpen Graph Protocol(OGP)です。ソーシャルグラフにおける「点」は「人」を表していますが、Webの世界ではWebページを「人」とみなしてソーシャルグラフを構築します。WebページはHTMLで記述されHTTPプロトコルを用いてやり取りされますが、基本的にはOGPもその仕組みの上に成り立っています。具体的には以下の図のように単純な仕組みでメタデータのフォーマットのみがOGPで規定されていて、それ以外は既存のHTTPやHTMLの仕組みをそのまま利用しています。 OGPで必須とされているメタデータはog:title、og:type、og:image、og:urlの４つですが、og:site_nameやog:descriptionもよく利用されます。具体的なHTMLのヘッダに埋め込まれたメタデータの例は以下のようになります。 123456 2.4. favicon「favicon」はWebサイトのシンボルとして表示される画像のことでブラウザのタブやブックマークで表示されます。もともとInternet Explorer 5で「お気に入り」に画像を表示するための技術で、「favicon」の由来は「favorite icon」だとされています。 faviconの画像形式には特に決まりがなくブラウザ依存ですが、伝統的にはICO形式です。ICO形式はWindowsのアイコン形式で、以下のように正方形の任意の画像サイズを複数格納できるようになっています2。 Webサイトにfaviconを設定する伝統的な手法はWebサイトのルートディレクトリにfavicon.icoというファイル名でICO形式のファイルを配置することです。しかし近年のブラウザではそれ以外にも以下のようにHTMLのヘッダでfaviconを指定することもできます。 1 MIMEタイプを指定することによってgifやpngといった画像形式にも対応できます4。 1 2.5. favicon取得用APIfaviconはこれまで説明してきたとおり画像形式も配置場所もばらばらなので、単純にブログカードに表示させることはできません。JavaScriptでもある程度はできるかもしれませんがICO形式等のマルチ画像のフォーマットがあると厳しいです。そこでブログカード用にfavicon取得用のAPIをサーバサイドで実装するのが一般的です。favicon取得用APIではfaviconを取得してブログカードの表示に適切は画像フォーマットとサイズに変換してクライアントに返却します。このようなAPIは自作することも公開されているサービスを利用することもできます。以下にfavicon取得用APIの例を掲載します3。 GoogleのAPI 書式 http://www.google.com/s2/favicons?domain= 例 http://www.google.com/s2/favicons?domain=www.google.co.jp HatenaのAPI 書式 http://favicon.hatena.ne.jp/?url= 例 http://favicon.hatena.ne.jp/?url=https://hatenablog.com/ 2.6. ソーシャルカウントソーシャルカウントは一般的にSNSにおける「人気」を表す指標のことで、例えばはてなブックマークの数とかFacebookのいいねの数になります。ソーシャルカウントを取得する方法は、サービスによってそれぞれ異なります。具体例として以下にはてなブックマーク数を取得するAPIを掲載します。 はてなブックマークの取得API 書式 https://bookmark.hatenaapis.com/count/entry?url= 例 https://bookmark.hatenaapis.com/count/entry?url=http%3A%2F%2Fwww.hatena.ne.jp%2F 3. ブログカードの実装今回は以下のようなブログカードをサーバサイドJavaScript(Node.js)で実装してみたいと思います。 JavaScriptでHTMLを出力するイメージです。今回はgetTag関数を実装し、戻り値はPromiseとします。利用方法は以下のとおりです 1getTag({url: \"https://hinastory.github.io/cats-cats-cats/2019/12/29/visualize_ruby_development_by_file/\"}).then(e => console.log(e)) 3.1. HTMLの骨組みまずはHTMLを出力するにあたって骨組みを考えます。最初は出力するイメージを再現できる素直な入れ子構造を考えます。ポイントはブログカードをクリックしたらリンク先に飛びたいのでリンクを示すaタグでなるべく広く囲むことです。次にスタイル（CSS）を当てることを考えて不足しているレイヤーがあれば調整します。最終的にできた骨組みは以下になりました。aタグと書かれた箇所以外は全てdivタグで、class属性の骨格を示しています。 hbc-blog-card hbc-link-wrap hbc-link(aタグ) hbc-card hbc-info hbc-favicon hbc-site-name hbc-contents hbc-thumbnail hbc-text hbc-title hbc-url hbc-description 3.2. JavaScriptにおける実装実装は以下のとおりです。基本的には上記の骨格どうりにHTMLタグを組み立てているだけです。今回はソーシャルカウントの実装は行っていませんが、実装はそれほど難しくはないはずです。 blog_card.js12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879const util = require('hexo-util');const ogs = require('open-graph-scraper'); // Open Graph Protocol解析用const escapeHTML = require('escape-html');const url = require('url');const descriptionLength = 140;const className = 'blog-card';const faviconAPI = 'http://favicon.hatena.ne.jp/?url=$URL';function getTag(options){ return ogs(options) .then(function (result) { const ogp = result.data; const info = getInfo(options, ogp); const contents = getContents(options, ogp); const card = util.htmlTag('div', { class: 'hbc-card' }, info + contents, false); const link = util.htmlTag('a', { class: 'hbc-link', href: options.url, target: options.target, rel: options.rel }, card, false); const linkWrap = util.htmlTag('div', { class: 'hbc-link-wrap' }, link, false); const tag = util.htmlTag('div', { class: className }, linkWrap, false); return tag; }) .catch(function (error) { console.log('error:', error); return ''; });}function getInfo(options, ogp) { let name = ''; const urlParsed = url.parse(options.url); // ogSiteNameがなかった場合にホスト名を表示 if (ogp.hasOwnProperty('ogSiteName')) { name = ogp.ogSiteName; } else { name = urlParsed.hostname; } const siteName = util.htmlTag('div', { class: 'hbc-site-name' }, name); let api = faviconAPI.replace('$DOMAIN', encodeURIComponent(urlParsed.hostname)); api = api.replace('$URL', encodeURIComponent(options.url)); const favicon = util.htmlTag('img', { class: 'hbc-favicon', src: api } , ''); return util.htmlTag('div', { class: 'hbc-info' }, favicon + siteName, false);}function getContents(options, ogp) { let contents = ''; let text = ''; if (ogp.hasOwnProperty('ogImage')) { const image = util.htmlTag('img', { src: ogp.ogImage.url } , ''); contents = util.htmlTag('div', { class: 'hbc-thumbnail' }, image, false); } text += util.htmlTag('div', { class: 'hbc-title' }, escapeHTML(ogp.ogTitle), false); text += util.htmlTag('div', { class: 'hbc-url' }, options.url, false); if (ogp.hasOwnProperty('ogDescription')) { const description = adjustLength(ogp.ogDescription); text += util.htmlTag('div', { class: 'hbc-description' }, description); } contents += util.htmlTag('div', { class: 'hbc-text' }, text, false); return util.htmlTag('div', { class: 'hbc-contents' }, contents, false);}// 内容が長い場合に切り詰めるfunction adjustLength(description) { if (description && description.length > descriptionLength) { description = description.slice(0, descriptionLength) + '…'; } return description;}// 実行したい場合は以下のようにする// getTag({url: \"https://hinastory.github.io/cats-cats-cats/2019/12/29/visualize_ruby_development_by_file/\"}).then(e => console.log(e)) 3.3. 実行結果上記のプログラムの実行にはnode.jsとnpmによるライブラリ（hexo-utilとopen-graph-scraper)のインストールが必要ですが、興味がある方は実行してみてください。実行結果(HTML)は以下のとおりです。適当にスタイルを当てています。 See the Pen blog_card by hinastory (@hinastory) on CodePen. 4. まとめ本記事ではブログカードで使われている技術として以下の３つの技術について解説を行い、JavaScriptによるブログカードの実装例を紹介しました。 Open Graph Protocol favicon ソーシャルカウント 本記事がブログカードを支える技術の理解の一助となれば幸いです。 1. おまけ（1/24追記）記事本文では触れませんでしたが、ブログカードに使われる技術の一つとしてoEmbedというものもあります。これはWebサイトに動画や写真を埋め込むためのプロトコルで、YouTubeやInstagram等のリッチコンテンツを提供しているサイトが対応しています。一部のブログカードの実装ではWebサイトがoEmbedに対応していたらoEmbedを優先するという動作をします。 現在(2020年)時点では、oEmbedに対応しているサイトはOGPに対応しているサイトと比較するとごく一部であり、oEmbedに対応しているサイトも自分が調べた限りではOGPに対応していたので4、本記事ではOGPメインの構成にしています。oEmbedも非常に興味深い技術なので興味がある方は調べてみてください。 1.1. 参考文献 The Open Graph protocol Favicon - Wikipedia ICO (ファイルフォーマット) - Wikipedia ファビコン画像を取得する便利なWebサービス（API） | 俺の開発研究所 はてなブックマーク件数取得API - Hatena Developer Center1.リンクカードと呼ばれることもあります。 ↩2.ここではICO形式の一般的な特徴のみ説明しており、フォーマットの詳細については割愛させて頂きます。 ↩3.ここではfavicon取得用APIの説明用の参考例として掲載しています。おそらく非公式なのでご利用にはお気を付けください。 ↩4.YouTube、Twitter、Instagram、Flickr、Gyazo、Vimeo、Speaker Deck、SlideShareはOGPに対応していることを確認しています。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2020/01/23/blog_card/"},{"title":"サイレントでホットスワップなErgoDash miniを制作(後編)","text":"2018年のキーボード納めとして、年末にErgoDash miniを組みました。この記事はその後編にあたります。前編は以下になります。 サイレントでホットスワップなErgoDash miniを制作(前編) 目次1. 制作工程1.1. Pro Micro用のコンスルーの取り付け1.2. ホットスワップ用ベリリウムの取り付け1.3. スタビライザー、キースイッチ、Pro Microの取り付け1.4. Pro MicroにQMK Firmwareの書き込み1.5. 動作確認1.6. ケースの組み立て1.7. キーキャップの取り付け 〜O-ringによる静音化〜1.8. 完成!2. トラブルシューティング3. ホットスワップ化の長所と短所4. まとめ 1. 制作工程前回はUndergrow LEDの取り付けまで行いました。今回はいよいよPro Microとキースイッチのホットスワップ化に挑みます。 1.1. Pro Micro用のコンスルーの取り付けPro Microのホットスワップ化1ですが、これはなるべく行っておいたほうがよいです。理由は前述のとおりモゲる心配もありますし、初期不良でPro Microが動作しないこともあります。基盤にはんだ付けすると後で剥がすのが非常に大変です。Pro Microのホットスワップ化の方法は主に２つあって、コンスルー(スプリングピンヘッダ)を用いる方法と、ソケットを用いる方法です。今回はコンスルーを利用しました。コンスルーはピンの部分がバネのような構造になっており、基盤部分にはんだ付けしなくてもスルーホールと密着する仕組みになっています。ただし、コンスルーとPro Microのはんだ付けは必要なようです。コンスルー利用時の注意点としては、コンスルー自体に向きがあることです。 Helixビルドガイドより引用元 向きを間違えると基盤から抜けやすくなったり接触不良になりやすいので、Helixのビルドガイドを参考に取り付けをしました。 1.2. ホットスワップ用ベリリウムの取り付けここからがようやくキースイッチのホットスワップ化の話です。キースイッチのホットスワップ化する方法も主に２つあり、スルーホールにベリリウム銅を埋め込む方法とスイッチ用PCBソケットを利用する方法です。後者は基盤やケースの対応が必要になるので、今回は「みんな大好きベリリウム」2を使います。 ベリリウムは上記の写真のとおり非常に小さな金属です。写真右下のキースイッチに実際にベリリウムを嵌めていますが、上の方の足のベリリウムは非常に硬かったので最初下の足だけピンセットでベリリウムを嵌めて、もう一つのベリリウムは基盤のスルーホールに嵌めてからスイッチを垂直に挿すようにして嵌めました。このとき大事なのはベリリウムの穴に対してちゃんと狙って真上から力をかけることです。スイッチの足は折れやすいので斜めに力を掛けるとすぐに穴に入らずグニャリと逝ってしまわれます3。このベリリウムを挿す作業は非常に辛い作業で一部の人々には「田植え」とも呼ばれていますが、基盤から生え揃った金色の稲穂を眼前にすれば目頭が熱くなること必死です(笑)。 ErgoDash miniではこの段階で親指付近のキーの物理配置を決定しますが、せっかくホットスワップにしたのでキーの足が入る全てのスルーホールにベリリウムを埋め込んでおくことをオススメします。こうすることで後ではんだ付けすることなくキーの物理配置を変更することができます。実際にキーマップをいじっていると物理配置をいじりたくなる瞬間が何回かあってホットスワップにしておいてよかったと実感できました。 さて、この田植え作業が終わったと思って一息つけるかと思えば、そうは問屋が卸しません・・・組立工程の屈指の難関、ベリリウムのはんだ付けです。このはんだ付けの何が難しいかと言えば、基盤から顔を覗かせているベリリウムの高さがあまりないので、ベリリウムの穴にはんだが入らないように周囲にだけはんだを盛るのが大変なのです。単にスイッチの足をはんだ付けすればいいのであれば、一つの足に付き数秒で済みますが、ベリリウムの周囲をはんだが入らないように固めるとすれば、はんだを数回に分けて慎重に盛らねばなりません。自分の場合、普通にスイッチにはんだ付けするより5倍程の時間がかかりました・・・4 1.3. スタビライザー、キースイッチ、Pro Microの取り付けホットスワップにしたのでここからは基盤に挿すだけのお仕事です。ただスタビライザーを取り付ける場合には静音化のためにグリスをしっかり塗りましょう。自分は手元にあったシリコングリスを塗りました。オイル系だとあまり音を抑えられなかったのでここはグリスがいい感じでした。キースイッチは足が曲がらないように慎重にベリリウムを狙って挿すようにしましょう。Pro Microは基盤に挿すだけなのですが、Pro Microに着いているLEDは光量があって透明のアクリルケースだと拡散して結構目立ちます。気になる人は不透明な絶縁テープで目張りをすることをオススメします。下の写真では左が目張りをしていて、右が目張りをしていません。 USBの口と反対側にライトがあるのでここにテープを貼ります。この写真では赤色のLEDしか目張りしていませんが、緑色のLEDも気になる場合は全体に貼りましょう。 1.4. Pro MicroにQMK Firmwareの書き込みこの作業は公式のビルドガイドどおりで大丈夫です・・・といっても公式もリンクで実際はQMK Firmwareのページにもろもろ書いてあります。とりあえずGMK Firmwareの環境構築は終わっているものとして、骨子だけ説明します。ターミナルを開いて以下の手順で実施すれば大丈夫です。 12345cd ＜QMK Firmwareのトップディレクトリ＞cp -r keyboards/ergodash/mini/keymaps/default keyboards/ergodash/mini/keymaps/hinastory # デフォルトのキーマップをコピーして自分専用のものを作るemacs -nw keyboards/ergodash/mini/keymaps/hinastory/rules.mk # 「RGBLIGHT_ENABLE = yes」を追加emacs -nw keyboards/ergodash/mini/keymaps/hinastory/keymap.c # お好きなキーマップに変更sudo make ergodash/mini:hinastory # ファームウェアのビルドと転送。 上記で「hinastory」の部分は適当に好きな名前にしてください。自分のオリジナルのキーマップになるはずなのでユーザ名にしておくとわかりやすいです。emacsの部分はお好きなエディタに読み替えてください。Backlight LEDを有効にするためにはrule.mkに「RGBLIGHT_ENABLE = yes」を追加する必要があります。ファームウェアのビルドの転送に関してはコマンド実行後に以下のメッセージが表示されたら、PCと片側のキーボードをUSBケーブルでつないで、キーボードのリセットスイッチを押してください。この作業時にはTRRSケーブルは抜いておいてください。もう片側も同様にファームウェアの転送が必要です。 1Detecting USB port, reset your controller now... 1.5. 動作確認おめでとうございます！ここまでの作業でマスターにUSBケーブルをつなぎ、左右のキーボードをTRRSケーブル5で繋げはキーボードは光り、全てのキーが入力できるはずです。動作に問題があった方はご愁傷様です・・・ケースを組む前にトラブルシューティングにお進みください・・・6 1.6. ケースの組み立てケース組み立てはビルドガイドの通りにアクリルプレートをネジ止めすれば大丈夫です。問題はキーボードに傾斜をつけたい場合です。もともとErgodash miniにはPro Microのスペースを確保するために手前から奥にかけて軽めの傾斜が付いていますが、もう少し奥側を高くしたい場合はゴム足で調整するのが簡単です。 上記の写真右がキーボードの手前側で写真左がキーボードの奥側になります。手前に着いている小さいゴム足がキットに付属しているもので、奥側の少し大きめのゴム足が追加で購入したものです。ゴム足は色々なサイズのものがあるので好みの大きさを購入してください。Ergodash miniはコンパクトなキーボードなのでこのくらいの傾斜が自分にはちょうどよかったです。また、キーボードの中央と外側で高さを変えたい場合もゴム足の高さを変えることで調整できます。 1.7. キーキャップの取り付け 〜O-ringによる静音化〜いよいよ最後にキーキャップを取り付け作業です。自作キーボードの制作の中でも屈指の楽しい作業です。まずは手持ちのキーキャップを机に並べる所から始めましょう。自分はピラミッドを組みました(笑)7。キーキャップはキースイッチに適合したものであれば何でもよいです。一般的にはCherry MX互換スイッチが大多数なのでCherry MX対応のキーキャップから揃えることをオススメします。今回使ったGateron SilentスイッチもCherry MX互換のスイッチだったので、大量のキーキャップとキャッキャウフフしながら、キーボードの着せ替えを楽しみました。まさに至福のひとときです。 どのキーキャップをつけるか決めたら最後に静音化の一手間をかけます。前編で説明したO-ringです。今回はグループバイ(共同購入)した3mm O-ringを装着しました。O-ringの大きさや素材によって静音性は変わるので好みのものを選んでください。 取り付け方は簡単で上記の写真のようにキーキャップの裏の足に嵌めるだけです。2uのキーキャップには3箇所嵌める必要があります。 1.8. 完成!お疲れ様でした。完成です！ 2. トラブルシューティング今回自分が体験したトラブルはダイオードの挿し忘れ、LEDのはんだ不良、ベリリウムの穴にはんだ流し込み、ベリリウムのはんだ忘れ、スイッチの足にベリリムを嵌めようとしてスイッチの足を壊す、スタビライザーつけ忘れ、TRSケーブルとTRRSケーブルの間違い・・・書いてて自分はアホの子ではないかとおもうくらいやらかしています・・・orzただこれはみんな通る道だと信じています・・・以下のリンク先にこれらのトラブルの原因と対処方法がまとまっているのでご確認ください。ここに自分が体験したほぼ全てのトラブルが記載してありました。本当にいい情報なのでまとめてくれた人に感謝です。自分が自作キーボードを始めた頃にこのページはなかったのが悔やまれます・・・ Self-Made Keyboards in Japanキーボードを組み立てるときに陥るミスと対策 - Self-Made Keyboards in Japanhttps://scrapbox.io/self-made-kbds-ja/%E3%82%AD%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%92%E7%B5%84%E3%81%BF%E7%AB%8B%E3%81%A6%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AB%E9%99%A5%E3%82%8B%E3%83%9F%E3%82%B9%E3%81%A8%E5%AF%BE%E7%AD%96はじめてキットを組む人に是非一読して欲しいもの、としてまとめていきたい htomine シチュエーションごとになっていたほうがわかりやすい気はするので、項目別の内容をそちらに転化していけるといいかも。 これだけは確認して欲しいリスト とても多い間違い・ミスをピックアップしました。… 上記のページで解決しない場合はDiscodeで質問しましょう。「#ergodash」という専用チャンネルもありますし、自作キーボードの入門的な質問なら「#beginner-help」でもいいでしょう。親切な人が多いのできっと誰かが手を差し伸べてくれるはずです8。 ここからはトラブルの傾向に対する自分の感想ですが、やはりいちばんの関門はキットやパーツの入手性の悪さですね。人気のキットはすぐに売り切れますし、グループバイ(共同購入)の情報も様々な情報を注視していないと拾えないです。最近は大分改善されましたが、一昨年まではまともに組もうとすると海外通販を避けて通れない状態でした。しかし、去年はコミケやTokyo Maker Fairや技術書店にいけばキットを購入でき、Boothでも手に入りやすい状態でした。今年はもうすぐ遊舎工房さんの実店舗が秋葉原にオープンするらしいので、入手性に関してはさらに改善される見込みです。次の鬼門ははんだ付けですね。特にPro Microのはんだ不良は結構耳にします。またダイオードやPro Microを逆付けしてはんだを剥がそうとして基盤を壊す人もそこそこいます。またLEDチップの実装は阿鼻叫喚の様相を呈しています。 あとはQMK Firmwareですね。苦労のタイプは大きく２つあって、QMK Firmwareそのものに苦戦する場合とQMK FIrmwareにたどり着く前の前提知識で苦戦する場合です。前者はQMK Firmwareのどこをいじればキーマップを思い通りにできるか悩む場合でドキュメントを読んである程度試行錯誤しないといけないでしょう。後者はそもそもビルド環境の構築やGit操作やC言語やmakeの仕組みの理解が足りなくて苦労しています。自分は前者の苦労だけで済んだので比較的楽でしたが、後者で苦労している人も多そうですね。残念ながらこの辺の苦労話を書くのはここの余白は狭すぎます・・・9 3. ホットスワップ化の長所と短所最後にホットスワップ化の作業について、自分が感じたProsとConsをまとめてみたいとおもいます。 Pro Micoのホットスワップ化(コンスルー利用時) Pros モゲ対策になる 初期不良対策になる Cons コンスルーの購入にお金がかかる コンスルーの向きに注意が必要 スイッチのホットスワップ化(ベリリウム利用時) Pros スイッチを自由に変えられる ケースを後から変えられる キーの物理配置を後から変えられる(ErgoDash等の一部のPCBのみ) Cons ベリリウム高い・・・ ベリリウムの入手性が若干悪い 田植えが辛すぎる・・・ キーキャップを引き抜くときに、一緒にキースイッチも抜ける場合がある Pro Microに関しては特に言うことないですね。大抵の人にオススメできると思います。スイッチのホットスワップ化は本当に人を選びます。とにかくお金と慎重さと忍耐が必要です。一旦ホットスワップ化をしてしまえば当然最高の気分になれます。特に末永くキーボードを使っていくとなると、後でキースイッチを変えたり、lubeをやり直したくなったりしますがこれをいつでもできるという安心感はすごいですね。ケースもスイッチをはんだ付けするとトッププレートはかえられませんが、これもスイッチを引き抜けると簡単に交換可能になります。あと最初は意識していなかったキーの物理配置を後から簡単に変えられるのも地味に嬉しかったです。 キースイッチのホットスワップ化後の注意点には、キーキャップを引き抜くときにキースイッチも抜ける場合があります。これはケースやキーキャップの装着部分のきつさにもよるのですが、自分の場合何もしないと3つに一つはキースイッチも一緒に抜けます。対策としてはキーキャップ引き抜き器(針金タイプのもの)の一つを治具代わりにして、キースイッチの上部を押さえて、２本めのキーキャップ引き抜き器でキーキャップを抜いています。 4. まとめまず、静音化ですがこれは体感でかなり変わりました。もちろん人によって評価は変わると思いますが、静かな会議室で利用しても問題ないくらいにはなったと感じています。また打ち心地に関してもlubeのおかげで大分なめらかになって、疲れにくくなったと感じました。ホットスワップ化に関しては、特にスイッチの作業自体は苦労しましたが一度壁を超えてしまえば恐れることはないし、その後に得られるものも大きいので次回のキーボード制作でも採用したいと思います。 最初は本当に静音化とホットスワップ化のことだけについて書く予定でしたが、書いている途中で色々盛り込みたくなって結局、前半、後半に分けても長い記事になってしまいました。本当にここまで読んで頂いてありがとうございました。この記事が読んでくれた方の自作キーボードの一助となれば幸いです。1.ここで言うホットスワップ化とは、はんだ付けせずにパーツを交換できる仕組みを指しています。電源を入れたまま抜き差しできるという意味ではないです。詳しくはここを参照してください。 ↩2.出典 ↩3.この作業でスイッチを2個ダメにしました・・・ ↩4.一回失敗してベリリウムの穴にはんだを入れてしまいました。5倍の時間というのは失敗したベリリウムを基盤から剥がす時間込みです。やっちまった瞬間は地獄に落とされたような感覚でした・・・ ↩5.TRSケーブルだとマスターの反対側のLEDが光りません。片側しかLEDがつかなかったらケーブルを疑ってみてください。 ↩6.当然自分もトラブルシューティングに進みました・・・なのでトラブルシューティングの節は体験談です・・・orz ↩7.キーキャップを入れている容器はダイソーで買ったアルミキャップPET容器です。Seriaでも買えるようです。安いし軽いしサイズも色々あってキーキャップ入れにちょうどよかったです。 ↩8.自分も1台目のときにお世話になりました。このときははんだ不良で貼り付けた写真にいろいろな人にアドバイスを頂きました。 ↩9.ようこそキーマップ沼へ を書きました。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/01/10/ergodash-mini-2/"},{"title":"HexoのoEmbedプラグインをnpmに公開した話(前編)","text":"HexoにYouTubeなどのサイトを記事に埋め込むためのプラグインを作成しました。本当はYouTubeが埋込たかったわけではなくて、Speaker Deckのスライドが埋込みたかっただけなんですが、oEmbedを利用して汎用的な作りにしたので結果的にそうなってしまいました。ついでにもったいないからnpm1に公開しました。この記事はそのプラグイン制作記事の前編です。 npmhexo-oembedhttps://www.npmjs.com/package/hexo-oembedembed oEmbed item on your Hexo article. 目次1. 前提2. なぜHexoのタグを新しく作ろうと思ったか3. Hexoのタグの作り方4. Speaker Deckのタグを作ろうとして惨敗5. oEmbedとは5.1. hexo-tag-oembed現る5.2. 真打ちoEmbed Discovery5.3. 図解oEmbed5.4. 後編へと続く・・・ 1. 前提この記事はHexoのhexo-oembedプラグインの制作記事です。Hexoプラグインを作って見たい方、oEmbedの仕様に興味のある方、OSSへの貢献方法を一連の流れで知りたい方向けの記事となっています。hexo-oembedの使い方そのものが知りたい方は以下のページを御覧ください。 hexo-oembedを公開しました 早速製作記事に入りたいと思いますが、このブログで使われている静的サイトジェネレータのHexoのことを知らない方も多いと思います。この記事はHexoのプラグインの話なので知らない方は過去に書いた以下の記事を一読されることをオススメします。 cats cats catsHexoでブログに再入門https://hinastory.github.io/cats-cats-cats/2018/12/02/start-blog-hexo/Advent Calendar用に久々にブログを立てて見ました。以前にパブリックに書いたのは多分10年以上前のような気がします。レンタルーサーバ借りてドメイン取ってRubyで動く何かを使って立てた気がするけど、よく思い出せません・・・ まぁ、過去のことは置いておいてせっかく立てた… Hexoは記事をMarkdownで記述することができるのですが、マークダウンで記述できないリッチなコンテンツ(例えばYouTubeの動画)を表示させたい場合は、以下のような形式で独自のタグを作成して表示させることができます。 1{% tagName param1 param2 %} 作成したタグは自分で利用できるのはもちろんですが、Hexoはプラグイン機能を持っているので、プラグイン化してタグを再利用しやすくすることもできます。 2. なぜHexoのタグを新しく作ろうと思ったか冒頭でも書きましたが、Speaker Deckのスライドを貼りたくなったからです。具体的には以下の前回の記事で貼りたくなりました。 cats cats catsKubernetes Meetup Tokyo #15 - KubeCon 2018 Recap に初参加https://hinastory.github.io/cats-cats-cats/2019/01/20/k8s-meetup-tokyo-15/Kubernetes Meetup Tokyoに初参加しました。会場は六本木ヒルズのGoogle東京オフィスです。倍率2倍の抽選を潜り抜けて参加できることになりました。 connpassKubernetes Meetup Tokyo #15 - KubeCon 2018 Reca… Speaker Deckのような埋込みを意識してあるサイトでは概ね埋め込み用のHTMLを取得できるようになっていて、MarkdownにそのままそのHTMLを貼れば表示させることもできるのですが、やはり記事の書きやすさを考えるとスライドのパーマリンクから自動的に埋め込み用HTMLを展開してくれた方が望ましいわけです。 しかし、希望のタグは公式のタグプラグインには存在せず、Hexoのプラグインの中にも該当するものが見当たらなかったので、自作することにしました。 3. Hexoのタグの作り方公式のリンクにある通りタグ名を決めて展開したいHTMLタグを文字列で返すだけです。引数もargsに配列で渡ってくるのでオプション付きのタグを作るのも簡単です。ちなみに空白付きの引数を渡したいときはダブルクォートで囲ってあげれば大丈夫です。 公式より1234hexo.extend.tag.register('youtube', function(args){ var id = args[0]; return '';}); ディスクからファイルを読み出したり、インターネットから取ってきた情報をもとにタグを作りたい場合には、非同期処理を利用することがよくあります。その場合は以下のようにオプションでasync: trueを渡してあげてプロミス2を返すようにしてあげれば問題ありません。実は最初は一番目の文字列を返す方式で実装するつもりだったのですが、いろいろと問題があって結局このプロミス方式で実装することになります。 公式より1234567891011var fs = require('hexo-fs');var pathFn = require('path');hexo.extend.tag.register('include_code', function(args){ var filename = args[0]; var path = pathFn.join(hexo.source_dir, filename); return fs.readFile(path).then(function(content){ return '&lt;pre&lt;code&gt;' + content + '&lt;/code&gt;&lt;/pre&gt;'; });}, {async: true}); 4. Speaker Deckのタグを作ろうとして惨敗さて、Hexoのタグの作り方が分かったところで本命のSpeaker Deckのタグを作ってみることにしました。目標はスライドのパーマリンクから埋め込み用のHTMLを生成することです。そして適当なスライドのパーマリンクと埋め込み用のHTMLを比較してみて愕然としました・・・ パーマリンク1https://speakerdeck.com/lynnandtonic/art-the-web-and-tiny-ux 埋め込み用HTML1 お分かり頂けたでしょうか？最初の想定ではパーマリンクが分かれば文字列処理で埋め込み用HTMLを生成できるはずでした。しかし、蓋を開けてみるとdata-idなるどこぞの馬の骨ともわからないものがなければ生成できないではありませんか！ F●●K———- おっと、失礼。ついつい心の叫びがキーボードに伝わってしまいました。僕は悪くないよ。こうして、当初の目論見が破れて涙に暮れること3000年、突如救世主が現れました・・・ 5. oEmbedとは閑話休題。真面目な話、正直どうしようかと色々調べていたらSpeaker DeckがoEmbedに対応していることを知りました。oEmbedは簡単に言えば、パーマリンクを埋め込み用HTMLに変換するためのWeb API仕様です。仕様はオープンになっておりYouTubeやTwitterやFacebookなども採用していて、コンテンツを埋め込むための業界標準となっています。そしてパーマリンクから埋め込み用HTMLを生成するサービスの提供側をoEmbedのプロバイダというのですが、Speaker DeckはoEmbedのプロバイダであり、サービス提供の受付口であるエンドポイントと受付可能なパーマリンクのフォーマットを定義するスキーマを公開していました。ここからは公式のサンプルを引用しながら具体的に説明します。 サービス提供者 flicker パーマリンク http://www.flickr.com/photos/bees/2341623661/ oEmbedプロバイダ flickr URLスキーマ http://www.flickr.com/photos/* APIエンドポイント http://www.flickr.com/services/oembed 消費者 oEmbedリクエスト(HTTP GETリクエスト) http://www.flickr.com/services/oembed/?format=json&url=http%3A//www.flickr.com/photos/bees/2341623661/ oEmbedレスポンス(JSON形式) 123456789101112{ \"version\": \"1.0\", \"type\": \"photo\", \"width\": 240, \"height\": 160, \"title\": \"ZB8T0193\", \"url\": \"http://farm4.static.flickr.com/3123/2341623661_7c99f48bbf_m.jpg\", \"author_name\": \"Bees\", \"author_url\": \"http://www.flickr.com/photos/bees/\", \"provider_name\": \"Flickr\", \"provider_url\": \"http://www.flickr.com/\"} まずはサービス提供者とoEmbedプロバイダと消費者という、３つの登場人物がいることに注目しましょう。サービス提供者は文字通り埋め込み用コンテンツを提供する側です。この例では写真共有サービスのflickrになっており、埋め込みコンテンツを指定するためのパーマリンクを公開しているものとします3。 oEmbedプロバイダは前に説明したとおり、パーマリンクを埋め込み用HTMLに変換するための情報を提供します。ここでURLスキーマと見てみると、photosの後にアスタリスク(*)が指定されていることがわかります。これは任意の文字列を表していて、photos/以降の文字列がどんな場合でもパーマリンクはマッチすることになります。 最後に消費者にです。消費者は埋め込み用HTMLが欲しい人です。そのためにはパーマリンクおよび、パーマリンクを受け付けるoEmbedプロバイダの両方を知っている必要があります。消費者はoEmbedプロバイダに埋め込み用コンテンツを要求しますが、リクエストの構造は単純でHTTPのGETリクエストでエンドポイントを指定し、クエリとして最低限パーマリンクを指定するためのurlとレスポンス形式を指定するためのformatを指定するだけです。ここではformatはjsonを指定していますが、xmlを指定することもできます。oEmbedプロバイダはどちらか片方のフォーマットだけサポートしてもよいし、両方サポートしても良いことになっています。 レスポンスは見たままなのであまり説明することはありませんが、typeだけ付け加えるとphotoの他にlinkとvideoとrichがあります。photoは写真の埋め込みに使います。画像のURLをurlパラメータで返してくれるのでそのURLをimgタグで表示してあげればよいだけです。titleパラメータがあればalt属性に設定しましょう。 videoとrichの２つのタイプはについて説明するとこれらはhtmlパラメータを持っていていてこの中身を直接埋め込めばいいので一番ラクです。htmlの中身は経験上iframeタグになっていることが多いです。linkは面白くないしあまり使われているところをみたことがないので割愛します。 5.1. hexo-tag-oembed現るさて、ここまで調べて実はoEmbedに対応したタグならあるんじゃないかと思ってGoogle先生に相談したら、案の定みつかりました。 しかし残念ながら、このタグはお目当てのSpeaker Deckには対応していませんでした。しかもエンドポイントをソースコードに埋め込んでいたのでSpeaker Deckに対応してもらうためにはプルリクエストを送らないといけません。最初は素直にプルリクエストを送ろうかと考えていたのですが、oEmbedに対応しているサイトは大量にあるのでそのたびにプルリクエストを送るのもなんだかなぁーというお気持ちになりました。そもそもいちいちプロバイダー探してエンドポイントの定義なんて面倒くさいし何とかならないのかと、もう一度公式のドキュメントを眺めてみると、うん、ちゃんとおあつらえ向きな仕様があるではありませんか！ Discoveryです。 5.2. 真打ちoEmbed DiscoveryDiscoveryは、簡単に言うと前述のoEmbedリクエストをHTMLのヘッダに埋め込める仕様です。これも公式のサンプルを見てもらった方が早いでしょう。 公式より123 素晴らしい。先程の悩みは一瞬で解決しました。パーマリンク先がDiscoveryに対応していれば、まずHEADリクエストでヘッダだけ引っ張ってきて、linkタグのtypeがapplication/json+oembedまたはapplication/json+xmlなものを探して、href属性が示すURLにリクエストするだけでお目当てのoEmbedレスポンスが手に入ります。リクエストは一回増えますが致し方ないですね。しかし、ここまでするならいっその事ヘッダにoEmbedレスポンスを埋め込んでくれよ・・・とか思ってしまいますが、ここはグッと我慢します。頭でっかちな奴(HTML)は嫌われますからね。 5.3. 図解oEmbedここまでのまとめとしてoEmbedのシーケンス図を描いてみます4。ポイントはサービス提供者とoEmbedプロバイダは分かれてても良いということです。一般的には同じ場合が多いですが、複数のエンドポイントをまとめてくれるoEmbedプロバイダも存在します。embed.ly/とかが有名です。あとDiscoveryはoEmbedの仕様としてはオプションです。しかも対応しなければいけないのはサービス提供者側の各ページなので、oEmbedに対応しているページでもDiscoveryには未対応な場合は結構あります5。 5.4. 後編へと続く・・・ここにきてようやくHexoのタグを作成する準備が整いました。長かった・・・後編ではいよいよコーディングの話と公開に至るまでを紹介します。 HexoのoEmbedプラグインをnpmに公開した話(後編)1.npmはNode.jsのパッケージ管理システムです。Node Package Managerが由来ですがnpmのページの左上を見るとクリックするごとに色々はNPMに出会えます(笑)。現在ではJavaScriptのパッケージ管理システムのデファクトスタンダートとなっています。 ↩2.一般的に「プロミス」といえば非同期処理を扱うための手法または概念の一つですが、ここで言うプロミスとはJavaScriptの標準仕様であるECMAScript 6th Edition(ECMAScript 2015)で定義されているプロミスを指しています。JavaScriptの仕様に比較的最近入った仕様ですが最近のモダンブラウザでは問題なく利用することができます。またHexoが基盤として利用しているNode.jsでも標準仕様のプロミスがサポートされています。標準のプロミスについてはJavaScript Promiseの本によくまとめられているので参考にしてみてください。ちなみにJavaScriptのライブラリとして実装されている「プロミス」は標準仕様に準拠したもの以外にも星の数程あるのでそれらと混同しないようにしてください。有名なものではBluebirdがあり、これはHexo内部でも利用されています。 ↩3.一般的にはパーマリンクが用いられることが多いですが、oEmbedの仕様上は特にパーマリンクでなければならないという制約はありません。ちなみに例のパーマリンクのURLは公式のサンプルに乗っていたものであり、実際の写真の中身には自分は関知しておりません。あのパーマリンクを開くことによるいかなる被害および損害の責任はとれませんのでご了承ください・・・誰？ ↩4.このシーケンス図の作成にはPlantUMLを用いています。Hexoのタグとしてはhexo-filter-plantumlを利用しました。 ↩5.2019年2月現在、flickr、Gyazo、InstagramはoEmbedには対応しているけどDiscoveryには対応していなさそうでした。ちなみにoEmbedそのものに対応していないサイトとしては自分が試した限りニコニコ動画やAmazonやメルカリがありました。ショッピング系は物量も多いし入れ替わりも早いので仕方がないとしても、ニコ動には頑張ってほしかった・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/02/09/hexo-oembed-1/"},{"title":"Developers Summit 2019に参加した感想など","text":"Developers Summit 2019(#devsumi)に参加してきました。会場は目黒のホテル雅叙園東京です。去年は仕事の都合上参加できなかったのですが、今年はなんとか両日とも午後を空けて参加してきました。 Shoeisha EventDevelopers Summit 2019https://event.shoeisha.jp/devsumi/20190214ITエンジニアの祭典「Developers Summit 2019」（デブサミ2019）は、2019年2月14・15日に開催！ 目次1. 1日目 参加セッション1.1. Cloud Native時代における Docker / Kubernetes による開発1.2. いまなぜGoogle Cloud Platformを学ぶのか！？〜GCPを支えるGoogleテクノロジーと愛を語る〜1.3. 【祝】k8sデビュー！ エンタープライズ巨大アプリをマイクロサービス・コンテナ化。段階的移行(中)の全記録を追う。1.4. 「ITエンジニアに読んでほしい！技術書・ビジネス書大賞 2019」プレゼン大会2. 2日目 参加セッション2.1. 機械学習システムのアーキテクチャ アラカルト ～ BrainPad における実例を交えて～2.2. エンジニアの皆さんに贈る最速キャリア戦略2.3. これをまだ知らない Web エンジニアへ贈る - 私が愛する Elixir/Erlang の楽しさと辛さ -2.4. サーバーレスで最高に楽しめるアプリ開発2.5. 無意味なアラートからの脱却 〜 Datadogを使ってモダンなモニタリングを始めよう 〜2.6. Webアプリのチューニングバトル「（社内）ISUCON」の魅力と楽しさ3. 場外バトル4. まとめ 1. 1日目 参加セッション午後から参加しましたが、この日の目玉は何と言っても【14-G-1】のGCP特別セッションです。Googleの実際の開発手法やCI事情が聞けるなんてめったにないですから本当に楽しみにしていました。あと、なにげにこの日の最終セッションの技術書・ビジネス書大賞は世相を反映しているので毎年楽しみにしています。 1.1. Cloud Native時代における Docker / Kubernetes による開発 【14-A-4】 13:05～13:50 青山 真也[サイバーエージェント] Kubernetes完全ガイドの著者によるセッションです。1/10にあったKubernetes Meetup Tokyo #15の発表も見ていたので、今回も楽しみにしていました。発表内容はタイトルどおりクラウドネイティブやKubernetesの解説だったのですが、その構成が素晴らしかったです。マイクロサービスとサービスメッシュから始めて徐々にブレークダウンしてKubernetesの話題にもっていき、最後にGitOpsで締める流れは非常に分かりやすかったです。もしクラウドネイティブ\bやKubernetesにもやもやしている方はスライドを一読することをオススメします。 ちなみにクラウドネイティブ関連ではJapan Container DaysやKubernetes Meetup Tokyo #15の参加レポートも書いているのでよろしければそちらもどうぞ。 cats cats catsJapanContainerDays v18.12 に初参加https://hinastory.github.io/cats-cats-cats/2018/12/06/container-days/12/4(火)にJapanContainerDays v18.12に初参加しました。このイベントは春に1回目を開催して今回で2回目の開催だそうですが1、会場には800人以上の人が詰め掛けたようで非常に盛況でした。 JapanContainerDays v18.12JapanCon… cats cats catsKubernetes Meetup Tokyo #15 - KubeCon 2018 Recap に初参加https://hinastory.github.io/cats-cats-cats/2019/01/20/k8s-meetup-tokyo-15/Kubernetes Meetup Tokyoに初参加しました。会場は六本木ヒルズのGoogle東京オフィスです。倍率2倍の抽選を潜り抜けて参加できることになりました。 connpassKubernetes Meetup Tokyo #15 - KubeCon 2018 Reca… 1.2. いまなぜGoogle Cloud Platformを学ぶのか！？〜GCPを支えるGoogleテクノロジーと愛を語る〜 【14-G-1】 14:10～15:40 中井 悦司[グーグル・クラウド・ジャパン]、他 さて、本日一番楽しみにしていたGCP特別セッションです。さすがGoogleだけあってテーブルはコンセント付きで、Wi-Wiカードもあって、コーヒーとお菓子付きで、バレンタインデーだからかもしれませんがGCPを象ったチョコとクッキーも貰いました。至れり尽くせりですね。 本題のGoogleの開発環境に関するプレゼンですが、いろいろと予想外のこともあったので、手元のメモからなるべく詳細に書き起こしてみました。 共通ミドルウェアサービス 分散ファイルシステム、分散NoSQL、分散ロックマネージャ、分散XXX・・・ 単一リポジトリによるソース管理 Gitは使っていない １日あたり4万コミット、20億行のソースコード 全てのプロジェクトを一つのリポジトリに保存。プロジェクトごとにディレクトリに分けて管理 他のプロジェクトのコードも閲覧可能。任意のプロジェクトのコードをインポートリンク可能 Pros プロジェクト間のソースコード共有の促進 バージョン間の依存関係問題が起きにくい 大規模なリファクタリングが可能 Cons 専用ツールの開発・メンテが大変 ライブラリの依存関係の管理が大変 ブランチを持たない開発ツリー ソースコードの修正は全てメインラインに反映 リリースブランチのみ分ける（チェリーピックで必要な修正をメインラインから取り込む） 自動ビルドツールによるテスト駆動開発 Blaze(Bazelに似たビルドツールで依存関係と自動テストのルールを指定 コミット時の自動テストでは変更コードに依存する他の自動テストを全て実行 １日あたり1万3千個のプロジェクトにおける、約80万回のビルド処理と1億5000万回のテストを実施 自動テストの工夫 定期的に発生するチェックポイントでテストを実施(重複テストを削減) リソース効率は良くなるがテスト結果が遅れるのでストレスになる 依存関係が遠いテストの実行頻度を落とす ソースコードの更新プロセス リポジトリ全体を(仮想的に)ローカルにコピー ローカルで修正した内容をコードレビュー レビュー終了後にリポジトリにコミット レビュー前後に自動テストを実行 コードレビューシステム CL1ごとに変更部分をレビュー Critique(Gerrit2に類似のレビューシステム)上で実施 軽量なレビュープロセス レビューアのアサイン後、小さなもので1時間、大きなもので5時間いないにコメントを受け取る 開発者は平均して週3時間のレビューを実施 99%のCLでレビューアは5人以下、75%のコミットでレビューアでは1人 90%のCLで変更されるファイルは10以下、35%のCLで変更されるファイルは1つ 一つのコミットの変更行の中央値は24行、10%のCLで1行のみ変更 コードレビューの目的 他人が理解できるコードを書くこと これが絶対の基準。開発者の入れ替わりが早いのでこれが最重要。 理解できない場合は、「理解できない」と突き返せばよい 個人的な感想としては単一リポジトリとブランチを持たない開発ツリーは結構衝撃でした。勇気をもってメインラインに突っ込む！は大胆だなぁと思いましたが、後続の話を聞いて納得しました。要はローカルの修正結果を自動テストしてレビューしてもらうシステムがありきなんですね。一般的にはレビューしてもらうにはブランチを切ってコードレビューシステムと連動している共有リポジトリにプッシュしますが、Googleはコードをリポジトリに突っ込むことなくローカルのコードをレビューしてもらえるシステムがあるので、常にレビュー済みのソースのみがメインラインに突っ込まれるようです。まぁ、それでもメインライン一本の運用はすごいと思いますが・・・ あと、コードレビューの目的もシンプルでイイねをしたくなりました。コードレビューの目的を他人が理解できるコードを書いているかのみにすれば、それを徹底すればバグは明白になるし、開発者の入れ替わりでも問題が起きにくくなるので非常に効率的だと感じました。コードレビューのチェックリストをたくさん覚えるよりもこっちのほうが効果がありそうです。 上記のプレゼンは下記の公開資料にもとづくそうなので、いつか読んでみたいと思います。 Why Google Stores Billions of Lines of Code in a Single Repository Modern Code Review: A Case Study at Google Taming Google-Scale Continuous Testing Lessons from Building Static Analysis Tools at Google セッションの二本目はパネルディスカッションで、Googleエンジニアの経歴紹介やGCPとの関わり等を聞けて非常に興味深かったです。気になった話題で言えば、Javaの\"Write once, run anywhere\"は嘘3というパワーワードが出ました。なぜこの話題が出てきたかというとGCPがアプローチを変えてWrite once, run anywhereを達成したという矜持からだと思います。Googleのサービスは今や世界中で欠かせないインフラです。そしてGCPでコードを書けばGoogleのインフラに乗せて世界中に動くサービスを提供することができます。これは恐らくSunの達成したかった本質であり、GCPが誇っていい真実だと思います4。 あと情報収集の仕方も話題になりました。Googleエンジニアでもブログ5、SNS、RSS、SlideShar、Qiitaは一般的な情報源のようです。面白かったのはスマホのChromeでおすすめ記事の表示がトップ画面にでてきますが、これが意外と自分の興味にあった記事を勧めてくれるそうです。 1.3. 【祝】k8sデビュー！ エンタープライズ巨大アプリをマイクロサービス・コンテナ化。段階的移行(中)の全記録を追う。 【14-D-7】 16:20～17:05 石田 健亮[ドリーム・アーツ] 既存のJavaアプリをKubernetesを用いてマイクロサービス化する話でした。k8sを用いたモダナイズの事例としてとてもよい発表だったと思います。 旧環境 Angular,Velocity,Struts,俺々PersistenceAPI,MySQL 新環境 AKS, SpringBoot, kustomize, fluentd(サイドカーでログ収集、DaemonSetを使ってノード単位で立てる) 工夫した点 マイクロサービスは重い(実測で4倍遅い) キャッシュを利用 非同期、並列にする Netflix OSSをチェキラ デブサミ 2019 【祝】k8sデビュー！エンタープライズ巨大アプリをマイクロサービスコンテナ化 from Kensuke Ishida 1.4. 「ITエンジニアに読んでほしい！技術書・ビジネス書大賞 2019」プレゼン大会 【14-A-8】 17:25～18:45 密かに楽しみにしていたセッションです。やっぱり本を読むのって時間がかかるのでなるべくハズレは引きたくないものです。かといって売れているから自分にあっているとも限らないので悩ましいところですが、ここでは著者または関係者が読みどころをプレゼンしてくれるので、非常に参考になります。今日行われたプレゼンも非常に熱がこもっていて、実際に自分が投票した2冊は絶対に買おうと思いました6。 ShoeishaITエンジニアに読んでほしい！技術書・ビジネス書 大賞2019https://www.shoeisha.co.jp/campaign/award/2019/「ITエンジニアに読んでほしい！技術書・ビジネス書 大賞 2019」技術書・ビジネス書の各ベスト10を発表。大賞は、2月14日のプレゼン大会にて決定します！ そして2019年の大賞はこの2冊に決定しました。おめでとうございます。 2. 2日目 参加セッション2日目も午後からの参加でしたが、雪が降っていたので積もらないことを祈るばかりでした。雅叙園前から駅に向かう坂は激坂なので・・・ 2.1. 機械学習システムのアーキテクチャ アラカルト ～ BrainPad における実例を交えて～ 【15-C-3】 12:10～12:40 太田 満久[ブレインパッド] データ活用を生業としている会社からの機械学習の適用事例をおよびパターンの紹介で非常に参考になりました。一番のポイントはデータ分析が必要とされる場面でも機械学習はオーバースペックのケースが多々あって、その場合は古典的な統計処理を利用しようという事と、機械学習を利用する場合でもなるべく既成のAPIを利用するようにして、独自訓練は避けようというものでした。やはり独自訓練は大量のデータが必要だしコストも高いので、実践では費用対効果を考えながら必要な精度を見極めて、最低限の作り込みにする必要があるみたいです。途中の独自vs既成で出てきた Cloud AutoMLは使ってみたいと思いました。 あとバッチ推論の方がオンライン推論よりも非機能要件がゆるくてデータサイエンティストが行うモデル設計の環境に近いというのもなるほどと思いました。推論はリアルタイムで行うものばかりだと思いこんでいましたが、要件さえあえばバッチ推論の方が問題は少なさそうです。 機械学習システムのアーキテクチャアラカルト from BrainPad Inc. 2.2. エンジニアの皆さんに贈る最速キャリア戦略 【15-B-4】 13:05～13:50 松本 勇気 [DMM.com] CTO大変ッスね・・・という感想がまず一言。生き急ぎたい人向けのセッションでした。自分は途中で振り落とされましたが、正論は正論なので生まれ変わったら実践できる人になってみたいです・・・ AWS・GCP実弾演習場はマジうらやましぃ 暇なら仕事量増やしてください・・・ カッツモデル・・・ヒューマンスキル・・・ツライ 全部Hello Worldはつらいよね？ 取捨選択大事 マネジメントはみんなが学ぶべき概念 正しい意見より、解決できる意見 ITエンジニアとして学んだスキルは他の分野にも流用可能 人はオブジェクトでメッセージパッシングで動いていると捉える 自分を知ってもらう。自分の能力・目標について、周囲は思ったほど知らない ジョハリの窓 意思決定者が誰で、何で困っていて、何を目標として、何を考えているのか それを知って自分がそこにどう貢献するかを考える 意に沿わなくとも、組織の意思決定内容に貢献する 時間をすべて経験に変換する 意思決定されたことに反対するのに無駄な時間を使うな 文句があるなら意思決定に絡む努力をすること 「正しいことをしたければ、偉くなれ」を思い出した・・・ 2.3. これをまだ知らない Web エンジニアへ贈る - 私が愛する Elixir/Erlang の楽しさと辛さ - 【15-D-5】 14:10～14:55 幾田 雅仁 [gumi] すごい軽妙なトークで引き込まれました。まさしくElixirはWeb開発のための言語だと思います。 今年もErlang & Elixir Festをやるそうなので、是非参加したいと思います7。 Pros(楽しさ) Elixir + Phoenix はWeb開発に割り切れば学習容易 適当に書いてもCPU(コア)を程よく使い切ってくれて、そこそこ速くて、安全 GCでシステム全体が止まることはない(プロセス8単位でメモリを持ち、GCする) Cons(辛さ) EVM9は高機能だが、カッコいい機能は沼なので、Web開発時には無視すること モジュールは誤解されがち。関数単位で考えること 2.4. サーバーレスで最高に楽しめるアプリ開発 【15-B-6】 15:15～16:00 江藤 武司[Riotz Works] スライド リアルタイムの動画やフィードバックを始めて扱う場合でも24時間、3人で作れるよ、という感じのセッションです。楽しそうですね。肝はPWAとFirebaseとAWS(Lambda,DynamoDB)、SkyWayといったところでしょうか。サーバレスアーキテクチャなのでイベントドリブンでサービスをラムダで繋いでいく感じですが、そのことをピタゴラ装置と言っていたのが非常に面白かったです。あとDynamoDBでレコードの生成イベントやTTLイベントをトリガにする話も参考になりました。サーバレスは基本的にポーリングではなくイベントを如何に活用するかがキーポイントなんだなと改めて感じました。 ちなみにサーバレスなので費用は使った分しかかからないわけですが、このようなイベントでもワンコイン(500円)程度しかかからないそうです。 2.5. 無意味なアラートからの脱却 〜 Datadogを使ってモダンなモニタリングを始めよう 〜 【15-B-7】 16:20～17:05 池山 邦彦[Datadog] Datadogを使った監視のお話です。Datadogはデフォルトの監視設定が豊富に提供されているので基本的なOSやアプリの監視で困るようなことはなさそうです。あと長年弱点と言われてきた外形監視ももうすぐ使えるようになるそうなので非常に楽しみです。余談ですが、Datadogの社員は犬派よりも猫派が多いらしいです。このブログ名のとおり自分も猫派なので、親近感を覚えました(笑)。あと話題の 入門監視もやはりオススメらしいです。 なぜ無意味なアラートが発生するのか 監視する必要のないものは監視しない 通知する必要のないものは通知しない モニタリングのポイント ワークメトリクス スループット, 成功, 失敗, パフォーマンス リソースメトリクス 使用率, 飽和度, 失敗, 可用性 イベント 変更, アラートスケーリング APM ログ モニタリングの種類 閾値, 外れ値(Outlier), 異常値検知(Anomaly), 予測(forecast), ログ, APM, 複合条件 もう少しで対応予定 外形監視 サービスが本当にReadyなのか確認できる 【15-B-7】無意味なアラートからの脱却 ～ Datadogを使ってモダンなモニタリングを始めよう ～ from Developers Summit 2.6. Webアプリのチューニングバトル「（社内）ISUCON」の魅力と楽しさ 【15-B-8】 17:25～18:25 櫛井 優介[LINE]/古川 陽介[リクルートテクノロジーズ]/南 直[Wantedly] パフォーマンスチューニングバトルの ISUCONのお話です。名前だけは聞いたことあって最初は海外コンテストをリスペクトしてるのかなと思っていましたが、どうやら日本発祥のコンテストだったみたいで元々の意味は椅子(ISU)を投げるから取ったみたいです。ちなみに正式名称はIikanjini Speed Up Contestです。面白そうだったので過去問あさって見ようかなという気分にはなりました。優勝賞金100万円のISUCON9も開催が決まったようですが出場するかどうかは未定です・・・10 3. 場外バトル場外でも熱いバトルと言うか、宗教戦争が繰り広げられていました。 デブサミ2019に出展ブースにて「あなたが好きなエディタ＆開発環境」アンケートまだまだ実施中です。Visual Studio Codeが圧倒的人気！ここまで差が出るとは思いませんでした。#devsumi pic.twitter.com/0jlx3GFRf3— 開発室/虎の穴ラボ (@toranoana_lab) February 14, 2019 最終日の結果です😃Javaが強かった💪🏻Java:304JavaScript:285Python:262C#:194Ruby:132PHP:116C++:89#devsumi #language pic.twitter.com/pLOESAXNc9— ymk9638 (@ymk9638) February 15, 2019 現場からは以上です11。 4. まとめやはり参加して良かったです。特にGoogleの開発環境なんて今まで聞いたことがなかったので非常に面白かったです。また、このブログでは全然書ききれていませんが現場の空気感とかセッションの盛り上がりとか実際に参加してみないと分からないことだらけなので、もし機会があれば来年も参加したいと思います。 本記事はデブサミに参加したくても諸般の事情で参加できなかった人や、興味があるけどいろいろと悩んでいる人のためになるべく面白さが伝わるように心がけて書きました。その結果長文になってしまいましたが、本ブログを読んでデブサミに興味を持っていただけば幸いです。 ちなみにデブサミ関連の資料は以下のリンクにまとまっていますのでご参考まで12。 Codezineデブサミ2019、講演関連資料まとめhttps://codezine.jp/article/detail/11383 翔泳社主催のソフトウェア開発者向けカンファレンス「Developers Summit 2019」（以下、デブサミ2019）の関連資料一覧です。以下、敬称略。随時更新します（2019/04/16 14:36 更新）。 1.Change Listの略。一回のコミットに含まれる変更ファイル ↩2.GerritはGoogleが開発したOSSのコードレビューシステム。GerritはCritiqueを元にOSS化されたものらしい。 ↩3.Javaでクロスプラットホーム開発したことあるエンジニアになら大抵知っていることですが、まともな製品を開発をする場合は大抵JVMの下側がWindowsなのかLinuxなのか、はたまたAndroidなのかは当然意識します。要はWrite once, run anywhereは言い過ぎってことです。特にインストーラ周りは壊滅的だし、UIは残念なことになります。というのも結局はJVMでは最大公約数的なアプローチしかできないから、それで事足りる分はいいのですが大抵はそれだけではうまくいかないからです。もちろんこれは当時のSun Microsystemsが掲げた理念に過ぎないのかもしれませんが、Java生誕から20年以上経った今でも達成されていないことを考えると完全に誇大広告だったということだと思います。 ↩4.GCPの場合スケールさせることができるのでWrite once, run everywhereの方が近いかもしれませんが、これもさすがに言い過ぎ感があるので忘れてください・・・ ↩5.日本語ブログはありがたいのでブログ書いて！と訴えていました。このブログが届くとは思いませんが、まぁ書き続けたいと思います・・・ ↩6.自分が投票した2冊とも大賞に選ばれました。技術書部門大賞の方は2日目の翔泳社の販売ブースで購入しました。読んだ感想とかいつかブログでまとめたいと思います。 ↩7.過去2回は両方参加しています。もともとElixirはRuby on Railsのコミッターが作った言語で文法もRubyに似ていることからRubyユーザが多い印象です。 ↩8.ここで言うプロセスはOSのプロセスではなく、Elixirのプロセスです。一般的にはアクターと呼ばれる非同期処理の単位です。 ↩9.EVM=Erlang VM。ElixirはErlang言語の仮想機械(VM)であるEVM上で動作します。Java言語のVMであるJVM上で動作するKotlinやScalaのようなものです。 ↩10.最大の難関はいいチームが組めるかどうかですね・・・ ↩11.自分はEmacs派ですが、このブログはVS Codeで書いています・・・あと好きな言語はRubyとScalaです。ただ機械学習の関係でPythonを使う機会も増えてます・・・ ↩12.自分もこの記事を書く際に参考にさせて頂きました。ありがとうございました。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/02/15/devsumi-2019-winter/"},{"title":"Scala3に入るかもしれないContextual Abstractionsを味見してみた(更新・追記あり)","text":"Scala3のリサーチコンパイラであるDottyにImplicitsに代わる「Contextual Abstractions」と呼ばれる一連の機能が実装されていたので一部を味見してみました。 (2019年3月10日追記・更新: 追記内容はここを見てください) (2019年6月22日追記・更新: 追記内容はここを見てください) (2019年9月15日追記・更新: 追記内容はここを見てください) (2019年9月28日追記・更新: 追記内容はここを見てください) (2019年2月8日追記・更新: 追記内容はここを見てください) 目次1. TL;DR2. Scala3とは3. Dottyとは4. Contextual Abstractionsとは5. 味見の方法6. 味見の結果6.1. 基本的な例(givenパラメータ、using節)6.2. 基本的な例(拡張メソッド)6.3. 高度な例(型クラス)7. Contextual Abstractionsのその他の機能8. 味見してみた感想9. 追記・更新内容9.1. 2019年3月10日の更新内容9.2. 2019年6月22日の更新内容9.3. 2019年9月15日の更新内容9.4. 2019年9月28日の更新内容9.5. 2019年2月8日の更新内容 1. TL;DR この記事はDottyに実装されたImplicitsに代わる「Contextual Abstractions」と呼ばれる一連の機能を味見してみたものです 利用したDottyのバージョンは2019年2月時点で最新の0.13.0-RC1です。Dottyの開発は非常に活発なので異なるバージョンでは本記事の内容とは異なる場合があります 2019年6月時点で最新の0.16.0-RC3で変更があった文法の更新を反映しました。Dottyの開発は非常に活発なので異なるバージョンでは本記事の内容とは異なる場合があります 2019年9月時点で最新の0.18.1-RC1に更新しました。Dottyの開発は非常に活発なので異なるバージョンでは本記事の内容とは異なる場合があります 2019年9月時点で最新の0.19.0-RC1に更新しました。Dottyの開発は非常に活発なので異なるバージョンでは本記事の内容とは異なる場合があります 2020年2月時点で最新の0.19.0-RC1に更新しました。Dottyの開発は非常に活発なので異なるバージョンでは本記事の内容とは異なる場合があります 「Contextual Abstractions」は従来のImplicitsで初学者が躓きそうな機能を整理して使いやすくしています 「Contextual Abstractions」には従来のImplicitsでは実現できなかった機能(暗黙のインポート、型クラス導出、コンテキストクエリ等)も含まれています 「Contextual Abstractions」の機能はまだ提案段階でありScala3の正式な仕様に決定したわけではありません 今後機能が変化したり、機能が採用されなかったりする可能性も十分あります 「Contextual Abstractions」がScala3に正式採用された場合、古いImplicitsは段階的に廃止される予定です 「Contextual Abstractions」への移行はScalafixでサポートされる予定です この記事で紹介したサンプルコードは以下のリポジトリにあります。 hinastory/dotty_examples: Example code of Dotty (Scala 3) 2. Scala3とは2020年初頭に出ることが予定されているScalaの次世代版です。コンパイラの高速化と大幅な機能強化が行われる予定です。基本的には現行のScala(Scala2)とのソースコードレベルの後方互換性を意識して機能強化が行われていますが1、非互換が生まれるところや推奨の書き方が変わる所はScalfix2で対応することが予定されています。 3. DottyとはDotty3はScala3の研究用コンパイラで、Scala3\bの仕様や実装を研究するためのものです。Scala3はDottyがベースになることがアナウンスされています4。 4. Contextual Abstractionsとは現行のScalaには俗にImplicitsと呼ばれる機能がありますが、初学者を非常に混乱させる機能として悪名高いものでした5。そこでDottyには、この混乱に決着を着けるべくImplicitsの機能を包含しつつより整理された「Contextual Abstractions」と呼ばれる一連の機能が実装されました6。Implicitsの代替という面でみるとこれらの機能は「implicit」というキーワードをなるべく使わずに別の用語(given等)で置き換えて、型クラスをより書きやすいようにチューニングしたような内容になっている印象です。本記事ではDottyドキュメント7を参考にしながら、「Contextual Abstractions」の機能の一部を味見してみました。以下が味見した機能の一覧です8。 givenインスタンス(Given Instances) 従来のimplicitで定義されていたインスタンスと同等です using節(Using Clauses) 従来のimplicitで定義されていたパラメータリストと同等です givenインポート(Given Imports) 通常のimportではgivenで定義されたgivenインスタンスはインポートされず、別途import A.givenでインポートする必要があります import A.{given, _}でパッケージAのgivenインスタンスも含めた全てをインポートできます デリゲートがどこから来たのかを明確にするために導入されたようです 拡張メソッド(Extension Methods) Dottyの新機能です 型が定義された後にメソッドを追加することができます 型クラスの実装(Implementing Typeclasses) 「givenインスタンス」、「using節」、「拡張メソッド」でよりシンプルに型クラスが実装可能になりました 5. 味見の方法ここから`dotty-0.22.0-RC1.zip`をダウンロードして解凍します。解凍後のフォルダの`bin`にパスを通せば利用できるようになります。 dotcがコンパイラです。dotrはクラス名を指定するとコンパイル済みのバイナリを実行します。単独で起動した場合にはREPLになります。 12$ dotc # ソースコードのコンパイル$ dotr # コンパイル済みのコードを実行 もしくはサンプルコードをGitHubで公開したので、sbtをすでにインストールされている方はそちらの方が早いと思います。使い方はREADME.mdをご覧ください 6. 味見の結果Dottyドキュメントに記載されている例をベースに味見をしてみました9。 6.1. 基本的な例(givenパラメータ、using節)従来のImplicitsが分かっている人から見れば、おおよそ以下のコードの意味が分かると思います。拡張メソッド記法だけは、最初は戸惑うかもしれません。自分は最初Go言語に似ているなと思いました・・・givenインスタンスを定義してみた例が以下になります。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** `given`インスタンス、`using`節のサンプル */object GivenExampleDefs { /** 順序型の定義 */ trait Ord[T] { def compare(x: T, y: T): Int def (x: T) < (y: T) = compare(x, y) < 0 // 拡張メソッド記法を使って定義してあります def (x: T) > (y: T) = compare(x, y) > 0 // 上記と同様 } /** 順序型のIntの`given`インスタンスの定義 */ given intOrd: Ord[Int] { def compare(x: Int, y: Int) = if (x < y) -1 else if (x > y) +1 else 0 } /** 順序型のListの`given`インスタンスの定義 */ given listOrd[T](using ord: Ord[T]): Ord[List[T]] { def compare(xs: List[T], ys: List[T]): Int = (xs, ys) match { case (Nil, Nil) => 0 case (Nil, _) => -1 // 空リストよりも非空リストの方が大きい case (_, Nil) => +1 // 同上 case (x :: xs1, y :: ys1) => val fst = ord.compare(x, y) // 先頭の大きさがLists全体の大きさ if (fst != 0) fst else compare(xs1, ys1) // 同じだったら次の要素を再帰的に繰り返す } } /** `using`節 */ def max[T](x: T, y: T)(using ord: Ord[T]): T = if (ord.compare(x, y) < 1) y else x /** 無名`using`節 */ def maximum[T](xs: List[T])(using Ord[T]): T = xs.reduceLeft(max) /** コンテキスト境界使った書き換え(Scala2と同様) */ def maximum2[T: Ord](xs: List[T]): T = xs.reduceLeft(max) /** `using`節を使って新しい逆順序型クラスインスタンスを作る関数 */ def descending[T](using asc: Ord[T]): Ord[T] = new Ord[T] { def compare(x: T, y: T) = asc.compare(y, x) } /** より複雑な推論 */ def minimum[T](xs: List[T])(using Ord[T]) = maximum(xs)(given descending)} givenインスタンスの利用例が以下になります。 12345678910111213141516171819/** `GivenExapmple`の利用方法 */object GivenExample { import GivenExampleDefs.{_, given} // givenは `Ord`の` X] { def [A, B](r: Ctx => A).flatMap(f: A => Ctx => B): Ctx => B = ctx => f(r(ctx))(ctx) def pure[A](x: A): Ctx => A = ctx => x } /** 関手の利用 */ def transform[F[_], A, B](src: F[A], func: A => B)(using Functor[F]): F[B] = src.map(func) /** コンテキスト境界を使った書き換え */ def transform2[F[_]: Functor, A, B](src: F[A], func: A => B): F[B] = src.map(func)} 型クラスは以下のように利用できます。 123456789101112131415161718192021/** `TypeClassExampleDefs`の利用方法 */object TypeClassExample { import TypeClassExampleDefs.{given, _} def use(): Unit = { println(\"\\n--- start TypeClassExample ---\") println( sum(List(\"abc\", \"def\", \"gh\")) ) // abcdefgh println( sum(List(1, 2, 3)) ) // 6 println( summon[Monad[List]].pure(12) ) // List(12) println( transform(List(1, 2, 3), (_:Int) * 2) ) // List(2, 4, 6) // Reader Monad Example val calc: Int => Int = for { x e + 1 y e * 10 } yield x + y println( calc(3) ) // 34 }} 7. Contextual Abstractionsのその他の機能Contextual Abstractionsの機能で味見できなかった機能を簡単に紹介します。 マルチバーサル等価性(Multiversal Equality) Scala2では文字列と数値が比較可能でしたが、この機能により厳密に型が合っていないとコンパイルエラーにすることもできるようになりました 型クラスの導出(Typeclass Derivation) Haskellのderivingと同等の機能です。現在のDottyで導出可能な型クラスはマルチバーサル等価性を表すEqlのみのようです メタプログラミングを使えば自分で導出可能な型クラスの定義もできます 暗黙の型変換(implicit conversion) 元々のImplicitsのトラブルメーカーです。新たにConversionという型が定義されてその暗黙のインスタンスを定義することで利用できます コンテキスト関数(Context Functions) 以前はImplicit Function Typeと呼ばれていた機能で0.13.0-RC-1で構文が変更されました ビルダーパターンを簡単に実装できます 8. 味見してみた感想Implicitsが大分飼いならされたような印象でした。特に従来はimplicitをパラメータリストで受け取っていたのをgivenという専用構文で受け取るようになったのが非常に分かりやすかったです。ただ、従来のimplicitlyの名称はまだかなり揺れているみたいです10。 もともとは「A Snippet of Dotty」を読んで、あまりにも自分が知っているScalaと違っていたので調べ始めたのがこの記事を書こうと思ったきっかけです。この記事がScala3がどういう方向を目指しているのか知りたい人の参考になれば幸いです。 9. 追記・更新内容9.1. 2019年3月10日の更新内容本家のブログが公開されたようです。`0.13.0-RC-1`のタグが打たれてから10日以上経ってからの公開なのでかなり遅い方だと思いますが、それだけ今回のリリースが盛りだくさんだったと言うことだと思います。本家のブログには従来の`implicit`がなぜダメだったのか丁寧に説明されていました。 The implicit keyword is used for both implicit conversions and conditional implicit values and we identified that their semantic differences must be communicated more clearly syntactically. Furthermore, the implicit keyword is ascribed too many overloaded meanings in the language (implicit vals, defs, objects, parameters). For instance, a newcomer can easily confuse the two examples above, although they demonstrate completely different things, a typeclass instance is an implicit object or val if unconditional and an implicit def with implicit parameters if conditional; arguably all of them are surprisingly similar (syntactically). Another consideration is that the implicit keyword annotates a whole parameter section instead of a single parameter, and passing an argument to an implicit parameter looks like a regular application. This is problematic because it can create confusion regarding what parameter gets passed in a call. Last but not least, sometimes implicit parameters are merely propagated in nested function calls and not used at all, so giving names to implicit parameters is often redundant and only adds noise to a function signature. Dotty Blogより 意訳すると従来のimplicitにはimplicit conversionsとconditional implicit valuesの２つの用途があったけど、意味が違うし初学者は混同しやすいので構文的に別にするという話です。というかconditional implicit valuesという言い方は自分は初めて目にしました。単純なimplicit valuesよりもわかりやすいですね。 この本家のブログを受けてというわけではないですが、前回の記事でサンプルの解説が大分雑だったのでいろいろと見直して、サンプルコードもGitHubに公開しました。興味のある方は味見をして頂けると幸いです。 9.2. 2019年6月22日の更新内容先日発表されたDotty 0.16.0-RC3も0.16.0-RC3にしてあります。あと何回キーワードが変更されるんだろう・・・ 9.3. 2019年9月15日の更新内容先日発表されたDotty 0.18.1-RC1でリーダーモナドの例がコンパイルできるようになっていました。また、0.18.1-RC1で追加されたインデントベースの構文についても記事を書いたので興味があればご一読ください。 Scala 3、Pythonのようにインデントベースの構文で書けるようになるってよ！ 9.4. 2019年9月28日の更新内容先日発表されたDotty 0.19.0-RC1で本記事に関する文法が大きく変更されました。関連するプルリクは主に以下の4つです。 Trial: given as instead of delegate for by odersky · Pull Request #6773 · lampepfl/dotty Change to new given syntax by odersky · Pull Request #7210 · lampepfl/dotty Drop old syntax styles for givens by odersky · Pull Request #7245 · lampepfl/dotty Replace the[...] by summon[...] by odersky · Pull Request #7205 · lampepfl/dotty 簡単に言うとdelegateがgivenに置き換えられてgiven節がgivenパラメータになってsummon大復活です。0.19.0-RC1より前のものも含まれていますが、今回合わせて修正しました。 9.5. 2019年2月8日の更新内容Dottyは0.21.0でめでたくfeature-completeしました。つまりこれ以降は大きな機能追加はないはずです。 ・・・と安心していたら0.22.0でusingキーワードが追加されました。文法の調整はまだ続くようです・・・今回は以下の３つのリリースで行われた修正を反映しています。 Announcing Dotty 0.20.0-RC1 – `with` starting indentation blocks, inline given specializations and more Announcing Dotty 0.21.0-RC1 - explicit nulls, new syntax for `match` and conditional givens, and more Announcing Dotty 0.22.0-RC1 - syntactic enhancements, type-level arithmetic and more 大きな変更はusing、on、as、extentionなどのキーワードが登場してより読みやすくなったことだと思います。実際の使い方は記事本文をご覧ください。あと、今まで拡張メソッドはこの記事では説明がなかったので追加しました。1.Pythonの教訓を活かして、なるべく言語の世代間の断絶を起こさないように配慮して開発が進められているようです。もちろん配慮が足りていない可能性もありますが・・・ ↩2.ScalafixはScalaにおける汎用的はリファクタリング、リンティングツールです。Scala3専用ではありません。 ↩3.コンパイラの理論的な基盤にDependent Object Types (DOT)を用いているのが名前の由来です。 ↩4.Dottyに現在入っている機能が全てScala3に取り込まれるわけではありません。Scala3に正式に取り込まれる機能は Scala Improvement Process (SIP)(JavaのJCP/JSRのようなもの)を通過する必要があり、ここで承認が得られなければScala3に取り込まれることはありません。2019年2月時点ではDottyの新機能の多くはまだSIP以前の提案段階であり、これから徐々にSIPのレビュープロセスに乗せられていくものと予想されます。 ↩5.現行のImplicitsの混乱するポイントについてはこちらの記事で詳しく取り上げられています。 ↩6.暗黙のインスタンスと推論可能パラメータが追加された経緯を知りたい方は#5458と #5852をご確認ください・・・#5458の方は長すぎてまともに追っていませんが元々はwitnessというキーワードで提案されて途中でinstanceに変わって#5825でimpliedに変わったようです。本当に大激論で互換性に対する懸念が何回も強く出ています。とりあえずこの機能はSIPを通さないとScala3に入ることはないという念押しでマージされました。それ以外のContextual Abstractionsの機能(拡張メソッドや型クラスの導出等)はここまでもめた様子はなかったです。さらに#6649でdelegateに変更されました。そしてさらに、#6773と#7210で大幅に文法チェンジ！！delegateが排除されてgiven一色になりました・・・ 本当に何回変わるんだろう・・・ツライ・・・ ↩7.このドキュメントは最新版のスナップショットなので、どんどん書き換えられています。今の所過去のバージョンは参照できないみたいです・・・ ↩8.機能の日本語訳は自分がしました。間違っていたら教えてください。 ↩9.一部で実行しやすいように手を加えたり、コメントで説明を加えたり、例が間違っている箇所を修正したりしているのでドキュメントそのままというわけではないです。 ↩10.もともとsummonという名前で提案されていましたが、0.13.0-RC-1ではinferに変わり、#5893ではtheに変更されています。しかし、#7205でまさかのsummonの復活!! 追うのも楽じゃない・・・ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/02/24/scala-dotty-contextual-abstractions/"},{"title":"全プログラマに捧ぐ！図解「ノートブック」","text":"Jupyter Notebookを使い始めて半年経ちました。最初は機械学習のためのツールとして触り始めましたが、徐々にデータサイエンティストだけの玩具にしておくのは勿体無いと思うようになってきました。本記事では一般のプログラマにも伝わるようになるべく図解で「ノートブック」を解説してみたいと思います。 目次1. はじめに2. ノートブックとは3. ノートブックと実行結果4. ノートブックとセル5. ノートブックとカーネル6. ノートブックとファイル7. ノートブックの変換8. ノートブックの共有9. ノートブックとJupyterLab10. ノートブックを始めてみる11. ノートブックの用途12. まとめ 1. はじめに本記事はJupyter Notebookにおける「ノートブック」という機能に着目して解説したものです。「ノートブック」は、REPL1の進化形態であり、仮にデータサイエンティストでないとしても全プログラマが知っていて損はないアイデアだと思います。 2. ノートブックとはノートブックとは以下の図のように「コード」と「実行結果」と「コードのメモ」をまとめたものです。コードのメモは恐らくプログラマの方にはおなじみのMarkdown形式で記述することができます。 ノートブックの凄いところはこのプログラムの理解に必要な３点セットを上手にまとめている点です。これらがバラバラに管理されていたとなると、プログラムのコードの動作を他人に理解させようとすると非常に苦労することが目に見えますし、もちろん書いた本人にも優しくないです(笑)。 面白いのは以下の図のようにコードだけでなくメモも「実行(Run)」できる点です。実行後のプログラムのコードの下にはコード実行結果が出力されているのはお分かり頂けると思いますが、メモの実行結果はMarkdownがレンダリングされた結果となります。また図を見てお分かりのとおり、コードやメモはきちんと色付けされており視認性もよいです。 ノートブックが実行されるのは以下の図のようにノートブックサーバになります。コードやメモはそれぞれノートブックの中の「セル」と呼ばれるものに格納されていて、セル単位で対話的に実行が可能です。 また、コードの編集や実行はブラウザを通して行うことになりますが、マウス操作以外にもキーボードショートカットやコード補完等が使えるので一般的なWebアプリケーション以上に快適な作業が可能です。 3. ノートブックと実行結果実行結果とはプログラム自体が画面に出力したものと、プログラムの「最後の行の戻り値」になります。以下のノートブックの一部を使って説明します。 上記の灰色の枠内がプログラムです。半径5の円の面積を計算しています。その枠の下の「78.53981633974483」が戻り値になります。これはプログラムの最後の行「math.pi * r ** 2」を評価した結果が戻り値になっています。 初めての人が陥りやすい罠として、最後の行を「area = math.pi * r ** 2」のような代入文にして、何も表示されず困るというものがあります。これはPythonの代入が戻り値を持たない仕様のためにおこる現象です2。この場合は代入文の後に「area」を追加して最後の行を表示したい変数に変更するか、以下のようにprint関数で明示的に画面出力すればよいです3。 さらにノートブックは、テキスト以外にも他のライブラリと連携してリッチな出力結果を実現できます。以下は表を表示してみました。 グラフも表示可能です。 画像処理結果の表示もお手の物です。 動画の埋め込み4なんかも可能です。 4. ノートブックとセルセルとは以下の図5のように、ノートブック内のプログラムのかたまりであり、セル単位でプログラムの編集、実行、削除が可能です。 セルには編集モードとコマンドモードがあり、直接マウスで選択すると編集モードになります。編集モードでプログラムを記述して「Shift + Enter」キーを入力することでセルの実行が可能です。編集モードで「Esc」キーを入力することでコマンドモードに移行することができます。コマンドモードではその名のとおりセルに対して様々な命令を実行することができます。例えば「d d」を入力することでセルの削除が可能です。 ノートブックでは基本的に下にセルを追加して、実行結果を確認したい単位でプログラムを書いて実行することを繰り返します。一度実行したセルもまたセル内のプログラムを編集して再実行することができます。そのためセルの左側には実行の順番を表すための番号が振られており、実行するごとに１増えていきます。 赤枠で表示されているのは、プログラムを実行して発生した例外のスタックトレースです。このようにスタックトレースも色付きで分かりやすく表示されるのでデバッグには重宝します。プログラムでエラーになった場合を特定するためにセルを分割して、例外が発生した箇所を狭めていく手法もよく使われると思います。 セルは一番下だけでなく、セルとセルの間にも挿入することができます。これは後で忘れていた処理を追加するのに便利ですが、注意すべき点があります。以下の図は上から２番めのセルの下に新しいセルを挿入して、プログラムを記述して実行した後の状態を示しています\b。セルの実行順番はセルの左の番号の通りです。 この図で注意すべきは3点あります。１つ目の注意点は、挿入したセルの実行結果はすでに実行済みのセルの実行結果には影響を与えないということです。この図の場合一番下のセルの最終行はa + bとなっていますが、今回のようにその上のセルに挿入してbの値を書き換えても最後のセルの実行結果は変わりません。 次に注意すべき点は、いったんこのノートブックを終了して再度ノートブックを「先頭から」実行したときに実行結果が変わる点です。問題は最後のセルで変数aとbを上書きしている点です。このため上から3番目のセル(挿入したセル)で、実行結果が変わってしまいます。この場合array([9, 12, 15])だった結果が[1, 2, 3, 4, 5, 6, 4, 5, 6]になります。さらに続けて最終セルを実行しようとすると例外が発生します。 最後に注意すべき点は一番最後のセルでimport文を使っている点です。一回実行されたimport文はノートブックの範囲内で有効なので、セルの挿入位置にかかわらずインポートされたライブラリの機能が利用できます。しかし、ノートブックを終了して再度先頭から実行しようとすると、当然import文の実行前にそのライブラリの機能を使おうとすると怒られます。今回の例ではさらに紛らわしくて実行結果が変わるという現象にもつながっています。 従って、ノートブックを先頭から再実行する可能性があり、かつセルを一番下以外に挿入する場合は、セルを一番上から実行しても問題ないかということを意識する必要があります。このトピックはおそらくノートブック利用者が一度は通る道だと思い言及しました6。 5. ノートブックとカーネル一つのノートブックには必ず一つのプログラミング言語が紐付きます。下の図のようにノートブックを開くと裏では紐付いたプログラミング言語の実行環境が起動して、セルの実行のために待ち受けています。このプログラミング言語の実行環境のことを「カーネル」と呼びます。現在、非常に多くのカーネルが公開されています。 またカーネルは図のようにノートブックを開くたびにひとつ起動されるイメージで、カーネル間でのメモリ等の共有はなく完全に独立しています。また、このカーネルは明示的にシャットダウンしないとずっと動きっぱなしになります。従ってノートブックを開くとカーネルが起動してマシンのリソースを消費するので、不要なカーネルはこまめにシャットダウンすることをおすすめします。 6. ノートブックとファイルノートブックの実体は一つのファイルです。もう少し具体的に述べると拡張子が「.ipynb」でJSONフォーマットで保存されています。単なるファイルなのでフォルダによる整理やGit等のバージョン管理システムによる管理も可能です7。 7. ノートブックの変換ノートブックは以下の図のように様々な形式に変更可能です。単純に実行スクリプトに変換できるのはもちろんですが、HTMLやマークダウンにも変換できるのでQiitaやブログサービスへの埋込も可能です。Reveal.jsのスライドにすればプレゼンテーションにも利用できます。 8. ノートブックの共有ノートブックの実体はファイルだということは前述のとおりですが、このファイルは様々な環境で開くことができます。例えばVisual Studio Codeでは拡張機能でノートブックに対応しています。さらに、クラウド上にはノートブックに対応したマネージドサービスが揃っているので、ローカルで作成したノートブックをクラウドで実行するのも簡単です。また編集や実行機能が必要なければ、nbviewerのGitHubのように表示だけが可能なサービスも存在するので、コードや実行結果の共有だけがしたい場合はこちらのほうが手軽です。 9. ノートブックとJupyterLabJupyterLabはJupyter Notebookの次世代ユーザインターフェースです。タブインタフェースで非常に使いやすく自分も愛用しています。まだ、バージョン1.0には達していませんが、普段の利用にはほぼ問題ない感じです。現在は混在\b期間でどちらも非常に多く使われており甲乙付け難いです。拡張機能も含めた安定性や情報の多さを取るならJupyter Notebookに軍配が上がりますが、一部の先進的な機能や格好良さを求めるならJupyterLabをおすすめします。以下の画面は自分が実際に利用しているJupyterLabの画面です8。 10. ノートブックを始めてみるノートブックの始め方はいろいろあるので、ざっくりとしたチャートを自分の主観で書いてみました。最初の一歩を踏み出すための道標としてご利用ください9。 エントリー URL Try Jupyter https://jupyter.org/try Colaboratory https://colab.research.google.com Sage Maker https://aws.amazon.com/jp/sagemaker Azure Notebooks https://notebooks.azure.com Binder https://mybinder.org zero-to-jupyterhub-k8s https://github.com/jupyterhub/zero-to-jupyterhub-k8s jupyterhub-deploy-docker https://github.com/jupyterhub/jupyterhub-deploy-docker JupyterHub https://jupyterhub.readthedocs.io/en/stable Jupyter Notebook https://jupyter.org/install base-notebook https://hub.docker.com/r/jupyter/base-notebook minimal-notebook https://hub.docker.com/r/jupyter/minimal-notebook all-spark-notebook https://hub.docker.com/r/jupyter/all-spark-notebook pyspark-notebook https://hub.docker.com/r/jupyter/pyspark-notebook tensorflow-notebook https://hub.docker.com/r/jupyter/tensorflow-notebook datascience-notebook https://hub.docker.com/r/jupyter/datascience-notebook Go back to TOP OF THIS PAGE このページの先頭から再度読み直してください 11. ノートブックの用途ノートブックは非常に用途が広いので、プログラミングにおける様々な場面で活用することができると思います。以下の図は思いついた用途です。 12. まとめ以下、「ノートブック」のまとめです。 「ノートブック」はコードと実行結果とコードのメモをまとめたものである 「ノートブック」の実体はファイルである 「ノートブック」はMarkdown、Asciidoc、LaTeX、HTML、PDF、実行スクリプト、スライドなど様々な形式に変換できる 「ノートブック」を利用できる複数のクラウドサービスが存在する 「ノートブック」にはプログラミング全般に幅広い用途がある Jupyter Notebookはデータサイエンスの分野ではほぼ必須とまで言われるようなツールになりましたが、一般のプログラマへの浸透具合はいまいちと感じたので、データサイエンスの文脈からなるべく切り離して解説をしてみました。 本記事が「ノートブック」の理解と普及の一助になれば幸いです。1.REPLは「Read-Eval-Print loop」の略で、入力された部分ごとに評価して表示することを繰り返す、対話型実行環境を意味しています。PythonにおけるiPythonやRubyにおけるirbにあたります。インタラクティブシェルと呼ばれることもあります。 ↩2.Pythonでは代入以外にもifやwhile等の制御構造も戻り値を持ちません。一方Rubyのように、代入やifやwhileが戻り値を持つ言語もあります。 ↩3.printは戻り値を持たない関数なので、戻り値は表示されません。しかし一般的なPythonの関数は戻り値を持つもの(returnで値を返す)が多いので、大抵は関数の結果確認のために「最後の行の戻り値」を有効活用できます。 ↩4.埋め込まれている動画は「 https://www.youtube.com/watch?v=XQB3H6I8t_4 」になります。 ↩5.ノートブックの図はJupyter Notebookの新インターフェースであるJupyterLabを元に作成しています。Jupyter Notebookだと若干見え方は異なりますが、ここで説明される概念は共通です。 ↩6.この注意点はPythonを例にしているので他の言語では当てはまらない場合があります。ノートブックの特性と言語の特性の組み合わせで思わぬ落とし穴があるかもしれないということだけ覚えておくといいかと思います。 ↩7.ノートブックには実行結果も含まれますが、実行結果を除いてバージョン管理しようとすると少し工夫が必要になります。手段はいろいろあるので気になる方は検索してみてください。 ↩8.画面に写り込んでいるプログラムは PythonによるAIプログラミング入門の例題になります。 ↩9.ツッコミどころがあるのは理解しております。スルーして頂けると幸いです。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/04/06/understanding-notebook/"},{"title":"RubyKaigi 2019 in 福岡に参加してきた話","text":"前代未聞の公式パーティーが開催されたRubyKaigi 2019に参加してきました。去年は仙台、一昨年は広島でしたが今年は福岡での開催でした。自分は広島から連続で参加していますが、このイベントはホスピタリティが高く今年も楽しみにしていました。ただし今年は参加者の度肝を抜くネタが２つも仕込まれていたことから特筆すべき「RubyKaigi」になったといっても過言ではないでしょう。 RubyKaigi 2019RubyKaigi 2019https://rubykaigi.org/2019RubyKaigi 2019, 4/18...4/20, Fukuoka, Japan #rubykaigi 目次1. 会場について1.1. 会場内の様子2. セッション内容2.1. The Year of Concurrency (Keynote) @yukihiro_matz2.2. 福岡県知事2.3. A Type-level Ruby Interpreter for Testing and Understanding @mametter2.4. Pattern matching - New feature in Ruby 2.7 @k_tsj2.5. その他のセッション3. 食事3.1. 朝食3.2. 昼食3.3. 間食3.4. Official Party in 川端商店街4. 令和時代のRubyKaigi5. まとめ 1. 会場について日時と場所は以下のとおりです。初日は羽田から飛行機で飛んで、8時半頃に福岡空港に着きました。時間があり天気も良かったので、地下鉄で中洲川端まで出てそこからは徒歩で会場に向かいました。 日時 4/18(木) - 4/20(日) 10:00〜 場所 福岡国際会議場 1.1. 会場内の様子到着は9時半ごろです。すでに受付には行列ができていました。参加証の後ろの紙はCookpadさんで配っていたRubyパズルです。一日一枚配っていて一枚あたり3問出題されていました。写真に写っているProblemsの3-1は何とか解けましたが、その他はあまり解けませんでした。コメントや改行を用いたトリッキーなものが多かった印象です1。 2. セッション内容2.1. The Year of Concurrency (Keynote) @yukihiro_matz初日のキーノートのもちろんRubyの父であるMatzこと\b「まつもとゆきひろ」さんのキーノートです。 タイトルからして並列性の話かと思いましたが、Ruby3全般の展望のお話が聞けました。個人的に安心したのは静的型付けへの道筋が立ったようだという点です。去年の段階だとかなりもやっとしていた印象ですが、今年は基礎技術が出揃ってアーキテクチャが固まってきたという印象を持ちました2。そしていつもの「型は書きたくない」発言が聞けて大満足です(笑)。 個人的なメモです。詳細を見たい場合はクリックして開いてください。 Rubyは良いと言われている 生産性 柔軟性 楽しい！ Rubyは悪いと言われている 性能 伝統的に遅いと評判だが、年々速度向上している マルチコアを活かしきれない GIL(Giant Interpreter Lock)があるから 大きなチームや大規模プロジェクト向きではない Rubyは大抵の用途には十分良い GitHub、Airbnb、Instacart、Cookpadという実例がある Twitterのようなサービスには向いていない。(リアルタイム性や性能要求が高いから？) そもそもTwitterは遅い1.8を続けたから自滅したんじゃない？ 1.9以降を使っていたら未来は変わったかも Ruby3の改善点 性能 MJITの導入(2.6) ファミコンエミュレータでは速くなるけどRailsでは遅くなる 改善予定 並行性 Guild アクターモデル AutoFiber I/O待ちの改善 静的解析 型は書きたくないでござる Rubyソースには型アノテーションは入れずに、別ファイル(.rbi)に型定義を書く 以下の４つをRuby 3で行う予定 型定義文法 標準ライブラリの型定義 Type Profiler 静的型チェック Sorbet Steep 我々は生き残らなければならない ご飯を食べなければいけない 前進し続けるしかない 賢く進むしかない 自分は天才じゃない。後悔もする(スレッドをいれたこととか) みんなで進むしかない Rubyで世界を良くしたい 2.2. 福岡県知事小川福岡県知事がRubyKaigiに来られて挨拶されました。内容は観光案内とRubyの力に期待を込めた内容でしたが、国際会議の流儀どおりに英語でスピーチしたのはポイントが高いと思います。ただ福岡県がRubyに力を入れていること知っていたので、予想はできたというかこれで度肝を抜かれたというわけではないです2。 2.3. A Type-level Ruby Interpreter for Testing and Understanding @mametter15:40 - 16:20 / Main Hall (3F) @ rubykaigiA 前述したRuby 3で入る予定Type Profilerの発表です。Type Profilerはタイトルの「型レベルインタプリタ」のとおり、型に関する部分を「実行」して、プロファイリングするもののようです。 A Type-level Ruby Interpreter for Testing and Understanding from mametter Type ProfilerはRuby 3で提供予定の静的解析のためのコンポーネントの一つです。静的解析の全体像としては以下の図が分かりやすかったです。Type Profilerの主目的は通常のRubyアプリケーションコードからクラスやメソッドの型を推定して型定義ファイルのプロトタイプを生成することです。この型定義は健全ではないので間違うこともあり、手動で修正が必要な場合があります。 Ruby 3の静的解析Ruby 3 Progress Reportより Type Profilerの動作については以下の図が分かりやすかったです。従来のインタプリタが「値」に注目して実行しているのとは対照的に、Type Profilerでは呼び出し時の引数の型と戻り値の型に注目して評価し、その結果を型シグネチャとして抽出します。 Type Profilerの動作セッションスライドより このような動作のため実際にテストコードを通さないと型シグネチャは得られないみたいです。またその他にもメタプログラミングに弱かったり、状態爆発して遅かったりまだまだ問題はあるそうですが、とりあえず動作するものが公開されたので、今後の進捗に期待しつつ自分も少し触ってみたいと思います3。 2.4. Pattern matching - New feature in Ruby 2.7 @k_tsjこれですよ、これ。最近のRubyで一番欲しかったやつ。パターンマッチングです。どうやら発表前日にtrunkに取り込まれたそうなので、すでに遊べるようです。 その有用性は以下のJSONのパターンマッチングの例を見れば一目瞭然でしょう。 JSONのパターンマッチング例セッションスライドより こういったマッチングはScalaのような静的言語でやろうとすると非常に面倒ですが、Rubyだと本当にシンプルに書けていい感じです。その他にも配列やハッシュで使いやすいように考慮されていたりElixirで言うところのpin operator(^)が実装されていたりするのでかなり実用的になっているなと感じました。ただ一つ残念なのはキーにはまだシンボルしか使えないらしいので、そこは今後に期待です。 2.5. その他のセッションその他も素晴らしいセッションはたくさんありましたが、全てを紹介するのは難しいので簡単な感想(メモレベル)を書きました。自分の心の声や脳内(誤)変換もそのまま消さずに残しておきました。嘘や間違いが見抜けて優しくスルーできる人だけが読んでください(笑)4。 覚悟ができた方はここをクリックしてご覧ください。 Matz Keynote テスト嫌いなんだよね DRYじゃないでしょ Ruby 3 Progress Report RubyコアチームからのRuby3に向けての進捗報告 実質Matzの話の続きで少し詳細化した内容 How To Use OpenApi3 For Api Developer @ota42y OpenApi 3の話 分かる人にはSwaggerがそのままOpenAPIに名前を変えたといったほうが分かりやすいかもしれない 個人的にはRESTよりもGraphQLのほうがスマートだと思っているが・・・ Pragmatic Monadic Programming in Ruby @joker1007 モナドをRubyに実装した話 RubyのASTを 悪用 活用した素晴らしい例 Scalaを パクった インスパイアした素晴らしい例 個人的にはこういうのを待ってた 👍 リポジトリは こちら。後で遊ぶよ。 Fibers Are the Right Solution @ioquatix Call back hell！ Async/Await Hell！ So Fibers are right！ Auto Fiberに期待しておこう All bugfixes are incompatibilities (Keynote) @nagachika リリースブランチのメンテナの6年間の苦労話 「人間は1年に1歳年をとる脆弱性がある」というパワーワードが飛び出る 美しいパッチだと取り込みたくなる 必要のないパッチを入れるのはやめておけ。バグるかもしれない・・・ parse.yは魔境 Syntax Errorは辛い・・・ 回避不能 括弧つけろよ、と思わなくもない Be Practical コミットログを読もう コンフリクトは2割ぐらい 発生バージョンが細かく書いてあったり、具体的に困っているプロダクトがあると取り込みやすい Six Years of Ruby Performance History: But How to Measure…? @codefolio RRB(Ryby Rails Bench)によるとRailsはRuby 2.6は2.0と比較して172%速くなった RRBは実アプリケーションに合わせたマクロなベンチでどの部分が遅いかは教えてくれない そこでRSB(Ryby Simple Bench)の出番 intimate Chat with Matz and mruby developers about mruby @Hir0_IC 今日4/19はmrubyお誕生日・・・\bです mrubyはメモリ消費を抑えるのを頑張っている MatzにGCを作って欲しければ、退屈な会議に参加させればよい 捻挫したからセパレートキーボードにした by Matz 回答者よりも先に質問者が回答を述べる。誘導尋問とかそういうレベルではない Building a game for the Nintendo Switch using Ruby @amirrajan Nintendo SwitchでRubyを動かしやがった・・・ こ、こいつ動くぞ！ RubyData Workshop 資料その2 @mrkn, @284km, @kozo2, @ktou, @znz Red Data Toolsの話 無理しすぎんなよ rubydownは面白そう お菓子とフルーツのデプロイが完了した What is Domain Specific Language? @tanaka_akr Syntax + Semantics = Language この二つの違いをちゃんと知りたい方はコンピュータサイエンスを学びましょう(笑) Smalltalkの2項演算子には優先順位がない。ま、まじか！ 良いことには限界があるが悪いことには限界がない 名言頂きました Lightning Talk Sessions How does TruffleRuby work @kis Javaの話をします。Javaを書こう！ Graal VMとGraalは違う 第3二村射影がTruffleになるという話。コンパイラジェネレータのお話。 How to Make Bad Source @jimlock ブラックジャックを糞コードにした話と仕様を都合よく変える話(ぉぃ ワレ、TRICK2018ノ、フッカツヲ、ノゾム 奴らクレイジーだな Dive into middleware with mruby @spikeolaf 人生を生きているとRDBMSを作りたくなる・・・まじか！ From ㍻ to U+32FF @Y_MITSUBOSHI まぁ令和ネタはあるだろうと思ってた Invitation to the dark side of Ruby @tagomoris Rubyには決してマクロは入れない by Ma●ｚ 👉 Maccro作りました。てへぺろ。 Applying mruby to World-first Small SAR Satellite @ShunsukeOnishi mrubyのコードを衛生に送る ラーメン屋の行列を見てから行きたいじゃないっすか？ The TracePoint bumb! @koic TracePointを使ったテロの話 頭沸いてるんじゃねーか by ko1 悪とかそういうレベルじゃねえなこれは by usa バグ・オブ・ザ・イヤー by nobu wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww やめろ、こんなん草しか生えんわ！ Make Ruby Differentiable @nagachika Rubyでも自動微分したい！的な何か Ruby Committers vs the World curryはお手本を見せるために作った。使うべきではない。 新機能をガシガシ増やしたい人 ノシ by 多数の観衆 投票は信じないよ by Matz わざわざ福岡に来るのはアグレッシブな人なの。真に受けて実行したあとに責任取らされるのは僕なの！ by Matz 公開開発者会議(議事録) HackMDは便利。OSS版はCodiMDで自分はこちらをよく使う。 Haskellの$やElixirの|>相当の演算子がRubyにも欲しい 👉 が演算子の候補になったときは大爆笑。草はやすわ むしろ右代入が欲しい こうしてゆるくてほとんど決まらないのが平常運転 もう少しでRubyコアの開発をsubversionからcgitに移行するよ → したよ! Cleaning up a huge ruby application @riseshia 立ち見が出るほどの超人気セッション Cookpad社で使われているcookpad_allというリポジトリはコード量が50万行以上あり 未使用コードを削りたい KitchenCleanerを開発 1年間更新の無いコード、未実行のコードを自動で見つけ出す GitHubにIssueを立ててgit logからランダムで人を選んでアサイン。まじか！ IseqLogger 使われていない命令を洗い出す。要コンパイル Ruby 2.6から入った“oneshot coverage”は一回でも実行されたかどうかを計測。これは便利そう。 The challenges behind Ruby type checking @soutaro Rubyの型チェッカーの Steepの話 FontAwesomeでRubyを検索するとアヒルがでてくる。分かっているじゃないか（ニヤリ） アヒルはスケールするよ Rubyのシグネチャ定義言語 後付の型定義言語なのでかなり柔軟に表現できる。TypeScriptの影が見えた・・・ openの型定義がとんでもないことになってる・・・闇が深い・・・ The future of the Bundled Bundler with RubyGems @hsbt RubyGemsの多要素認証を設定してください。🙏 Rubyを使うのにbundlerをまず入れなきゃ、みたいなのは体験が良くないのでやめたい Reducing ActiveRecord memory consumption using Apache Arrow @mrkn Apache Arrowを使ってActiveRecordのメモリ削減できるかという話 先日の発表は間違っていました 列指向。Pandasと言われて納得した。データフレームね。 Red Chainer and Cumo: Practical Deep Learning in Ruby @sonots @hatappi 現在のPythonのDNNスタックに対応するものがだいたいRubyにも出揃った Chainer-> RedChainer, TensorFlow -> TensorFlow.rb, MXNet -> MXNet.rb, NumPy ->Numo::NArray, CuPy -> Cumo ONNXを使った連携。ProtocolBuffer使う。 numoをcumoにしたら奇跡的に動いた！ Ruby3で3倍速くするという話をしてますけど、Cumoを使うと75倍速い（ドヤ） Optimization Techniques Used by the Benchmark Winners (Keynote) @jeremyevans0 SequalやRodaといったベンチマークで高性能を叩き出すフレームワーク作者の最適化術 執念は見習いたい Rubyが速くなれば万事解決(ぉぃ 3. 食事いつも何がでるか楽しみですが、今年は特にやばかったです。 3.1. 朝食二日目と三日目は朝食がでました。バイキング形式です。前日に飲みすぎて胃もたれしていたので少なめにしています・・・ 3.2. 昼食まさかの屋台！トップページが伏線だったとは・・・屋台は5軒出ていたのでパノラマ写真にしてみました。 タイムラプスにした方もいるようです。 10分タイムラプス #rubykaigi pic.twitter.com/o7WdPthxgI— ORGiL Mk.Sein (@orgil_) April 19, 2019 ビーフストロガノフと焼きラーメンを食べました。とても美味しかったですが、屋台の暖簾はフェイクだったので店を間違えました・・・Rubyラーメン食べたかった・・・ 一日だけお弁当にしてみました。これも美味しかったです。 3.3. 間食お茶やコーヒーはブースで入れてもらいました。特に八女茶の甘みと旨味のコラボレーションが忘れられない・・・ お菓子やフルーツも大量にGetできました。 3.4. Official Party in 川端商店街技術カンファレンスとしては前代未聞の商店街のアーケードをまるまる使った公式パーティーです。常人の発想じゃない(褒め言葉)ですね。震えが止まりません・・・ Yahooニュースにもなったみたいです。 このパーティーは本当に驚愕でしたが、控えめに言って最高でした。参加できてよかったです。 4. 令和時代のRubyKaigi最後の最後でまさかの令和ネタが飛び出ました。次の開催は長野県松本市です。この「松本」で開催されるRubyKaigiは名前的に縁起がよく、正しく令和時代の最初のRubyKaigiに相応しい場所と言えると思います。これがRubyKaigiで一番度肝を抜かれたネタでした。不意打ちすぎる・・・ 5. まとめRubyKaigiはフレンドリーで活気に満ちた素晴らしい国際カンファレンスです。この記事を読んでくれた方に、少しでも\u001cRubyKaigiの楽しさをお伝えできたなら幸いです。 参加者、関係者の皆様、お疲れ様でした！ 1.問題はExtraステージも含めて公開されているので興味がある方はチャレンジしてみてはいかがでしょうか？ ↩2.とは言っても一介の技術イベントで県知事が挨拶するのは異例のことだとは思いますが・・・ ↩3.Gitリポジトリの中にpdfのペーパーも入っていたので合わせて読もうと思います。 ↩4.メモの内容は登壇者の発言も自分の感想もごちゃまぜです。また、登壇者の発言と思しきものも自分の印象で若干誤変換が入っている可能性があります。従って実際のセッション参加者なら多少は意味のある内容ですが、それ以外の人は鵜呑みにしないでください。RubyKaigiのセッションはいずれYouTubeに上がると思われるので、それを見たあとでなら楽しめるかもしれません(笑)。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/04/23/ruby-kaigi-2019/"},{"title":"「エンジニアの心を整える技術」が予想以上に「技術書」だった件","text":"「エンジニアの心を整える技術」は実は気になって技術書展で一度は手にとったのですが、ちょうど開いたページがプロジェクト炎上中の様子を記述していて、また「プロジェクト炎上の火消しネタか・・・」と勝手に思い込んでしまいそっと本を閉じてしまいました。今まで何回かその手の本を読んだことがあったのですが、結局は体験談止まりで一回読めばお腹いっぱいというものがほとんどでした。 しかし、「エンジニアの心を整える技術」は一味違いました。著者の方がnoteで2章まで公開されていて、本の目次まで見てようやく勘違いに気づきました。この本は体験談から一歩進んだれっきとした「技術書」だったのです。 note（ノート）「エンジニアの心を整える技術」をnoteで2章まで無料公開します! #感想まとめ #技術書典6｜karamage@柿本 匡章｜notehttps://note.mu/karamage/n/n39f4cc266ba9 このnoteは、2019年4月14日に開催された「技術書典6」で頒布し、これまで600部以上(書籍＋PDF版)販売している「エンジニアの心を整える技術 - 誰でも実践できる心のリファクタリング術 - 」のPDF版のダウンロードページになります。 技術書典6での当日の販売の… 目次1. 過去の閉ざされし封印2. 再び邂逅する3. 3章まではIT業界「あるある」4. マインドフルネス5. アドラー心理学6. 人は本当に変わることができるのか？7. 「エンジニアの心を整える技術」を読み始める前の事前準備8. まとめ 1. 過去の閉ざされし封印技術書展6は自分も参加していてレポートはここに書いたとおりですが、ちょうどこの本の販売ブースの前を通ったときに「エンジニアの心を整える技術」というタイトルで目を惹かれました。しかも、サブタイトルが「誰でも実践できる心のリファクタリング術」だったので、大いに反応しました。エンジニアならみんな大好き「リファリクタリング」です1。検証しないわけにはいきません(笑)。 しかし最初に開いたページが、運悪くプロジェクト炎上ネタだったのが災いしました。過去この手のネタは読んだことがあったのですが、体験談止まりで「あるある」以外の感想しか出てこなかったのでこの本もきっと同じだろうなと決め込んでしまいました。 もうちょっと他のページも開いてみれば良かったのですが、「⽉の残業時間200 時間超えが続く」とか「ベテラン主任さん死亡」とか書いてあると自身の封印していた記憶をこじ開けてしまいそうで、「これ以上読んではいけない」と心が警鐘を鳴らしました。楽しいはずの技術書展でこれ以上「SAN値」を削られるわけにはいかなかったので、軽く笑顔で会釈をしながら本を元の位置に戻してその場を去りました。 このようにして「エンジニアの心を整える技術」とのファーストコンタクトは見事に失敗してしまいました。 2. 再び邂逅するそして先日のnoteの記事で目次を目にして、速攻で購入してしまいました。いや、本当にまた巡り会えてよかったです。私が感じたこの本の魅力は4章以降にありました。でも、3章までもいい内容なので順を追って振り返ってみたいと思います。 ただここからは多少のネタバレを含みますので、少しでもネタバレをするのが嫌な方は読後にお読みください。購入を迷われている方はぜひこの先を読んで購入の参考にしてください。 3. 3章まではIT業界「あるある」まず、1章にはこの本が必要となる背景が書かれています。「マインドフルネス」や「アドラー心理学」に反応できればこの本の価値が分かるのですが、そのことは後述します。2章/3章はプロジェクトの炎上案件と、IT業界のブラックな場面の紹介になっています。まぁ、業界経験が長ければ耳にしたことがあるネタが多くて、正直自分は「あるある」と頷いて終わりました。 内容は非常に良かったので、業界経験の浅い方にはぜひ読んでもらいたい内容です。 4. マインドフルネス4章ではこの本で是非読んでもらいたい「マインドフルネス」のことについて書かれています。マインドフルネスとは瞑想および瞑想によって得られる精神状態を指すようです。 ベトナムの僧侶ティク・ナット・ハンさんが瞑想法のキーワードとして⽤いたため、そこに⾄るための瞑想そのものを｢マインドフルネス｣と表現されています。Google やApple が社員研修に取り⼊れたことで、⼀気に有名になりました。 「エンジニアの心を整える技術」13ページより 瞑想というとちょっと迷信っぽく思いましたが、学術的な検証はされているそうです。 瞑想を⾏うことによって幸せを感じる脳内分泌物質とも呼ばれるセロトニンの分泌が促されることにより、⼼の落ち着きや満⾜感・充⾜感がもたらされるという学術的裏付けがあります。 「エンジニアの心を整える技術」59ページより 自分はヨガの文脈で過去に瞑想をしたことがありますが、運動やスポーツ以外の文脈での瞑想を思いつかなかったのでこれは新鮮でした。確かに瞑想は心を落ち着けるのでこれをエンジニアの生活に取り入れることに何ら支障はありません。 それでは具体的にどうやって取り入れていけばいいのかということが、本書の4章に書いてあるのでぜひ読んでみてください。エンジニア向けの瞑想術が書いてあります。自分がこの本を「技術書」だと思ったのはまさにこの部分で、学術的な裏付けを述べてそれを実践するための具体的な手法を述べているのが気に入りました。著者曰く「瞑想のコスパは最強」だそうですが、自分も同感です。 あと本格的にマインドフルネスを実践したい方には以下の本が本書のなかでオススメされていたので、自分も後で読んで見たいと思います。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"サーチ・インサイド・ユアセルフ――仕事と人生を飛躍させるグーグルのマインドフルネス実践法\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\\/images\\/I\",\"p\":[\"\\/41td8q2cgzL.jpg\",\"\\/31w8YWEptfL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E3%82%B5%E3%83%BC%E3%83%81%E3%83%BB%E3%82%A4%E3%83%B3%E3%82%B5%E3%82%A4%E3%83%89%E3%83%BB%E3%83%A6%E3%82%A2%E3%82%BB%E3%83%AB%E3%83%95%E2%80%95%E2%80%95%E4%BB%95%E4%BA%8B%E3%81%A8%E4%BA%BA%E7%94%9F%E3%82%92%E9%A3%9B%E8%BA%8D%E3%81%95%E3%81%9B%E3%82%8B%E3%82%B0%E3%83%BC%E3%82%B0%E3%83%AB%E3%81%AE%E3%83%9E%E3%82%A4%E3%83%B3%E3%83%89%E3%83%95%E3%83%AB%E3%83%8D%E3%82%B9%E5%AE%9F%E8%B7%B5%E6%B3%95-%E3%83%81%E3%83%A3%E3%83%87%E3%82%A3%E3%83%BC%E3%83%BB%E3%83%A1%E3%83%B3%E3%83%BB%E3%82%BF%E3%83%B3\\/dp\\/4862762271\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"}}); 5. アドラー心理学5章の「アドラー心理学」も見どころです。「アドラー心理学」は聞きかじった程度だったので、うまく説明ができなかったのですがこの章を読んでようやく腹落ちしました。 アドラー⼼理学は、他者を変えるための⼼理学ではないです。⾃分を変えるための⼼理学です。⼈間関係を改善したいと考える⼈が、周りを変えるのではなく、⾃分⾃⾝の考え⽅を変えればいいんだと気づくことが重要です。 「エンジニアの心を整える技術」71ページより それでは具体的にどうやって変えていけばいいのかということが本章で書かれています。あまり細かいことを書くとネタバレになってしまうので、自分が面白かったと思う節タイトルを抜き出してみました。以下を見て気になった方はぜひ本書を手にとって読んでみてください。自分はこれを読んでかなり心の中のもやもやがスッキリした感じがしました。 5.3.1 エンジニア的劣等コンプレックス 5.3.2 技術マウンティングは劣等コンプレックスの裏返し 5.5.2 ⾃分の課題、他⼈の課題を分ける 5.5.4 他者の期待を満たす必要はない 5.5.5 他者の課題に⼟⾜で踏み込まない 5.6.1 しかってはいけない、ほめてもいけない 5.6.2 評価ではなく、感謝する 5.7 エンジニアの承認欲求を否定する 5.8.1 エンジニアの約束された勝利の剣 5.10.1 ⼼理的安全性 6. 人は本当に変わることができるのか？ただちょっと気になった点もあります。本書では「人は変わることができる」と述べていますが本当にそうでしょうか？ 変えようと思えば、今この瞬間からでも、⼈は変わることができます。 「エンジニアの心を整える技術」73ページより 確かに行動は変えることができます。しかし行動した結果「自分を変えること」ができず、元の行動に戻してしまうこともあると思います。これは単に根性論の問題ではなく「変わりやすい部分」と「変わりにくい部分」があるからだと思います。 「変わりにくい部分」は、幼いころに身に着けた習慣や長いこと続けてきた習慣、長いこと触れてきた環境に対する反応等です。具体的には言語やコミュニケーションや性格、思考のクセなどです。これらは個人のアイデンティティと深く結びついており、考え方や行動を変えただけではなかなか変えることは難しいと思います。 例としては海外で長く暮らしてもその土地の言語が身につかない日本人は大勢いますし、ネイティブの感覚まで持っていける人はかなりの少数派です。そしてどんなに長く海外で暮らしても日本人としてのアイデンティティを捨て去ることは難しいです。もちろん絶対に変えられないわけではありませんが、そのためには「行動」と「環境」を変えた上で「専門家」のカウンセリングが必要です。その上で完全に変わったと確信できるまでには長い時間が必要な場合があり、変わり切る前に元の行動や環境にまた触れてしまった場合には台無しになる可能性もあります。「トラウマ」や「PTSD」も「変わりにくい部分」の例です。 「コミュ障」を克服したなると美談に聞こえるかもしれませんが、全ての人が同じことをして変われる保証がないことは十分に強調されるべきことです。「コミュニケーション」を支える基礎能力の獲得には訓練が必要です。「コミュニケーション」は「視覚」、「聴覚」、「読解力」等を駆使して脳で処理する高度な技術であり、幼い頃にこれらの訓練が十分でなかった場合には生涯に渡って身につかない可能性もあります。「目」が物理的に悪くなくても「耳」が物理的に悪くなくても、脳に処理回路がない場合にはコミュニケーションに必要な情報を引き出すことはできません。繰り返しますがコミュニケーション能力は高度な脳の処理であり、コミュニケーション能力の高さは幼いうちから親や環境からシャワーのようにコミュニケーションの訓練を「知らずの内に」受けてきた結果です。 そして「コミュニケーション」の基礎能力の訓練不足は、極端な例を持ち出さなくても一般家庭でも程度の差はあれ普通に起こり得ることであり、子供の頃に訓練してこなかった能力が大人になったら自然と身につくと考えるのは完全な誤解です。「コミュ障」を克服した人というのはコミュニケーションの基礎能力がある程度高く(幼少期の環境に恵まれていた)、行動や考え方を変えるだけ、もしくは個人的な努力の範囲内でなんとかなった一部の人だと考えるのが妥当だと思います。 私自身の主張としては自分の「変わりやすい部分」と「変わりにくい部分」を見極めて、「変わりやすい部分」で自分が気に入らない部分をまず変えるべきだと思います。その見極めのためにまず「行動を変えてみる」はありだと思います。そして、「変わりにくい部分」で自分が変えたいと思う部分があった場合はどうすればよいかというと、それは投資対効果で判断します。具体的には「行動」、「環境」、「専門家」を揃えた上で変わるために必要な訓練期間を想定して、かかる費用を算出してみます。そしてその費用を投資と見做して自分の人生の中で回収できるかを判断します2。ただ大抵の場合は「変わりにくい部分」を無理やり変えようとせず、「変えずにうまく付き合う」という選択肢を増やしたほうが幸せになれる気がしています。 7. 「エンジニアの心を整える技術」を読み始める前の事前準備さて、ちょっと水をさしてしまいましたが、本書は個人的には超オススメなのでぜひ多くの方に読んで頂きたいです。ただその前に事前準備を忘れないようにしてください。それは「SHIROBAKO」というアニメを見ることです。 TVアニメ「SHIROBAKO」公式サイトTVアニメ「SHIROBAKO」公式サイトhttp://shirobako-anime.com/記事の説明 これを見なくして本書を十分に味わうことはできません。SHIROBAKOには様々な名言やパワーワードが散りばめられていますが本書ではそれが多く引用されており、より実感を深めるのを手伝ってくれます。特に本書の最後に差し込まれた「あの画像」が明確なメッセージ性と熱量を伝えていて、涙しました。そして気づいたら再度SHIROBAKO全24話を徹夜で見直していました・・・ そういえば劇場版の制作も発表されました。今から待ち遠しくてたまりません。 8. まとめ本書を読んでみて意外と「技術書」だったなというのが読後の感想です。単なる体験談や自己啓発書とは一味違い、とても面白く読めました。ただ「技術書」にカテゴライズした場合に少し残念なのは、4章と5章には批判的な内容や注意事項がなかったことです。一般的にはどんなに優れた技術にも「長所」と「短所」があり、また適用範囲や副作用に対する注意事項や制限事項があるものですが、本書には「マインドフルネス」や「アドラー心理学」の良いところしか書いていないように見えたので少しフェアではないなと感じました3。以下まとめです。 本書はプロジェクトの炎上案件、IT業界のブラックな場面の紹介があり、業界経験の浅い人はぜひ読んでほしい 本書は「マインドフルネス」について書かれており、学術的な裏付けのある「瞑想」の効果の説明とそれを実践するための「技術」が述べられてる 本書は「アドラー心理学」について書かれており、自分を変えるための考え方や行動について書かれている しかし、「人は本当に変わることはできるのか？」という点で自分は疑問を持っている 「変わりやすい部分」と「変わりにくい部分」があると考えている 本書を読む前に「SHIROBAKO」を見るべき。これを見ずして筆者が伝えようとしたかったことを正しく受け取るのは難しい ここまでお読み頂き本当にありがとうございました。本書は本当におすすめです。2章までは無料で読めますので、まずは試し読みしてみてください。 note（ノート）「エンジニアの心を整える技術」をnoteで2章まで無料公開します! #感想まとめ #技術書典6｜karamage@柿本 匡章｜notehttps://note.mu/karamage/n/n39f4cc266ba9 このnoteは、2019年4月14日に開催された「技術書典6」で頒布し、これまで600部以上(書籍＋PDF版)販売している「エンジニアの心を整える技術 - 誰でも実践できる心のリファクタリング術 - 」のPDF版のダウンロードページになります。 技術書典6での当日の販売の… 1.私見です。自分の観測範囲の大体はそんな感じですが、観測範囲が偏っていることを否定しません。 ↩2.「変わりにくい部分」を無理に自分の行動や考え方だけで変えようとすると最終的にはメンタルに来るので十分注意して判断してください。「行動」、「環境」、「専門家」を「必要な期間」確保できない場合は「無理をせず妥協する」が最も合理的な判断です。 ↩3.単なる自己啓発書として見ればよくある内容ですが、エンジニアとして「技術書」として本書を見た場合には少し評価が落ちるという意味です。本書に対する注意点のひとつはこの記事の「人は本当に変わることはできるのか？」に書いたとおりです。さらに言えば「マインドフルネス」や「アドラー心理学」が他の自己啓発手法と比べてどのようなメリット・デメリットがあるのかを解説していれば、本書はより素晴らしいものになったと思います。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/05/11/mindfulness/"},{"title":"全プログラマに捧ぐ！図解「パターンマッチ」","text":"パターンマッチを使い始めてかなりの時間が経ちました。最初は関数型言語の一機能として触り始めましたが、徐々に関数型言語のユーザだけの玩具にしておくのは勿体ないと思うようになってきました。プログラミングにおいて、パターンマッチほど有用であるにもかかわらず普及が遅れている言語機能は他にないと思います。本記事ではその状況に一石を投じたく、一般のプログラマにも伝わるようになるべく図解で「パターンマッチ」を解説してみたいと思います。 目次1. はじめに2. パターンマッチとは3. パターンマッチとパターン4. パターンマッチの構造5. パターンマッチと照合6. パターンマッチと分解7. パターンマッチと変換8. パターンマッチと合成9. パターンマッチとパターンの重なり10. パターンマッチと広がる世界11. まとめ 1. はじめに本記事はプログラミング言語における「パターンマッチ」1という機能に着目して解説したものです。「パターンマッチ」は、switch文の強化版2であり、仮にパターンマッチを持たないプログラミング言語のユーザだとしても全プログラマが知っていて損はないアイデアだと思います。 2. パターンマッチとはパターンマッチは以下の図のように、入力データを「パターン」と呼ばれる特定の構造と照合して、データとパターンが適合(マッチング)した場合に分解して要素を取り出します。 パターンマッチの凄いところは、入力データの値だけではなく、構造にも着目してデータを抽出できることです。上記の図は入力データとして同じ整数の配列([1,6,5,2])を用いて、4種類のパターンを用いてパターンマッチを実行している様子です。1番目のパターンは特に説明は不要だと思うので、二番目のパターンから説明するとパターン([1,x,5,y])のように、構造から値を抽出しておきたい場所を変数にしておけば、データ構造を分解することができます。抽出した変数は後の変換処理で利用できます。3番目のパターンはパターンマッチが失敗していますが、理由は簡単でパターンと入力データの長さが異なるからです。4番目のパターンは再帰処理でよく使うパターンで配列の先頭とそれ以外にデータを分割できます。 このようにパターンマッチを用いると、データ構造をパターンを用いて分解して処理ができるので、様々なアルゴリズムが書きやすくなるというメリットがあります。 パターンマッチの構文はプログラミング言語ごとに異なりますが、概ね「入力」と「パターン」と「変換処理」を記述できます。以下はscalaを用いた一番シンプルなパターンマッチ構文の例です。 scalaのパターンマッチ構文123入力 match { case パターン => 変換処理} 変換処理では分解によって取り出した変数を利用した処理を書くことができます。そして、変換処理の結果自体がパターンマッチ全体の結果となります。パターンマッチに失敗した場合は一般的には例外が発生します。 最初の図の4つのパターンマッチをScalaで記述した例が以下になります。Scalaを知らなくても上記の図が理解できていれば、どんな処理をしているのか想像がつくと思います。 scalaを用いたパターンマッチ例12345678910111213141516171819val result1 = Seq(1, 6, 5, 2) match { case Seq(1, 6, 5, 2) => \"ok\"}// result1 = \"ok\"val result2 = Seq(1, 6, 5, 2) match { case Seq(1, x, 5, y) => x + y}// result2 = 8val result3 = Seq(1, 6, 5, 2) match { case Seq(x, y) => x + y}// 例外発生！val result4 = Seq(1, 6, 5, 2) match { case Seq(x, tail @ _*) => tail}// result4 = Seq(6, 5, 2) 3. パターンマッチとパターンパターンはいくつかの基本形に分類されますが、以下は自分が特に重要だと思う9つの基本形です3。パターンマッチで一番誤解を受けいているのがこの部分で、この基本形を十分に理解せずにはパターンマッチを使いこなしているとは言えないと思います。そしてこの基本形を使いこなせるようになればパターンマッチ、ひいてはプログラミングの世界が大きく広がると思います4。 基本形は組み合わせてより複雑なパターンを記述することができます。重要な役割を果たすのが入れ子構造にすることが可能なシーケンスパターンとタプルパターンです。 上記の複雑なパターンは「定数パターン」、「変数パターン」、「ワイルドカードパターン」、「シーケーンスパターン」、「タプルパターン」、「asパターン」、「パターンガード」の7つの基本形を組み合わせたパターンになります。このようにパターンは入れ子にすることで飛躍的に表現力が増し、応用範囲が広がります。さらにはパターンマッチで分解した結果を「変換処理」の中でさらにパターンマッチすることもできます。つまりパターンだけでなくパターンマッチ自体も入れ子にすることができます。以下の例はScalaを用いたパターンマッチを入れ子にした例です5。 scalaを用いたパターンマッチの入れ子の例1234567val result = Seq(Seq((true, 5), (false, 10)), Seq(), Seq((true, 1))) match { case a @Seq(Seq((true, _),(false, x)), _*) if x > 3 => a match { case Seq(_, y @ _*) => x + y.size }}// result = 12 ここまで読んでいただいた方はパターンマッチにおける基本形の重要さが理解できたと思います。そして一番最初に図解したパターンがどの基本形にあたるかも簡単に分かると思います。プログラミングの重要な目的の一つはデータを処理することだと思います。そしてパターンマッチはデータ構造をパターンで分析し処理するのにうってつけの機能です。従ってパターンマッチは間違いなくプログラミングの本質に迫る機能だと考えられます。 4. パターンマッチの構造プログラミング言語のパターンマッチは、以下の図のとおり入力と出力がある一種の関数と考えることができます。パターンマッチの内部は「パターン」と「変換処理」と呼ばれるユーザが定義するデータと「照合(check)」、「分解(destructure)」、「変換(transform)」と呼ばれる３つの工程から構成されています6。 上記の図は「シーケンスパターン」を用いて、整数の配列[1,2,3]の入力に対するパターンマッチを実行している様子を示しています。この入力のパターンマッチは成功して実行結果として5を返しますが、もし、仮に入力が[1,2]だった場合にはパターンの照合に失敗して赤矢印で示した「NG」へ行き、パターンマッチが失敗します。 パターンマッチを関数とみたときにこのように失敗する可能性がある場合は、失敗しない関数（数学的な関数、全関数とも言う）と区別して「部分関数」と呼ぶことがあります。部分関数を全関数にするにはパターンを網羅的にする必要があります。 5. パターンマッチと照合「照合」の役割は、以下の図のようパターンに適合(マッチ)する入力データを選別することです。そして適合したデータは次の「分解」のフェーズに送られます。 6. パターンマッチと分解「分解」の役割は、以下の図のようにパターンに従ってデータを分解して、変数に対応する値を入力データから見つけて変数に入れることです。分解の結果は次の「変換」のフェーズに送られます。 この分解はパターンマッチ構文以外でも見ることができます。例えば以下は擬似コードですが代入がシーケンスパターンのパターンマッチになっています。 12[a, *, b] = [1, 2, 3]// a = 1, b = 3 このように代入に見えてパターンマッチになっているケースもあるので、実は知らないうちにパターンマッチのお世話になっているかもしれません。 7. パターンマッチと変換「変換」の役割は、以下の図のように変換処理の変数に分解結果の変数を引き当てて、評価することです。評価した結果は出力としてパターンマッチ全体の結果になります。 8. パターンマッチと合成パターンマッチの合成には直列合成と並列合成があります。直列合成のイメージは以下の図のように通常の関数の合成のイメージと同じで前の関数の出力と後ろの関数の入力の型が合えば合成することができます7。 ソースコードの方が理解しやすい方がいるかも知れないので以下にScalaで２つのパターンマッチを直列合成をした例を記載します。 Scalaでパターンマッチの直列合成12345val match1 = (a:Int) => a match {case x if x % 3 == 0 => x * 2}val match2 = (a:Int) => a match {case x if x > 5 => x * 10}val composed = match1 andThen match2composed(3) // 60composed(5) // MatchError 以下はパターンマッチの並列合成です。並列合成は大抵の言語のパターンマッチ構文に組み込まれているので、あまり「合成」と意識することは少ないかもしれません。しかし直列合成と比較するとプログラミングの論理演算であるandとorと類似していることが分かると思います。つまり、直列合成の場合はパターンマッチが全て成功しないと合成されたパターンマッチが成功しないのに対して、並列合成ではパターンマッチが一つでも成功すれば、合成されたパターンマッチが成功します。 以下は直列合成の例を並列合成に書き換えたものです。結果が変わっているのがわかると思います。 Scalaでパターンマッチの並列合成123456val composed = (a:Int) => a match { case x if x % 3 == 0 => x * 2 case x if x > 1 => x * 10 }composed(3) // 6composed(5) // 50 9. パターンマッチとパターンの重なり並列合成のパターンマッチの場合には、パターンの重なりを意識することが重要です。以下の図は整数の集合における基本的なパターンの重なりを分類したものですが8、このようにパターンを図で思い描けるようになるとパターンの設計に非常に役に立ちます。 並列合成では上のパターンマッチから順番に照合されるため、パターンに重なりがあると上のパターンマッチが優先されることになります。特に包含関係にあるパターンは上に大きいパターンを持ってくると下のパターンが隠れてしまって全く照合されない事態になるので、注意が必要です。 またパターンを網羅的にすることでパターンマッチの失敗がなくなり、無意識にバグを作り込むことを防ぐことができます。従ってパターンマッチは特に理由がない場合は網羅的にすることが望ましいです。網羅的にするのに適した基本パターンは「変数パターン」と「ワイルドカードパターン」になるので、並列合成の一番最後にこれらのパターンを入れることを検討してください。 10. パターンマッチと広がる世界従来パターンマッチはHaskellに代表されるような関数型言語の十八番でしたが、現在では関数型プログラミングに源流を持たないプログラミング言語でもパターンマッチを実装するようになってきました。C#では7.0以降でパターンマッチが利用可能であり、Rubyでもすでにtrunkではパターンマッチが利用できます9。また比較的新しく出た言語は最初からパターンマッチが使える場合が多く、パターンマッチの世界は広がり続けています。仮にお気に入りの言語にパターンマッチがなかったとしても諦めるのはまだ早いかもしれません。使い勝手は言語に統合された機能よりは劣るかもしれませんが、パターンマッチのためのライブラリも数多く公開されています。 言語としての変わり種は Egisonです。「直感をそのまま表現するパターンマッチング 」という謳い文句で、パターンマッチとして非常に面白いので気になった方はぜひ触って見てください。 このように少しずつですが着実にパターンマッチが使える言語が増え続けているのは、パターンマッチがプログラミング全般で非常に用途が広く、使いこなすことで直接的にプログラマの能力を拡張するからだと思っています。以下の図は思いついたパターンマッチの用途です。 11. まとめ以下、「パターンマッチ」のまとめです。 「パターンマッチ」は入力データを「パターン」と呼ばれる特定の構造と照合して、データとパターンが適合した場合に分解して要素を取り出す 「パターンマッチ」のパターンには9つの基本形があり、基本形を組み合わせてより複雑なパターンを表現できる 「パターンマッチ」は「照合」、「分解」、「変換」から構成される 「パターンマッチ」の合成方法は２種類ある 「パターンマッチ」ではパターンの重なりを意識する必要がある 「パターンマッチ」は様々な言語で利用可能になってきている 「パターンマッチ」はプログラマの能力を直接的に拡張する パターンマッチは関数型プログラミングでは特に再帰関数と相性が良く欠かせない存在ですが、一般のプログラマへの浸透具合はいまいちと感じたので、関数型プログラミングの文脈からなるべく切り離して解説をしてみました。 本記事が「パターンマッチ」の理解と普及の一助になれば幸いです。 1.ここで言うぱ「パターンマッチ」はパターンマッチを実装しているプログラミング言語の総和でイメージしており、特定のプログラミン言語のパターンマッチを意味していません。従って、本記事で解説しているパターンマッチの機能や用語は個別の言語でそれぞれ異なる場合があります。 ↩2.ここで言う「switch文」とはC言語のswitch文をイメージしています。また、ここではswitch文とパターンマッチとの歴史的な繋がりではなく、機能的な包含関係について「強化版」という表現をしています。 ↩3.パターンの記述は疑似言語で記載しています。また、パターンマッチの対応を謳っているプログラミング言語でも、いくつかの基本形が使えない場合があります。しかし上の6つのパターンはだいたい使えるのではという感触です。 ↩4.9つの基本形は非常に重要なのでA４サイズで収まりがいいように図を工夫しました。チートシートとしてご利用ください。机や冷蔵庫に貼って忘れないようにするのもいいかもしれません(笑)。 ↩5.ネストができることを示すだけの例なのでコードに特に深い意味はないです。 ↩6.工程の呼び方はいろいろあります。例えばdestructureとextractと呼んだりtransformをmapと呼ぶ場合もあるようですが、ここでは自分が一番分かりやすいと思った表現を採用しています。また、「照合」と「分解」だけを指して「パターンマッチ」と呼ぶ流儀も存在しますが、多くのプログラミン言語のパターンマッチの構文には変換処理も含まれるので、この記事では「変換」も含めて「パターンマッチ」と呼びます。 ↩7.実際には型だけでなく関数の定義域や値域を考慮する必要がありますが、数学的な話になってくるので詳細は割愛します。 ↩8.整数にしたのはイメージが簡単にできると思われるためであり、ここで説明するパターンの重なりは整数以外の集合にも同じことが言えます。 ↩9.正式にはRuby 2.7で利用可能になる予定です。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/04/30/understanding-pattern-matching/"},{"title":"gRPC UIを使ってみんなが触れるgRPCの遊び場を作りました","text":"gRPCは近年非常によく使われるようになったRPCフレームワーク1ですが、その柔軟なメッセージフォーマットに対応するリッチなGUIクライアントに欠けていました。REST APIでいうPostman的な存在です。このようなツールがなければgRPCを利用した開発が面倒なのですが、その問題は現在においてはほぼ払拭されたといっても過言ではありません。gRPC UIが登場したからです。 (2019/10/4 追記)gRPC UI playgroundの運用に利用していたArukasが2020年1月31日で終了するようです。今後gRPC UI playgroundをどうするかは検討中です。 (2020/2/2 追記)gRPC UI playgroundの運用に利用していたArukasが2020年1月31日で終了したので、gRPC UI playgroundを以下の場所に移動しました。 http://grpcui.hinastory.com 目次1. TL;DR2. はじめに3. gRPCとは4. gRPC UIについて5. gRPC UIで遊ぼう！ 〜 gRPC UI playground 〜6. grpcui-playgroundの作り方6.1. Dockerfileを書く6.2. GitHubでDockerfileを公開する6.3. Docker Hubでイメージをビルド & 公開する6.4. ArukasでDockerコンテナを公開する7. まとめ8. 参考文献 1. TL;DR gRPC UIで作成した遊び場はこちらです 👉 gRPC UI playground gRPC UIはgRPCサーバへの要求と応答がWebで簡単にできるので、今後のgRPCを利用した開発に広く使われるツールとなりそうです gRPC UI playground´\u001eは以下のサービスを利用して、無料で立ち上げました。当面はみんなが遊べるようにしておくつもりです GitHub Docker Hub Arukas Arukasは正式に終了しました。Arukas関連の説明はそのままにしておきますが試すことはできません。gRPC UI playgroudは以下の場所に移転して稼働させています。 http://grpcui.hinastory.com 2. はじめに本記事はgRPCのWeb UIであるgRPC UIの紹介記事です。gRPCに興味がある方、gRPCを利用している方を対象にしています。実際にgRPC UIを触れる遊び場も作ったのでgRPCに興味がある方は遊んでいってください。また本記事ではこの遊び場をどうやって作ったのかも簡単に解説します。 3. gRPCとはgRPCはGoogleが開発してオープンソースとして公開したRPCフレームワークです。IDL2としてProtocol Buffers3が利用可能です4。gRPCは基本的には以下の図ように単純で、クライアントから要求(Proto Request)を送ってサーバが応答(Proto Response)を返すというものです。クライアントやサーバの記述には、Protocol Buffersが対応している様々な言語(C++, Ruby, Java等)が利用可能です。 What is gRPCより引用 gRPCは上記のように単純な要求/応答型以外にも様々なやりとりに対応できます。具体的には以下の４つです。 Unary RPC(単純な要求/応答型) Server streaming RPC(サーバが複数の応答を返すことができる) Client streaming RPC(クライアントが複数の応答を返すことができる) Bidirectional streaming RPC(クライアントとサーバがそれぞれ複数の要求/応答ができる。一般的な双方向通信) 上記のような柔軟な呼び出しはHTTP/25の基盤に支えられて実現されています。 4. gRPC UIについてgRPC UIはgRPCサーバとブラウザでやりとりできるツールです。百聞は一見にしかず。以下の画面を御覧ください。 この画面を見て震えたのはやはり要求の入力画面(Request Form)の充実っぷりです。gRPCは自分でデータの型を定義して入れ子のようなメッセージを定義できますが、そのような入力も簡単にできます。上記の図のではtest.TestMessageというメッセージの中にperson(test.Personというメッセージの型)が内包されています。また複数のデータが入力可能なフィールドは「Add Item」で簡単に追加できます。その他の注目すべき点は赤字で説明を入れておいたので参考にしてください。基本的にはgRPC(Protocol Buffers)で定義されている入力形式はほぼ網羅されています。 また、驚いたのはgRPCには基本的なスカラ値(stringやint64等)以外にも「Well-Known Types」と呼ばれる型が定義されていますが、gRPC UIにはこれらにもリッチなインタフェースが提供されていたことです。以下の画面はtimestamp型に対するインタフェースです。 またリッチなフォームだけではなく、JSON形式でリクエストを投げることもできます。 JSON形式で便利なのはコピペが容易なところです。また省略されているフィールドは出力されないので見やすくなります。フォーム型の編集画面とも連動しているので、非常に使いやすいです。リクエストは「Invoke」ボタンを押すことでサーバに投げることができます。このボタンはフォーム型のリクエストにも一番下にあります。以下が「Invoke」を押して返ってきた応答になります。 この画面も特に説明がいらないくらい分かりやすいものになっています。gRPC UIで対応できないのは双方向の複雑なやり取りです。これは従来どおりCUI等を利用して行うしかありません。 5. gRPC UIで遊ぼう！ 〜 gRPC UI playground 〜さて、ここまで来たらgRPC UIで遊んでみたくなったと思います。以下にみんなが遊べるように「遊び場」を立ち上げたのでぜひいろいろ遊んでみてください。 gRPC UI playground gRPC UIはバックグラウンドで起動しているgRPC UIのテストサーバ(testsvr)とやりとりしています。プロトコルは「test.proto」、サーバの実装は「testsvr.go」に記述されています。実際にgRPC UIを動かしながらソースやIDLを読むとgRPCの理解が深まると思います。 メソッドに関しては「DoManyThings」が全部詰めになっていて、残りのメソッドはそれを機能単位で分割したものになっています。たぶん・・・ 6. grpcui-playgroundの作り方さて、ここからはおまけです。上記の遊び場(以降、grpcui-playgroundと記載)の構築方法の興味がある方を対象に簡単に説明したいと思います。grpcui-playgroundは3つのサービスを利用して無料で立ち上げました。以下がその構成になります。 上の図をざっくり説明すると、基本的にはDockerを使ってサービス提供しています。一つのコンテナの中にgPPC UIとgRPC Server(testsvr)という二つのプロセスが動いています。一つのコンテナに二つのプロセスはあまりお行儀がよくありませんが、今回は遊び場ということと無料で立ち上げることを重視したのでこのような構成になっています。利用したコンテナホスティングサービスはArukas6になります。今回はFreeプランを利用しています。Arukasにコンテナイメージを提供するためにはDocker Hubも利用しました。Docker HubにはGitHubと連携しておくとGitHubの変更を検知してコンテナをビルドする機能があるのでそれも利用しています。 6.1. Dockerfileを書くDockerfileを書くときの注意点は二つです。まず、ビルドイメージを小さくするためにAlpine Linux7をベースイメージにします。次にgRPC UIが提供しているテストサーバ(testsvr)はgoで書かれているのでgoのビルド環境が必要です。ただしgoのビルド環境は動作には必要ないのでマルチステージビルドを活用して、なるべく小さなイメージにするようにしましょう8。 Dockerfile12345678910111213141516171819202122FROM golang:1.12.7-alpine as build-envMAINTAINER hinastoryWORKDIR /testsvrCOPY testsvr /testsvrRUN apk update && apk add --no-cache gitRUN go buildRUN go get -x github.com/fullstorydev/grpcuiRUN go install -x github.com/fullstorydev/grpcui/cmd/grpcuiFROM alpineWORKDIR /COPY --from=build-env /testsvr/testsvr /bin/testsvrCOPY --from=build-env /go/bin/grpcui /bin/grpcuiCOPY start.sh /start.shEXPOSE 8080ENTRYPOINT [ \"/start.sh\" ] 6.2. GitHubでDockerfileを公開するここは特に説明はしませんが、GitHubに公開レジストリを作成し、Dockerfileとtestsvrのソースと起動スクリプトをpushします。pushした自分のリポジトリは以下になります。 6.3. Docker Hubでイメージをビルド & 公開するDocker Hubでイメージをビルドするのには、GitHubと連携しておいたほうが楽です。GitHubと連携しておけばGitHubのレポジトリをDocker HubのUIから選択できるようになり、GitHubにpushするだけでDockerfileをビルドしてくれるようになります。 注意すべき点はDockerイメージのタグの付け方です。デフォルトだとブランチを監視してlatestタグを付けるようになっていますが、Arukasはイメージをキャッシュする場合があるようなのでバージョンが入った適切なタグをつけることが望ましいです。そしてDockerのイメージタグはGitHubのタグと連動しているとよいと思われます。 Docker Hubでその設定をするのは簡単でDockerHubのリポジトリのBuildsの画面からビルドルールを設定でき、Source TypeをTagにして、SourceとDocker Tagを正規表現で記述することでお望みのタグが構成できると思います。またここでDockerfileの位置やビルドコンテキストも指定可能なので、少し複雑なリポジトリ構成でも対応出来ます。 ここの設定が終わったらGitHubにタグをpushしてビルドが始まるか確認します。基本的には検知はすぐに行われてIn Progressの状態にすぐに変わりますがビルドが終わるまでには時間がかかるので、コーヒーでも飲んでまったりと待ちましょう(笑)。自分のコンテナイメージは以下に公開してあります。 hinastory/grpcui-playground - Docker Hub 6.4. ArukasでDockerコンテナを公開する最後にArukasuでコンテナを公開します。FreeプランはO.1vCPUと128MB RAMの非力な環境ですが、今回のような遊び場には充分だと思われます。また「転送量課金」がないのでDDoSとかの心配がまったくいらないのも嬉しいところです。もちろん最悪サービスは落とされますが、料金に怯える心配は無いわけです。ちなみにFreeプランでも電話認証とクレジットカードの登録は必須です。このアカウント登録が最大の難関でコンテナの起動は驚くほどあっけなく終わりました。アプリ起動の詳細は「アプリの作成 – Arukas Help Center」を参照してください。以下は起動したコンテナの管理画面です。Endpointはアプリの再起動で変わることが無いURLです。Portはインスタンスの起動毎に変わります。ちなみにgRPC UIは8080ポートでHTTPでサービスを公開していますが、エンドポイントではHTTPSになっているので自動でArukasがHTTPSに包んでくれるみたいです。 7. まとめ本記事では、gRPCサーバとブラウザでやり取りできるgRPC UIを紹介しました。gRPC UIは非常に便利なので今後gRPC関連の開発で広く使われていくものと思われます。そして実際にgRPC UIを触れる遊び場を作成して以下に公開しました。 遊び場 gRPC UI playground プロトコルは「test.proto」、サーバの実装は「testsvr.go」 遊び場は当面は公開予定ですが、ある日突然告知なしで死ぬ可能性があるのでご容赦ください Dockerfile hinastory/grpcui-playground -GutHub コンテナイメージ hinastory/grpcui-playground - Docker Hub また、おまけとしてその遊び場の作り方を(無料でコンテナサービスを立ち上げる手順)を簡単に説明しました。本記事がgRPCを理解し、より便利に使えるようになるための一助になれば幸いです。 8. 参考文献 gRPC Documentation Developer Guide | Protocol Buffers RFC 7540 - Hypertext Transfer Protocol Version 2 (HTTP/2)(日本語訳) about Alpine Linux Docker Hub Documentation Arukas Help Center 遠隔手続き呼出し - Wikipedia インタフェース記述言語 - Wikipedia 1.RPC(Remote Procedure Call)は、簡単に言えばプログラムの中から内部の関数を呼ぶのと似たような感覚で、外部のネットワーク上の関数や手続きを呼べるようにする技術のことです。他のRPCにはSOAPやJSON-RPCなどがあります。 ↩2.IDLはインタフェース記述言語(Interface Description Language)のことです。簡単に言えば関数呼び出しの宣言部分を定義する際に使う言語です。 ↩3.Protocol BuffersはGoogleが開発したシリアライズフォーマットです。IDLは独自の言語でシンプルで分かりやすいのが特徴です。IDLのファイルは.protoの拡張子を持っており、protocというコンパイラを用いてIDLファイルからGo言語やRuby、Python、Java等様々な言語のバインディングを生成できます。現在のProtocol Buffersのバージョンは3になります。 ↩4.JSONも利用可能みたいですが、自分は使ったことはありません。 ↩5.HTTP2はHTTP/1.1の後継バージョンでRFC7540で定義されています。HTTP/1.1と比べてヘッダーの圧縮やバイナリメッセージ構造、双方向通信のサポート等様々な改善がなされています。 ↩6.Arukasはさくらインターネットが提供しているDockerのホスティングサービスです。Arukasを反対から読むと・・・ ↩7.Alpine Linuxは軽量、シンプル、セキュアをコンセプトにしたLinuxディストリビューションです。組み込み系で実績のあるmusl libcとbusyboxをベースにしています。その軽量さからコンテナのベースイメージとしてよく利用されています。 ↩8.最初は無邪気にベースイメージをstretchにしてシングルステージでビルドしたため、イメージのサイズが400MBを超えてしまいました・・・現在のサイズは 18MBです。特にパブリックなレジストリに登録する際はコンテナイメージのサイズに充分気を配り、リソースを無駄にしないように心掛けましょう(自戒)。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/07/27/grpcui-playground/"},{"title":"夢と希望とアイディアに満ち溢れたモノ作りの祭典、Maker Faire Tokyo 2019に行ってきました","text":"今年もMakerさんたちの熱い作品とパフォーマンスから目が離せないお祭りがやって来ました。遊園地や花火大会を否定するつもりはありませんが、やっぱりいちばん楽しくて嬉しいのは自分の手を動かして作ったモノが動いたときではないでしょうか？ そんな夢と希望とアイディアに満ち溢れたモノ作りの祭典、Maker Faire Tokyo 2019に行ってきました。 Maker Faire Tokyo 2019Maker Faire Tokyo 2019 | Maker Faire Tokyo 2019 | Make: Japanhttps://makezine.jp/event/mft2019/Maker Faire Tokyo 2019は8月3日（土）、4日（日）東京ビッグサイトで開催！ユニークな発想と新しいテクノロジーの力で作り出される作品や、Makerたちに会いにいこう 目次1. 日時・場所2. Maker Faireとは3. 作品紹介4. Make:本の紹介5. まとめ6. 参考文献 1. 日時・場所日時と場所は以下のとおりです。自分は初日に当日券を買って入場しました。開場は12:00からだったので20分ほど余裕を持ってビッグサイトに到着したのですが、すでに50m近くの長い行列ができていました。 日時 2019年8月3日(土) 12:00～19:00 2019年8月4日(日) 10:00～18:00 場所 東京ビッグサイト（西3・4ホール） 入場料 当日：大人 1,500円、18歳以下700円 紛らわしいのはちょうどMaker Faire Tokyoと同日の隣にMF-TOKYOという全く別のイベントが開催されていたことです。二枚の看板が並んでいるところを見ると狙ってるのかと勘ぐってしまいそうです（笑）。 2. Maker FaireとはMaker Faireは日本最大級のモノ作りの祭典です。いわゆる「自作」と「テクノロジー」がテーマで新しい技術で面白いものを作っている「Maker」さんたちが集うお祭りになっています。実際にはただ展示を見るだけではなく様々なパフォーマンスや体験型のコーナや物販やプレゼンもあり、子供から大人まで様々な楽しみ方ができる空間になっています。以下は去年の様子です。 この祭典はもともと「Make」という、アメリカ発のテクノロジー系DIY工作専門雑誌があって、Makerムーブメントが起こりその流れで誕生しました。Maker Faireは世界中で行われており、Maker Faire Tokyoはその中の一つになります。 3. 作品紹介それではMakerさんの作品の中で自分が目に止まったものをいくつかご紹介したいと思います。 まず最初はダンボールでできたこの作品です。後ろの扇風機の風を受けて羽が回転して、それを四足歩行の動力に変換して前進してきました。同時に目蓋が開いたり閉じたりしてとてもかわいかったです。危うく契約してしまうところでした（笑）。 次に目に止まったのがGame Boyです。単純に懐かしくてテトリスで遊んでいた子供の頃を思い出しました1。もちろん目に止まったのは懐かしいからだけではなく、テーブルいっぱいに敷き詰められた圧巻の光景が目に飛び込んできたからです。それも全て電源が入った状態です！危うく「こっち見んな ！( ﾟдﾟ )」と叫んでしまいそうでした（笑）。 意外と驚いたのが紙飛行機のパフォーマンスです。有名なのでご存じの方もいるかも知れませんが、紙飛行機の飛距離世界記録を持つチャンピオンで「The Paper Airplane Guy」という異名を持つJohn M. Collinsさんです。手のひらのサポートだけでくるくろと回り続ける様子は正しく紙飛行機の概念を覆されました。 その他にもブーメランのように戻って来る紙飛行機や、鳥のように飛ぶ紙飛行機などもあってどれも面白かったです。YouTubeで動画もたくさん公開されているので興味がある方はぜひ一度ご覧ください また、紙飛行機本の販売も行っていました。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"世界チャンピオンの紙飛行機ブック (Make: Japan Books)\",\"b\":\"オライリージャパン\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/41qNbt9Bk6L.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E4%B8%96%E7%95%8C%E3%83%81%E3%83%A3%E3%83%B3%E3%83%94%E3%82%AA%E3%83%B3%E3%81%AE%E7%B4%99%E9%A3%9B%E8%A1%8C%E6%A9%9F%E3%83%96%E3%83%83%E3%82%AF-Make-John-M-Collins\\/dp\\/4873118832\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"mV9XS\"});リンク 普段はオライリー本は電子書籍でしか買わないのですが2、サインがもらえるとのことでついつい現物購入してしまいました。どこまでも飛んでいきたい・・・ 次にご紹介したいのがロボットプロレスです。特に目を引いたのが動きが非常に切れが良くコミカルなことです。下の写真は本当に見ていて笑いました。その他にもバックドロップをかけたり、ころんだ後に起き上がる様子とか見ていて非常に飽きませんでした。ロボット制御もここまできたのかと感じさせる光景でした。 そして、とうとうやってきました。キーボード 沼 島です。去年と比べて本当に増えたなぁというのが正直な感想です3。最近は定期的に届くキーキャップを着せ替えたり、オイルを塗り塗りしたりしかできてなかったのでもう少し真面目に自キ活(自作キーボード活動)しようかなと思いました。夏休みの宿題が一つ増えてしまった・・・ これ、説明しなくても分かるやつですよね・・・好きな人には堪らないアレです。 これも純粋に純粋に凄いと思ったものです。8方向からボールを打ち出す装置ですが、中央でも交差してもボールがぶつからないように制御されています。 実際に手を動かしたくて興味を持ったのが以下の深セン生まれのM5 Stackです。Raspberry Pi 4でおあずけくらっているので買おうと思っていたのですが、残念ながらお目当てのM5Stick-Cは売り切れていました・・・ 「School Maker Faire」という学園祭のノリのコーナーもありました。学生さんたちが一生懸命説明をしていました。 子どもたちに人気なのはやはり体験型のワークショップでした。「分解」や「はんだづけ」とかを楽しそうにやっている子どもたちや親子連れでいっぱいでした。 最後の紹介がMaker Faireのコスプレイヤーさんたちです。本当に観ているだけで幸せになれる一枚でした。 4. Make:本の紹介せっかくなので、電子工作を始めてみたい方のためにオライリーから出版されているMake:本を3冊ご紹介します。まずは電子機器の仕組みを知らないとどうにもならないので「エレクトロニクスをはじめよう」で基礎を学んで、次に本命の「Raspberry Piをはじめよう」で電子工作の大御所とも言えるRaspberry Piに入門します。最後に「ハンダづけをはじめよう」で意外と奥深い「ハンダづけ」と友達になるというシナリオです4。電子工作に興味を持った方はぜひチャレンジしてみてください。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"エレクトロニクスをはじめよう (Make: PROJECTS)\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/51PkUgk%2B6RL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E3%82%A8%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AD%E3%83%8B%E3%82%AF%E3%82%B9%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%82%88%E3%81%86-Make-Forrest-Mims-III\\/dp\\/4873118271\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"4iT74\"});リンク (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"Raspberry Piをはじめよう 第3版 (Make: PROJECTS)\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/51ahU-5w3NL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/Raspberry-Pi%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%82%88%E3%81%86-%E7%AC%AC3%E7%89%88-Make-PROJECTS\\/dp\\/487311831X\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"ifUmj\"});リンク (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"ハンダづけをはじめよう (Make: PROJECTS)\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/41V0dIQZ7IL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E3%83%8F%E3%83%B3%E3%83%80%E3%81%A5%E3%81%91%E3%82%92%E3%81%AF%E3%81%98%E3%82%81%E3%82%88%E3%81%86-Make-PROJECTS-Marc-Vinck\\/dp\\/4873118522\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"99SFy\"});リンク Maker Faireで個人的に興味を持った本は先行発売された「メイカーとスタートアップのための量産入門」です。中身をチラ見して面白そうな内容で即買いしようかとも思いましたが、「オライリーは電子書籍」ポリシーが発動したのでグッと堪えました。買うことは確定なので早く電子版が出てほしいです・・・ (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"メイカーとスタートアップのための量産入門 ―200万円、1500個からはじめる少量生産のすべて (Make: Japan Books)\",\"b\":\"オライリージャパン\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/513cIgLTe%2BL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E3%83%A1%E3%82%A4%E3%82%AB%E3%83%BC%E3%81%A8%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E9%87%8F%E7%94%A3%E5%85%A5%E9%96%80-%E2%80%95200%E4%B8%87%E5%86%86%E3%80%811500%E5%80%8B%E3%81%8B%E3%82%89%E3%81%AF%E3%81%98%E3%82%81%E3%82%8B%E5%B0%91%E9%87%8F%E7%94%9F%E7%94%A3%E3%81%AE%E3%81%99%E3%81%B9%E3%81%A6-Make-Japan-Books\\/dp\\/4873118840\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"dxcc0\"});リンク 5. まとめ本記事では夢と希望とアイディアに満ち溢れたモノ作りの祭典、Maker Faire Tokyo 2019を紹介しました。この祭典に来ると必ず新しいアイディアと何かを作ろうというモチベーションをもらい、Makerへの道へと繋がっていきます。まぁ、それが沼への第一歩でもあるわけですが、モノづくりの感触は一生何かの役に立つと思います。だからまだ自作(DIY)に踏み出していない人はぜひ何でもいいので、手を動かしてみてください。自作は失敗しても誰も叱ったりしないので、多くの学びと喜びを得ることができるでしょう。 この記事を読んでくれた方々に少しでもMaker Faireとモノづくりの楽しさをお伝えできなたら幸いです。 {If you can imagine it, you can make it.} 6. 参考文献 はじめての方へ | Maker Faire Tokyo 2019 | Make: Japan プレスの方へ | Maker Faire Tokyo 2019 | Make: Japan 1.今はTETRIS 99で遊んでいるので、子供の時とあまり変わっていない気もしますが・・・ ↩2.この本はMaker Faireの先行販売で正式発売は8/7です。 ↩3.去年は後少しのところでMint60が売り切れて、コミケでリベンジを果たしたのを思い出しました・・・ ↩4.このシナリオは大人向けです。子供向けには年齢に適した電子工作キットがたくさん販売されているのでいろいろ探してみてください。子供と一緒に大人も楽しめるものは「obniz」、「mBot」、「micro:bit」等があります。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/08/04/maker-faire-tokyo-2019/"},{"title":"Scala 3、Pythonのようにインデントベースの構文で書けるようになるってよ！","text":"ここ数年でインデントベースの記述は広くプログラマ界隈で受け入れられるようになってきました。プログラミング言語ではPythonの成功が大きく、ドキュメントではmarkdownとyamlが広く普及しています。そしてScala 3でもとうとうその波に乗ろうという動きが見えてきました・・・ (2019年9月28日追記・更新: 追記内容はここを見てください)(2019年11月16日追記・更新: 追記内容はここを見てください) 目次1. TL;DR2. まずはコードを御覧ください3. コードの解説3.1. 定義3.2. if式3.3. for式3.4. match式3.5. ラムダ式4. エンドマーカー4.1. インデントマーカー:5. 設定と書換え5.1. @main関数5.2. 今すぐ試してみたい!6. インデントベース構文の状況7. 最後に8. 参考文献9. 更新内容9.1. 2019年9月28日の更新内容9.2. 2019年11月16日の更新内容 1. TL;DR Scala 3のリサーチコンパイラであるDotty 0.18.1-RC1にインデントベースの構文が実装されました Dotty 0.19.0-RC1の変更に合わせて修正しました Dotty 0.20.0-RC1の変更に合わせて修正しました インデントベースの構文はまだ提案段階でありScala 3の正式な仕様に決定したわけではありません 今後機能が変化したり、機能が採用されなかったりする可能性も十分あります というか反対意見の方が多いです 従来の括弧ベースの構文も混ぜて使えます この記事はインデントベース構文の紹介記事です。この構文の良し悪しについては触れていません 2. まずはコードを御覧くださいウソみたいだろ・・・Scalaなんだぜ、それ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364object IndentBasedExample enum Day case Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday def isWeekend: Boolean = this match case Saturday | Sunday => true case _ => false def fromString(str: String): Day = try Day.valueOf(str) catch case _: IllegalArgumentException => throw new IllegalArgumentException(s\"$str is not a valid day\") end try end fromString trait A with def f: Int class B with def g: Int = 27 class C(x: Int) extends B with A with def f = x type T = A with def f: Int def use(dayString: String) = val day = fromString(dayString) if day.isWeekend then println(\"Today is a weekend\") println(\"I will rest\") else println(\"Today is a workday\") println(\"I will work\") if (day == Day.Wednesday) println(\"Today is a Wednesday\") println(\"Bad Day\") println(s\"B().g is ${B().g}.\") val optNum = for x println(\"bigger than 4\") case _ => println(\"Other\") val z = List(2, 3, 4) map: x => y = y - 1 y * y z.foreach: println@main def example: Unit = IndentBaseExample.use(\"Monday\") いつもはこんなコードを書いていたはず・・・ 従来のコード1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677object BraceBasedExample { enum Day { case Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday def isWeekend: Boolean = this match { case Saturday | Sunday => true case _ => false } } def fromString(str: String): Day = { try { Day.valueOf(str) } catch { case _: IllegalArgumentException => throw new IllegalArgumentException(s\"$str is not a valid day\") } } trait A { def f: Int } class B { def g: Int = 27 } class C(x: Int) extends B with A { def f = x } type T = A { def f: Int } def use(dayString: String) = { val day = fromString(dayString) if (day.isWeekend) { println(\"Today is a weekend\") println(\"I will rest\") } else { println(\"Today is a workday\") println(\"I will work\") } if (day == Day.Wednesday) { println(\"Today is a Wednesday\") println(\"Bad Day\") } println(s\"B().g is ${B().g}.\") val optNum = for { x println(\"bigger than 4\") case _ => println(\"Other\") } val z = List(2, 3, 4) map { x => { val y = x - 1 y * y } } z.foreach(println) }}@main def example: Unit = { IndentBaseExample.use(\"Monday\")} 3. コードの解説Pythonでインデントを用いてブロックを作る場合は改行の前に「:」が付きますが、Scalaの場合はもっと多くのキーワードがインデント構文の開始の合図になり得ます。 : = => true case _ => false enumはScala 3(Dotty)の機能でScala 2にはありません。気になる方は「Enumerations」を参照してください。 また、定義の行末を以下のようにwithで終わることもできます。 1234567891011trait A with def f: Intclass B with def g: Int = 27class C(x: Int) extends B with A with def f = xtype T = A with def f: Int withはオプションですが、インデント構文を使う場合は、基本的につけたほうが良さそうです。例えば、以下のようにクラスBの定義で次の行にインデントをつけ忘れた場合、withがあるとコンパイラが構文エラーにしてくれます。もし、withをつけていなかった場合はエラーにはならずに、defから始まる関数定義はクラスBに所属するのではなくクラスBと同じ名前空間のメソッドとして定義されてしまいます。 12class B with // `with`をつけているのでインデントを次行でつけ忘れても構文エラーになるdef g: Int = 27 3.2. if式if式の場合は、インデントブロック開始の合図が「then」になります。最初thenを書き忘れたらコンパイラにthenがないって怒られて、thenってなんぞや？？？となりました。どうやら新しいキーワードみたいです。行末のthenは0.20.0-RC-1でオプションになりました。つまり以下のthenがなくても正しい構文になります。 123456if day.isWeekend then println(\"Today is a weekend\") println(\"I will rest\")else println(\"Today is a workday\") println(\"I will work\") if式で括弧を用いる場合はthenは必要ありません。 123if (day == Day.Wednesday) println(\"Today is a Wednesday\") println(\"Bad Day\") 3.3. for式以下のコードには=とforとyieldがインデント構文の開始の合図になっています。 123456val optNum = for x println(\"bigger than 4\")case _ => println(\"Other\") 123optNum match case Some(x) if x > 4 => println(\"bigger than 4\") case _ => println(\"Other\") match以外にはcatchも同様に次のcaseのインデントは自由です。 3.5. ラムダ式ラムダ式の記号(=>)もインデント構文の開始の合図になっています。 123x => y = y - 1 y * y 4. エンドマーカーインデントだとブロックの終わりがわかりにくい場合には、エンドマーカーを使ってブロックの終わりを明示することができます。以下のコードでは「end fromString」が識別子のエンドマーカーです。エンドマーカーはオプションなのでなくても問題なく動作しますが、ドキュメントではひと目でブロックを識別することが難しい長いブロック(20行以上)で使うことを推奨しています。 1234567def fromString(str: String): Day = try Day.valueOf(str) catch case _: IllegalArgumentException => throw new IllegalArgumentException(s\"$str is not a valid day\") end tryend fromString エンドマーカーは以下の予約語と合わせて用いることもできます。 if while for match try new 上記の例では「end try」がエンドマーカーになっています。 4.1. インデントマーカー:次はPythonっぽい、行末コロン(:)の例です。開き中括弧({)が有効な箇所で行末をコロンにするとインデント構文を開始することができます。以下の例ではmapの後にインデントマーカー:が付いています。 1234val z = List(2, 3, 4) map: x => y = y - 1 y * y ただし、このインデントマーカーはまだ他のインデントスキームよりも議論の余地が大きく、コンパイラオプションに-Yindent-colonsを指定した時だけ有効になります。 5. 設定と書換えインデント構文はデフォルトで有効ですが、コンパイラオプション-noindent, -old-syntax,-language:Scala2のいずれかを指定すれば無効にできます。実際に試してみましたがインデント構文を利用している箇所はエラーになってコンパイルができなくなりました。 また、コンパイラオプションでインデント構文への書換えもできます。インデント構文への書換えは-rewrite -new-syntaxオプションをつけてコンパイル後に、もう一度-rewrite -indentオプションをつけてコンパイルする必要があります。つまり面倒ですが2回コンパイラを起動する必要があります。この書換えは上手くいきました。 逆方向の書換えを行うには-rewrite -noindentオプションをつけてコンパイル後に、もう一度を-rewrite -old-syntaxつけてコンパイルします。この書換えは0.19.0-RC1時点では失敗しました。どうやらエンドマーカーの書換えで失敗しているようです。 5.1. @main関数インデント構文とは欠片も関係ありませんが、0.18.1-RC1で導入された@main関数についても紹介しておきます。従来は以下のように書いていたmain関数ですが1、 123object Example { def main(args: Array[String]): Unit = IndentBaseExample.use(\"Monday\")} @main関数を使うと以下のように書くことができます。Scalaのスクリプティングが捗りそうです2。 1@main def example: Unit = IndentBaseExample.use(\"Monday\") 5.2. 今すぐ試してみたい!上記のサンプルコードをすぐに試せるようにGitHubに公開したのでご査収ください。インデントベースの構文と従来のブレースベースの構文はどちらも有効なので、実際に触ってみて感触を掴むのが一番だと思われます。念のため書いておきますが、サンプルコード自体に特に意味はありません。インデントベースの構文の雰囲気が分かるように適当に構文を並べただけです。 hinastory/dotty_examples: Example code of Dotty (Scala 3) IndentBasedExample.scala 6. インデントベース構文の状況インデントベースへの変更は実は2017年にOdersky先生が#2491で提案されていて、このときは大激論の末に一旦クローズされています3。そしてようやく今回執念のプルリク(#7083)を投げて、捩じ込みました。 「捩じ込む」と表現したのは今回も当然のように大激論が起こったからです。見た目が違うから拡張子変えた方がいいという意見や、読みにくさや曖昧さを指摘する意見や、初学者の混乱を指摘する意見まで様々です。 Odersky先生もこのプルリクはデータを集めるための実験としており、現在のところこのインデント構文はScala 3の新機能の中でも採用される可能性が最も低いものの一つだと考えられます。以下は#7083のリアクションですが、こんなに嫌われているプルリクには滅多にお目にかかれません(笑)。 経緯はともかくOdersky先生の壮大な実験は始まりました。Scalaのようにある程度普及した言語が途中からインデントベースの構文をサポートした例を自分は知りません。今後の成り行きが気になるところですが、Scala 3.0の機能凍結及びScala 3.0のM1リリースは今年(2019年)の秋で、Scala 3.0 finalのリリースは来年(2020年)秋と言われているので4、何か言いたいことがある方はなるべく早めに本家にフィードバックをかけた方が良いと思われます5。 ちなみに本記事はあくまでインデントベース構文の紹介が目的なので、この構文の良し悪しについて突っ込んだ批評は控えておきます。ただ一応参考までに個人的な初見の感想を述べておくと、自分が趣味で使うのには良さそうな感じです。仕事で使うのにはコーディング規約をどうするか悩みどころが多いと感じましたが、結局はエディタ、IDE、フォーマッタ、リンタ等のツール群のサポートがきちんと得られれば使えるとは思うので、それらの対応次第かなと思っています。 7. 最後に本記事ではScala 3へ入る可能性のあるインデントベースの構文を紹介しました。本機能は実際にScala 3に入るかどうかはわかりませんが、まだ多くの議論の余地があり、より多くのフィードバックが必要とされています。興味のある方は実際に触ってみてScala Contributors等でフィードバックしてみてはいかがでしょうか？ 8. 参考文献 Announcing Dotty 0.18.1-RC1 – switch to the 2.13 standard library, indentation-based syntax and other experiments Announcing Dotty 0.19.0-RC1 – further refinements of the syntax and the migration to 2.13.1 standard library Allow significant indentation syntax by odersky · Pull Request #7083 · lampepfl/dotty Change indentation rules to allow copy-paste by odersky · Pull Request #7114 · lampepfl/dotty Consider syntax with significant indentation · Issue #2491 · lampepfl/dotty Optional Braces 9. 更新内容9.1. 2019年9月28日の更新内容先日発表されたDotty 0.19.0-RC1でインデント構文が若干変更されました。 クラスやオブジェクトの定義でインデントをする場合に:が必要なくなった インデントマーカー:の利用時には-Yindent-colonsのオプションの指定が必要になった 上記の内容は本文にも反映済みです。またインデント構文への書換えも試してみたので追記を行っています。 下記のサンプルリポジトリに関しても0.19.0-RC1にバージョンアップして対応済みです。 9.2. 2019年11月16日の更新内容先日発表されたDotty 0.20.0-RC1でインデント構文が若干変更されました。インデントに関係ある変更は以下のとおりです。関連する箇所に関して記事の追記を行っています。 クラス、トレイトの後ろにオプションでwithを置けるようになった インデントのし忘れを防ぐため Tweaks to indent syntax by odersky · Pull Request #7363 · lampepfl/dotty if式の行末のthenがオプションになった Make `then` optional at line end by odersky · Pull Request #7276 · lampepfl/dotty 1.Appトレイトをミックスインして書く方法もあります。 ↩2.@main関数は引数をとることもできてコマンドライン引数を受け取れるのですが、話が脱線するのでこの記事ではこれ以上は触れません。 ↩3.#2491も今回と似たような提案ですが、大きな違いはwithがインデントマーカーとして使われているところです。またエンドマーカーも現在とは異なっていました。 ↩4.A Tour of Scala 3を参照。 ↩5.フィードバックの方法としてはScala Contributorsでトピックを立てるのが一番簡単だと思われます。もちろん問題が明確ならイシューを開いたりプルリクを送る方法もあります。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/09/15/scala-indentation/"},{"title":"★記事30本突破★ なぜ自分は技術ブログを書き続けるのか？","text":"ようやく本ブログの記事数も30本突破しました。お読み頂きありごとうございます。 記事30本は目標にしていた数字ですが、2018年12月に書き始めて達成したのが2019年9月なので10ヶ月かかりました。 普段は技術以外のことは全く書かない本ブログですが、今回は30本突破記念として特別に「なぜ自分は技術ブログを書き続けるのか」というテーマで書いてみたいと思います。あと、本ブログのPV数や人気記事も紹介したいと思いますので目次を見て気になるところから読み進めてください。 目次1. 技術ブログを書く目的1.1. 「自己紹介」としてのブログ1.2. 「自己研鑽」としてのブログ1.3. 「社会貢献」としてのブログ1.4. 「技術縛り」にした理由2. 実はブログを書くのは趣味じゃない3. 技術ブログを書く技術4. PV数と人気記事の紹介5. まとめ 1. 技術ブログを書く目的まず大前提としてブログを書くのは手間暇がかかります。そして多くの人は30本の記事に到達することなく燃え尽きてしまいます。自分も過去にブログを書いて30本いかずに挫折した経験があるのでその気持は非常によくわかります。少なくとも「なぜ自分はブログを書き続けるのか」という問いに対してある程度腹落ちする結論を出しておかないと継続は難しいでしょう。つまりブログを継続的に書き続けるには「目的の設定」が重要になってきます。 例えば、「自分の興味ある技術を紹介したい」という目的だったら、自分の興味のある技術を紹介し終えたらブログを書き続けることができなくなってしまいます。また、「何でもいいから適当に旬のネタでブログを更新しよう」ということにしたら、書き続けている内に何のためにブログを更新しているのか分からなくなる日が来ると思います。従って、これら2つは継続的に書き続けるための目的としては不適切だと思われます。 結局ブログを継続的に書くための目的としては短期的に達成できるような具体性を持たせてはだめで、かといって「何でもいい」にしてしまうと書き続ける意義を見失ってしまうリスクがあります。これらを踏まえて本ブログでは「技術を軸にして」以下の3つを達成することを目的としました。 自己紹介 自己研鑽 社会貢献 なぜ、これらを継続的に書き続けるための目的にしたのかをこれから説明してみたいと思います。 1.1. 「自己紹介」としてのブログ「自己紹介」のためにブログを持つというのは一番わかり易い目的かも知れません。例えば、「ブログ書いています」、「〇〇といった記事を書いたことがあります」と言えば、「ブログを書く人」、「〇〇について詳しい人」という第一印象を与えることが出来ます。運が良ければ、すでに記事を読んでもらっていて自己紹介するまでもないかもしれません。 しかし単なる自己紹介ではあまり多くの記事を必要としないので、継続的に書き続けるための目的としてはもう少し別の工夫をしています。それは自己紹介を「現在興味ある技術や過去に取り組んだ技術、これから手を付けてみたい技術の陳列棚」と捉える方法です。この方法では過去から未来に渡って「自分はどんな技術に注目してきたか」という視点が生まれるので、ブログの幅が広がってより深い自己紹介をすることができます。 さらに自己紹介より一歩進んでブログを自己分析ツールにすることもできます。これには「ジョハリの窓」という有名なフレームワークがあって、自己を以下の図のように「自分は知っている/知らない」という軸と「他人は知っている/知らない」という軸の2つの軸で4象限に分類する手法です。一般的には、「開放の窓」を広げ、「未知の窓」を狭めていくことが良いとされています。 自分は自己紹介が苦手だと思っていますが、その根本として自己分析が上手く出来ていないというのも原因のひとつにあると思っています。そのため、このジョハリの窓を使って「秘密の窓」や「盲点の窓」にあたる記事は何かということを意識しながら記事を考えることもあります。 1.2. 「自己研鑽」としてのブログ「技術ブログ」にした時点である程度「自己研鑽」の要素が必ず入ってくるというのは想像に難くないと思います。例えばそれがどんなに身近で詳しいと思っていた技術でも、記事一本書き上げるためにはあやふやな知識ではだめで、正確な情報とそれを元にした論理構成が必要になってきます。つまりある程度まともな技術記事を書き上げると必ず自己の成長がセットで付いてきます。 もちろん技術面だけではなく精神面における成長にも大きく寄与します。ブログを書き続けていると「自分の知識が浅い部分」や「よく理解できていない技術」について触れなければいけないことが多々訪れます。もちろんこの時は十分に調べて記事を書きますが、経験が浅いと最後まで記事に書いたことが本当に正しいのか自信を持てないことがよくあります。しかし、よくよく考えてみるとブログに書かれた全ての情報を誰の目から見ても完璧に仕上げることは到底不可能なことだと分かるはずです。 従って、大事なのはブログの記事をなるべく正確にしつつ、自信がない部分は断り書きをきちんと書くことであり、そうして公開した記事に全責任を負う態度だと思っています。 自信のない記事に責任を持つことは大きな不安が付き纏い、勇気がいることだと思います。しかし記事は一度書き上げたらそれで終わりと考えるのではなく、新たな第一歩を踏み出したのだと考えると見える景色も違ってくるはずです。 間違っていたら記事を訂正してもいいですし、新しい記事で補足をすることもできます。また次に書く新しい記事ではより正確な記事を書くように心がけるようになると思います。つまり、記事を公開して文責を持つことは常により良い記事を書こうとする原動力になり得るわけです。 1.3. 「社会貢献」としてのブログ今こうして技術ブログを書けるのは、自分の努力の結果だけではなく、その多くは恵まれた自分の環境によるものです。例えばこの技術ブログを書くために利用しているソフトウェアの多くはオープンソースとして公開されており、誰もが無料で使うことができます。そういったオープンソースを使うための知識はインターネットで公開されていたり、書籍で入手できます。そして、そうした書籍を理解するための力は両親や学校や社会が提供してくれたものです。つまり、今自分が置かれている環境は当たり前などでは決してなく、多くの人の努力と献身によって支えられているということになります。 このような相互扶助の輪の中に自分も加わることは、大きな恩恵となって社会全体に広がり、最終的には自分にも帰ってくるものだと考えています。従って「技術ブログ」を書くことの最終目的は「自己研鑽」の結果として「社会貢献」に繋がっているかだと自分は考えています。自分と社会の双方がWin-Winの関係になれることが理想です。そのために当ブログは社会貢献として一人でも多くの人に「少しでも役立つ記事」を提供したいと考え、以下のようなことを心がけています。 世の中の知識を整理してストーリーにする 読者に多くの知識を仮定しない わかりやすく図示する 写真を多く載せる 読者の次の行動につながるように工夫する 導入手順を分かりやすく載せる サンプルコードをGitHubに公開する リンクを多く貼る 参考文献を載せる 当ブログが行える社会貢献は僅かですが、多くの人の「僅か」が集まって少しずつ社会を良くしていくものだと信じています。最後に自分が影響を受けた東大名誉教授の上原千鶴子さんの言葉で本節を締めようと思います。 がんばったら報われるとあなたがたが思えることそのものが、あなたがたの努力の成果ではなく、環境のおかげだったこと忘れないようにしてください。(中略)あなたたちのがんばりを、どうぞ自分が勝ち抜くためだけに使わないでください。恵まれた環境と恵まれた能力とを、恵まれないひとびとを貶めるためにではなく、そういうひとびとを助けるために使ってください。 平成31年度東京大学学部入学式 祝辞より抜粋 1.4. 「技術縛り」にした理由ブログを書き続けるためには、縛りをつけずにより多くのネタを集められるようにしたほうが有利だと思われるかもしれません。概ねその意見は正しいのですが一つ条件が付きます。それは、「他の目的達成の障害にならない限り」です。 具体的に言うと「自己紹介」に関しては、あまりテーマが広すぎると他者に対する自己アピールが弱まり、最悪自己を誤解される恐れがあります。実は最初はたまには技術以外のネタも出してもいいかもと思いましたが、踏み止まりました。他のネタを書いた途端に本ブログは「技術ブログ」だと胸を張って言うことが出来なくなり、結果として中途半端な立ち位置の雑記ブログに見えてしまう危険があったからです。結局自分は「自己」として技術をアピールしたかったために「技術ブログ」にする選択をしました。 また、「社会貢献」の側面でも「技術ブログ」にした方がメリットがあります。それは社会貢献度で言えば高い専門性の方が希少なので、あまり詳しくない分野の記事を書くよりも自分の得意な技術系の記事に集中した方がよりよい社会貢献になりやすいからです。最近は単なるボランティアよりもプロボノ活動1に注目が集まっているのもこういった理由があるからだと思っています。 2. 実はブログを書くのは趣味じゃないブログを書いていると「趣味でやっているんでしょ？」と聞かれることがあります。もちろん仕事ではなくプライベートでやっているのは間違いないのですが、趣味(好きでやっている)かと尋ねられた場合には、正直「No」です。 冒頭でも書きましたが自分は過去にブログの継続を放棄した経験があります。その時は楽しいのは書きはじめて1ヶ月ぐらいのもので、それ以降は一気に苦しくなりました。それはなぜかと言うと自分が書いたブログが世界中に公開してもよい最低限のレベルのクオリティとオリジナリティを満たしているかを意識し始めたからです。最初は「達成感」、「自己陶酔」、「承認欲求」のおかげで気持ちよく書けていたブログが、一旦このことを意識しだすと記事を公開するのがだんだん怖くなり次第に更新頻度が減っていきました。 正直自分が趣味で作ったプログラムは基本的にはコピペ上等で「動けばいい」の精神で作っているので、公開できるレベルのものはかなり少ないです。趣味で書く文章も誤字脱字だらけの断片化した文章で他人が理解できるとは思えません。つまりブログは趣味ではない何かの力を用いて書いていることになります。 3. 技術ブログを書く技術趣味でないとしたら、どうやって技術ブログを書いているのかというと「環境」、「目的」、「技術」の力で乗り切っています。 まずは執筆環境です。実はこれが一番大事です。技術ブログを書くと何時間も机に向かうことになるのでなるべく快適な環境が必要になります。エディタとモニタは言うまでもないですがその他も結構重要です。特に広い机と快適な椅子は人生を変えるといっても過言では無いほど生産性に影響します。絶対に後悔はしないとおもうのでぜひ投資してください。以下は自分の執筆環境で、生産性に寄与している順番に並べています。ご参考まで。 エディタ VS Code + 様々な拡張 よく使う文章やコードをスニペットに登録する キーボードショートカットを覚える+必要に応じて調整 モニタ 27インチのデュアルディスプレイ キーボード 自作(笑) 机 幅160cm、奥行き70cmの木製デスク 椅子 ちょっと高めのゲーミングチェア コーヒーメーカー スマートスピーカー Amazon Echo Show 5 目的についてはすでに説明しました。「自己紹介」、「自己研鑽」、「社会貢献」の3つです。普段の趣味は楽しさ優先ですが、ブログを書く場合はどんなに辛くてもこの３つを最優先にして割り切って書くようにしています。下手に楽しさを優先すると自分の場合は書き上げられないことが多いので、目的の達成度を基準にブログをまとめるようにしています。 最後に技術です。基本的には以下のような流れ作業です。1の「ネタを貯める」だけがいわゆる「趣味」の範疇で、それ以降は趣味とは言えない自分にとっては苦しい作業ですね・・・ ネタを貯める 読んだ技術記事/技術書の感想をメモる 興味あるOSSのコミットやプルリクをGitHubでウォッチする 日常の不満や面白そうなことをプログラムで表現してみる 趣味で書いたプログラムをとりあえずGitHubで公開してみる とにかく書き始める ブレストみたいな感じ ブログに入れたいテーマやキーワードを元にして短文をいくつか書いてみる プロットを考える 読者に何を伝えたいかに焦点を絞って大まかなストーリーを練る 目次を考える まずは文章で説明してみる 図とコードに置き換えて説明する 前提を補足する ひたすら読み返して誤字脱字を訂正する リンクを貼ったり参考文献を付け足す 公開！！ 公開後も読み返して、問題ないか確認する 公開後も数回訂正することが多いです 技術に関して補足するとこのブログはHexoを使って書かれていて、ブログの記事全体も全てGitで管理されています。つまりブログサービスを使っている場合とは大分異なり、かなりの定型作業を自動化できます。例えば記事を全て検索してプログラムで処理するとか、記事に埋め込むためカスタムのタグを作ったりできます。Hexoについては以前に記事(Hexoでブログに再入門)を書いたので参考にしてください。 4. PV数と人気記事の紹介正直PV数は大したことないです。一応SEO対策はしているつもりですが、ニッチな技術ブログでブログサービスも利用していないので当然かもしれません。検索流入は少しずつは増えているので気長にやるつもりです。 人気記事はダントツでErgoDash miniの組み立て記事でした。 サイレントでホットスワップなErgoDash miniを制作(前編) ただ、このブログの記事は結構Qiitaにも転載をしていてここを見ると一番人気は以下の記事になっています。 全プログラマに捧ぐ！図解「ノートブック」 5. まとめ記事30本突破記念に「なぜ自分は技術ブログを書き続けるのか？」というテーマで語ってみました。正直、自分にとってブログを書き続けるのは楽じゃなかったので、それに見合う「目的」が必要であり、その苦痛を少しでも和らげるために「環境」と「技術」を見直してきました。もちろん今でも戦いは続いていますが。ここまで続けられたことでもう少し戦える自信も付きました。 30本は最初は遠い道程のように思えましたが、苦しさを乗り越えて一本書き上げる毎に成長を実感できて、振り返れば自分の貴重なアルバムになっていたので、続けてきて本当によかったと思っています。 本記事が、少しでもブログを書く人のヒントになれば幸いです。1.プロボノ（Pro bono）活動とは、各分野の専門家が、職業上持っている知識やスキルを無償提供して社会貢献するボランティア活動全般のことを言います。そういった意味ではOSS活動もプロボノ活動の一種と言えるかもしれません。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/10/07/why_write_technical_blogs/"},{"title":"AI・機械学習をビジネスに応用したい人向けの資格「G検定(JDLA Deep Learning for GENERAL 2019 ＃3)」を取得しました","text":"G検定はAI・機械学習をビジネスに応用したい人向けの資格で近年注目を集めています。AI・機械学習は「何ができるか」を誤解しているひとが非常に多く、魔法のように何でもできると思いこんでいる人や、以前のAIや機械学習のイメージを払拭できずにまだまだ「おもちゃ」だと思いこんでいる人まで様々です。 G検定はこれらの誤解を乗り越えて、現実的にAI・機械学習をビジネスに応用するための「視座」を身につけるためのうってつけの資格です。本記事では実際にG検定(JDLA Deep Learning for GENERAL 2019 #3)を受験した体験と得られた知見を紹介したいと思います。 目次1. はじめに2. G検定とは3. 出題範囲3.1. 学習のシラバス3.2. 試験の例題4. G検定対策5. 試験に費やした勉強時間と費用6. 受験した感想7. まとめ 1. はじめに本記事は2019年11月9日に行われたG検定(JDLA Deep Learning for GENERAL 2019 #3)の受験体験談です。G検定はAI・機械学習をビジネスに応用したい向けの資格ですが、この分野の進化は非常に速いので資格の名称自体に取得した時期が入っています。この体験談は2019年の３回目のG検定の体験談なので実際に受ける際には最新の情報を参照するようにお願いいたします。 また、本記事は筆者の体験談になりますので、「素人が短期間で取得した」系の話ではないのでご注意ください。以下参考スペックです。 機械学習に取り組み始めて1年目のソフトウェアエンジニア 一般的なITスキルは持っており、ソフトウェアの開発経験もある 機械学習環境を構築して、ディープラーニングを実際に動かしたことがある いくつかのサンプルは動かしてみたけど実務への応用はまだまだ・・・ 数学が苦手で理論の習得には苦戦している・・・ AIや機械学習に関して、体系的な勉強をしたことがない とりあえず動かしてみような的なノリでやってきた・・・ 実は数学が苦手・・・ 2. G検定とは一般社団法人日本ディープラーニング協会(JDLA) が実施している検定試験です。第一回が2017/12/16(土)に実施されており、最近は年に3回実施されています。 JDLAが主催している試験は以下のとおり「G検定」と「E資格」の２つがあり、前者がビジネスマン向けで後者がエンジニア向けです。 正直自分はE資格を受けたかったのですが、十万円オーバーの JDLA認定プログラムが受験資格になっていたのでとりあえず見送ってG検定を受けることにしました。G検定の概要は以下のとおりです。 概要：ディープラーニングを事業に活かすための知識を有しているかを検定する 受験資格：制限なし 試験概要：120分、小問226問（前回実績）の知識問題（多肢選択式）、オンライン実施（自宅受験） 申込期間：2019年 10月1日 (火) 13:00 〜 10月31日 (木) 1 試験日： 2019年 11月9日 (土) 13:00より120分 1 受験料： 一般 12,000円＋税 学生 5,000円＋税 受験サイト：https://www.jdla-exam.org/d/ 公式サイトより一部抜粋 上記のとおり受験料は少し高いですが、自宅で受けられるので気軽に受験可能です。もちろん試験中のインターネットの検索も可能なのですが、問題数も多く1問に30秒程しかかけられないのであまり検索に期待しないほうがよいです。 今回自分が受験した2019年の3回目の試験では4,652名が合格し、合格者数は累計14,523名になったようです2。今回の合格率は70.70%でここ最近の合格率もだいたい70%程度で推移しています。合格率が高いからと言って簡単だったかと言うとそのようには自分は感じませんでした。その辺はこのあとの記事で触れたいと思います。 3. 出題範囲3.1. 学習のシラバス公式サイトが公開しているG検定の学習のシラバスは以下のとおりです。 人工知能（AI）とは（人工知能の定義） 人工知能をめぐる動向 探索・推論、知識表現、機械学習、深層学習 人工知能分野の問題 トイプロブレム、フレーム問題、弱いAI、強いAI、身体性、シンボルグラウンディング問題、特徴量設計、チューリングテスト、シンギュラリティ 機械学習の具体的手法 代表的な手法、データの扱い、応用 ディープラーニングの概要 ニューラルネットワークとディープラーニング、既存のニューラルネットワークにおける問題、ディープラーニングのアプローチ、CPU と GPU ディープラーニングにおけるデータ量 ディープラーニングの手法 活性化関数、学習率の最適化、更なるテクニック、CNN、RNN 深層強化学習、深層生成モデル ディープラーニングの研究分野 画像認識、自然言語処理、音声処理、ロボティクス （強化学習）、マルチモーダル ディープラーニングの応用に向けて 産業への応用、法律、倫理、現行の議論 シラバスのとおり、AI/機械学習/ディープラーニングに関連する理論とビジネスに関連する事例、法律から幅広く出題されます。実際にこのシラバスはAI/機械学習/ディープラーニングの全体像を理解するのにバランスがよく組まれているので、AI/機械学習/ディープラーニングに関わる可能性があるビジネスマン、エンジニアはぜひ押さえておいたほうが良い内容になっています。 3.2. 試験の例題実際に公式ページに掲載されている試験の例題を2問引用します。１つ目は人工知能をめぐる動向に関連するものです。 問題：国際的な画像認識コンペティション“ILSVRC2012”について、正しいものをすべて選べ。 画像認識は､2017年現在でディープラーニングが最も高い精度を実現できるタスクである｡ ImageNetとは、手書き文字認識のためのデータセットである。 優勝チームはトロント大学のジェフリー・ヒントン教授率いるSuperVisionである。 このコンペティションであげられた成果は、「人工知能研究50年来のブレイク・スルー」と称された。 公式ページより人工知能をめぐる動向:例題 ２つ目はディープラーニングの手法に関連するものです。 問題：次の文章の（A）、（B）の組み合わせとして、最も適しているものを1つ選べ。 時系列データの分析には、もともと（ A ）が最も適していると考えられていたが、時系列データのひとつである音声処理の分野では（ B ）が非常に高い精度を記録している。 （A）リカレントニューラルネットワーク （B）畳み込みニューラルネットワーク（A）リカレントニューラルネットワーク （B）Autoencoder（A）畳み込みニューラルネットワーク （B）リカレントニューラルネットワーク（A）畳み込みニューラルネットワーク （B）Autoencoder（A）Autoencoder （B）畳み込みニューラルネットワーク（A）Autoencoder （B）リカレントニューラルネットワーク 公式ページよりディープラーニングの手法:例題 両方とも公式ページには解答が載っていないのでここでもあえて回答は書きません。受験する方はぜひ調べて解答できるようにしておいてください。実際にはこれらの問題は比較的優しいの部類に入り、実際の試験にはこれよりも難易度の高い問題もたくさん出題されます。 4. G検定対策実際に自分が行ったG検定対策は、以下の3冊の本を読みながら理解の怪しいところを適宜Webで調べていくというものです。まず、一冊目はG検定の公式テキストでこれは外すことができません。この本は2回読み直しました。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"深層学習教科書 ディープラーニング G検定(ジェネラリスト) 公式テキスト\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\\/images\\/I\",\"p\":[\"\\/518zmsdTIVL.jpg\",\"\\/51%2BkKCkq7qL.jpg\",\"\\/41u76lffFdL.jpg\",\"\\/51uovJ9TrGL.jpg\",\"\\/51KdxxnC1IL.jpg\",\"\\/5118Zzt4gdL.jpg\",\"\\/41NhrOKmNbL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E6%95%99%E7%A7%91%E6%9B%B8-%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-G%E6%A4%9C%E5%AE%9A-%E3%82%B8%E3%82%A7%E3%83%8D%E3%83%A9%E3%83%AA%E3%82%B9%E3%83%88-%E5%85%AC%E5%BC%8F%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88\\/dp\\/4798157554\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"6QrMz\",\"s\":\"s\"});リンク 2冊目は「徹底攻略ディープラーニングG検定問題集」(通称:黒本)です。この本はKindle版で購入して、マイノートの機能を活用して問題と解答を行ったり来たりしながら勉強しました。試験前に復習で全体を通して解きました。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"徹底攻略 ディープラーニングG検定 ジェネラリスト問題集\",\"b\":\"\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\",\"p\":[\"\\/images\\/I\\/51Ct%2B64OAXL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/%E5%BE%B9%E5%BA%95%E6%94%BB%E7%95%A5-%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0G%E6%A4%9C%E5%AE%9A-%E3%82%B8%E3%82%A7%E3%83%8D%E3%83%A9%E3%83%AA%E3%82%B9%E3%83%88%E5%95%8F%E9%A1%8C%E9%9B%86-%E3%82%B9%E3%82%AD%E3%83%AB%E3%82%A2%E3%83%83%E3%83%97AI%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE-%E6%98%8E%E6%9D%BE\\/dp\\/4295005665\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"Ws75e\",\"s\":\"s\"});リンク 3冊目は「AI白書」です。この本はボリュームがあるので全ては読んでいません。自信がなかった第４章の「制度政策動向」を重点的に読みました。 (function(b,c,f,g,a,d,e){b.MoshimoAffiliateObject=a;b[a]=b[a]||function(){arguments.currentScript=c.currentScript||c.scripts[c.scripts.length-2];(b[a].q=b[a].q||[]).push(arguments)};c.getElementById(a)||(d=c.createElement(f),d.src=g,d.id=a,e=c.getElementsByTagName(\"body\")[0],e.appendChild(d))})(window,document,\"script\",\"//dn.msmstatic.com/site/cardlink/bundle.js\",\"msmaflink\");msmaflink({\"n\":\"AI白書 2019\",\"b\":\"KADOKAWA\",\"t\":\"\",\"d\":\"https:\\/\\/images-fe.ssl-images-amazon.com\",\"c_p\":\"\\/images\\/I\",\"p\":[\"\\/51Ya%2Btgn-ZL.jpg\",\"\\/51y5Ud%2Ba0cL.jpg\",\"\\/51mqjolXxBL.jpg\",\"\\/5176N4jXayL.jpg\",\"\\/514ZEjtYamL.jpg\"],\"u\":{\"u\":\"https:\\/\\/www.amazon.co.jp\\/AI%E7%99%BD%E6%9B%B8-2019-%E7%8B%AC%E7%AB%8B%E8%A1%8C%E6%94%BF%E6%B3%95%E4%BA%BA%E6%83%85%E5%A0%B1%E5%87%A6%E7%90%86%E6%8E%A8%E9%80%B2%E6%A9%9F%E6%A7%8B-AI%E7%99%BD%E6%9B%B8%E7%B7%A8%E9%9B%86%E5%A7%94%E5%93%A1%E4%BC%9A\\/dp\\/4049110148\",\"t\":\"amazon\",\"r_v\":\"\"},\"aid\":{\"amazon\":\"1448335\",\"rakuten\":\"1448332\"},\"eid\":\"JD8QB\",\"s\":\"s\"});リンク 5. 試験に費やした勉強時間と費用以下は純粋にG検定対策に費やした勉強時間と費用です。趣味でディープラーニングで遊んだ時間と費用は入れていません。 勉強時間: 合計20〜30時間 期間: 1ヶ月 休日を中心に時間を確保 費用: 合計20,823円 試験: 13,200円 (税込み) 本 深層学習教科書 ディープラーニング G検定（ジェネラリスト） 公式テキスト Kindle版: 2,772円 徹底攻略 ディープラーニングG検定 ジェネラリスト 問題集 徹底攻略シリーズ kindle版: 2,079円 ＡＩ白書 ２０１９ Kindle版: 2,772円 6. 受験した感想7割が合格する試験と聞いて簡単なのかと思っていたら試験は以外と難しかったです。特に前半部分に出題された制度や法律関係はやはりもう少しやっておけば良かったと後悔しました。 試験は本当に時間との戦いで一応10分前に解き終わりましたが、途中で時間が足りなくなると思いかなり焦りました。試験の出題範囲は上記の3冊の本の範囲だとは思うのですが、しっかりと理解していないと解けない問題が多かったという印象で少なくとも「易しい試験」ではないと思いました。 実際に「資格の難易度」というサイトでは難易度は”「B」普通“に分類されおり3、同難易度には以前に受験したAWS ソリューションアーキテクト アソシエイトも含まれます。つまり、しっかりとした対策なしでは受からないと思っておいたほうがいいと思います。 試験は土曜日(11/9)に受けて結果が来たのは翌々週の月曜日(11/18)でした。前回は金曜日に通知が来たという情報があったので週末はだいぶもやもやしましたが、合格していてよかったです。合格通知は以下の通りシンプルなメールで来ました。合格証は後ほど発行されるようです。 7. まとめ実は最初シラバスを見たときにビジネスするのにディープラーニングの手法まで身につける必要があるのかと訝しがりました。また、エンジニアがG検定を受ける価値があるのかも懐疑的でしたが現在は実際に受ける価値のある試験だと思っています。 G検定を受ける過程でAIとディープラーニングの現実と本質の両方の理解が深まるので、今後AIの利活用が重要視されることを鑑みるとその重要性も比例して高まってくるものと考えられます。ビジネスに関しても現実的にはディープラーニングの理論な本質部分の理解は必須だと実感しました。それはAIとディープラーニングの理論は応用範囲は広いが、個々の事例に適用するためには多くの個別の課題があり、理論を知らないと何が課題になっているかが分からないからです。現在実現されているのは弱いAIなので適用分野や活用方法は慎重に検討する必要があることをよく理解できたことがG検定を受験した一番の収穫だと思っています。 本記事が、「G検定」を受ける方の一助になれば幸いです。 1.自分が受験した2019年の3回目の試験の情報です。 ↩2.詳細は プレスリリースを参照してください。 ↩3.詳細はG検定（ジェネラリスト検定） 難易度 | 資格の難易度を参照。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/11/21/deep-learning-for-general/"},{"title":"Markdownで書かれたブログ記事をRe:VIEWを使って「本」にしてみる","text":"新年あけましておめでとうございます。お正月といえば書き初めですが自分は字が下手なのでブログを書くことにしました。 本記事のテーマは「ブログを本にする」です。去年の約1年間で38本もの記事を書きましたが、それらを本にしたらどういうことになるかを解説してみたいと思います。 目次1. はじめに2. 本作りの下準備2.1. まずは文字数を確認してみる2.2. 出版形態を決める2.3. 媒体を決める2.4. 流通経路を決める2.5. ツールを決める2.6. Re:VIEWとは？2.7. テンプレートを決める3. 実際に本を作ってみる3.1. 環境の構築3.2. 変換の流れ3.3. 変換で嵌った点3.4. 印刷用原稿の完成3.5. 表紙、裏表紙の作成4. 完成（仮）5. まとめ 1. はじめにこの記事は「Re:VIEW」を利用した同人誌作成の初心者向けの記事になります。ブログを印刷可能な本にしてみたい方、Markdownで書かれた記事を電子書籍にしてみたい方向けになります。 2. 本作りの下準備本を作るといってもまだ分からないことだらけなので、調べながら下準備を進めていきます。 2.1. まずは文字数を確認してみるまずは単純に行数と文字数を数えてみたいと思います。本にする元ネタは本ブログの記事でMarkdownで書かれているのでそれらのファイルを一つのフォルダに集めて、wcコマンドで数えます。 1$ wc -lm * -lは行数を表示するオプションで-mは文字数（マルチバイトも一文字としてカウント）を表示します。結果は以下のとおりです。 19080 380110 total 最初この数字を見たとき目を疑いました。9000行、38万文字も書いた記憶がなかったからです。数え間違いかと思いましたが特に問題もなく・・・1 一般的な新書では400字詰めの原稿用紙で200〜300枚と言われているらしいので8万〜12万文字程度です。つまりこの分量だと3冊も本を出版できてしまいます。しかも自分のブログは画像が多いので画像も含めるとさらに膨らむこと間違いなしです。 2.2. 出版形態を決める次に出版形態を決めます。といっても大まかに以下の４つの選択肢しかないのでお試しの今回は「同人出版」か「オンデマンド出版」になります。 商業出版 出版社を通して出版 (一般書店に流通) 自費出版 出版社を通して出版（一般書店には流通、費用は自費） 同人出版 出版社を通さず出版（一般書店には流通しない） 人によってはこれを「出版」とは呼ばない オンデマンド出版 Amazon Kindle ダイレクト・パブリッシングとか 2.3. 媒体を決める媒体を決めを簡単に言えば「紙の本」にするか、電子書籍(pdf、ePub、HTML)にするかです。一番楽なのは電子書籍Onlyですが、「紙の本はロマン」なので今回は両方への対応を目指します。 2.4. 流通経路を決める媒体を決めた後はできた本をどのように配布するのかを考えます。販売する場合はさらにお金の回収も考える必要があります。同人誌の場合は「コミケ」等の同人誌即売会で販売するのが一般的ですが、技術書の場合は「技術書典」という選択肢もあります。自分はどちらも参加したことはありますが販売したことはありません。。。 インターネットでの販売は「BOOTH」を使うか、同人誌を扱っているショップに委託して販売するのが一般的なようです。Kindleなら「kindle direct publishing」で割と簡単に出版できるようです。もちろん販売せず自由にダウンロードしてもらってよいなら、ネットの適当な場所にアップロードするだけで済みます。 今回はとりあえずどこで頒布するかは決めていないです。とりあえず作ってみてから考えます・・・ 2.5. ツールを決めるようやくここまで決まったら、どのツールを使って制作するのかを考えます。とりあえず以下の３つの要件を満たすものです。 お手軽 これが一番重要 ローカルのMarkdownファイルを変換できる Web上で変換はファイルが多いのでつらい 印刷用PDF、電子書籍用PDF、ePubの３つに対応できる まず、MarkdownならPandocやGitBookがあるのですが、印刷用のPDFを作成するのに辛そうな印象があります。オンラインならFlightBooksやでんでんマークダウンがありますが、これはローカルファイルを変換する今回の趣旨には向いていないです。 ということで、すでにタイトルでネタバレしていますが、基本的にはRe:VIEWを使います。ただし、Re:VIEWはマークダウンの変換には対応してないのでmd2reviewを用いてMarkdownをRe:VIEW形式に変換します。 2.6. Re:VIEWとは？メインで使うツールなので少し詳しく説明します。Re:VIEW2は紙の本や電子書籍を作成するためのデジタル出版システムです。OSSとして開発されており主にRubyで記述されています3。デジタル出版システムといっても基本的な動作はフォーマットを変換するソフトウェアで、オリジナルのマークアップ言語（.reファイル）とyamlの設定ファイルとスタイルファイル、画像ファイルから様々なフォーマットに変換できるようになっています。 開発元より引用 上の図のとおりPDFだけは少し特殊でLaTeXフォーマットに変換後にPDFへ変換するようになっています。このためPDFの作成には面倒なLaTeX環境の構築が必要となります・・・もちろんDockerを使えば環境構築は避けられますがLaTexの呪いは避けられるべくもなく、後に苦しめられることになります・・・・ 2.7. テンプレートを決めるようやくツールが決まったと思いましたがまだ決めることは山程ありました・・・以下にちょこっと書き出してみましたが、これらを全て適正に作るのは至難の技です。 本のサイズ B5が同人誌ではポピュラー。A5も使われる レイアウト 横書き 一行の文字数 フォント、フォントサイズ 余白 デザイン 表紙、裏表紙 章と節のデザイン コードの表示 大扉、奥付 さらに紙の印刷では以下のように細かいレイアウトの指定が必要になります。 Re:VIEW knowledgeより引用 そこでテンプレートを使うことにしました。とりあえず有名なテンプレートは以下の２つです。 ReVIEW-Template Re:VIEW Starter 結局はReVIEW-Templateを使うことにしました。決め手はRe:VIEW 4.0に対応していたことです。Re:VIEW 2.0からRe:VIEW 3.0への移行は大変そうなのでとりあえず最新の環境に追随している方を選択しました。 3. 実際に本を作ってみるここまで調べたり決めたりすることが多くて疲れましたが、ようやくここからが本番です。 3.1. 環境の構築環境の構築で必要なのは主に以下の３つです。MacでLaTeX環境を維持するのは面倒だったのでDockerビルドを選択しました。RubyとNode.jsの環境はanyenvを使えば簡単に手に入るのでおすすめです。 Ruby anyenv経由でrbenvを入れてインストール md2reviewを利用するのに必要 Node.js anyenv経由でndenvを入れてインストール ReVIEW-Templateを利用するのに必要 Docker Docker for Macでインストール 3.2. 変換の流れ変換の流れは以下のとおりです。本ブログはHexoを用いて構築しており、Markdownを拡張してHexoのマクロを利用しているのとHexo独自のYaml Front Matterを取り除いて一般的なMarkdownに自力で変換する必要がありました。それ以外はそれほど難しくないはず・・・とおもっていましたがいろいろとハマリました(泣)。 マークダウンからHexoのマクロを取り除く(自前Rubyスクリプト) Hexoのマクロは{% マクロ名 引数... %}のような形をしている md2reviewでマークダウンをRe:VIEW形式に変換(.md → .re) Re:VIEW形式をPDFに変換 config.ymlに基本設定を書く。catalog.ymlに章構成を書く。 あとは./build-in-docker.shを実行してpdfファイルを出力させるだけ Re:VIEW内部では一旦LaTeXに変換されてPDFとして出力される 3.3. 変換で嵌った点とりあえずいろいろ嵌った点をメモっておきます。TeXコワイ・・ 取り消し線が引けない [ReVIEW Tips] LaTeXで取り消し線を引く - Qiita ソース上何も問題ないはずなのに，Text line contains an invalid character. ^^H や Package inputenc Error: Keyboard character used is undefined (inputenc) in inputencoding utf8 というエラーが出てコンパイルできない TeXShop FAQ - TeX Wiki Macの日本語変換のバグだと・・・ まじかよ・・・日本語入力中に制御コードが入力されてしまうことがあるとか罠でしかない・・・ この問題は2019/2/15から2019/9/11までの記事で合計15回観測されています。最近の記事では起こっていないのでもしかしたら修正されたのかもしれません・・・ エラーでLaTeXの行数が出力された・・・ config.ymlでdebugをtrueにすると出力されたLaTeXが削除されない 記法が入れ子になるところで所々エラーになる 例えば太字のなかでURL参照とか どちらかを諦める 出力場所(article)配下のpdfやhtmlファイルは次回実行時に問答無用で消される 必要なpdfやhtmlファイルはarticleディレクトリから退避させましょう 3.4. 印刷用原稿の完成以下が印刷用の原稿の見開きの様子です。上下左右や角に付いているラインは「トンボ」と呼ばれていて裁断ラインを表しています2。内側にある小さい数字は「ノンブル」といって印刷する全てのページに必要な通し番号です。これは印刷所が乱丁・落丁の確認に用いるもので必ず裁断ラインの内側に印刷する必要があります3。印刷原稿にはこのトンボとノンブルが必要で、更に言えば偶数ページにする必要があったりフォントをPDFに埋め込まなければいけない等、様々な制約があります。 3.5. 表紙、裏表紙の作成印刷用原稿はできましたが、表紙と裏表紙は原稿とは別に作成します。一般的には表紙と原稿は紙質やカラー/モノクロかの違いがあるのでファイルとしても別になります。今回は表紙と裏表紙の作成にはMac標準のKeynoteを使いました。といっても適当にiPhoneで撮った写真を加工しただけです。 注意すべきはサイズと解像度です。表紙は全面になるので本のサイズより少し大きい実寸で作成します。今回はB5なので595.28 x 841.89ポイントになります。ちなみに1ポイント0.3528mmなので覚えておくと便利です。KeynoteはそのままPDF出力ができるので最初にスライドの大きさをポイントで指定してPDFで出力するだけでOKです。 また印刷時のフルカラーの推奨解像度は350dpiぐらいです。推奨解像度は印刷所によって多少の差異はありますが、基本的に推奨解像度以上で入稿しても原稿が重くなるだけで印刷品質は変わらず、推奨解像度以下だと粗く印刷されます。350dpiのときのB5サイズのピクセル数は2591×3624になるのでまずはそれを基準に画像を作成します。今回のようにiPhoneのカメラで撮った写真を使う場合には4032 x 3024なのでエクスポートするときに少しサイズ減らす調整をしてください。電子書籍用はそこまでの画質を要求されないので印刷用の表紙からさらにサイズを落として利用します。 そして以下が完成した表紙と裏表紙です。表紙ができるだけで一気にテンションがあがるので、原稿に詰まったら表紙の作成をすることをおすすめします（笑）。 4. 完成（仮）なんとか画像や埋め込まれたオブジェクトを抜きにして完成（仮）しました。以下は電子書籍用に出力したPDFファイルです。２章まで公開しました。 目次をご覧頂ければ分かりますが300ページを超えています。画像抜きでこのページ数だと紙の本にはしづらいですね。とりあえず分冊を考えます・・・ 5. まとめ意外と簡単・・・とはいかなかったですが、すでに先人がいろいろ切り開いてくれていたので挫けずに念願の「自分の本」を作成することができました。正直言って一人で一冊の本を書くのは漠然と難しいと思っていましたが、蓋を開けてみれば1年間で300ページを超える長編小説並みの文章を生産していました。また目次を見ても意外と読み応えのある本になっていそうだったのでこれも驚きでした。 そして実際に本としても読んで見ましたが、エッセイっぽくて意外といけるなと思いました。確かに章ごとの繋がりはありませんが、章単位である記事がしっかりしていれば本としてもそれほど違和感なく読めました。少なくとも一般的な同人誌では章ごとに執筆者が違うことも多々あるのでそれと同じ感覚です。 最初はブログを繋げただけだと本にはならないだろうとなんとなくイメージをしていましたが、特に編集をしなくても記事単体がしっかりしていれば本としても読めるレベルになるというのが今回分かったことです。まぁよくよく考えれば一人で雑誌を書いたようなものなので一応「本」にはなりましたが、「ストーリーのある本」への道のりはまだまだです。 本とブログの違いで気になった点を一つ上げるとすれば、リンクやブログカードの扱いです。電子書籍はそのままリンクが機能しますが紙ではリンク先に飛べないので別途表示方法を考える必要があると感じました。あと画像やオブジェクト（地図や動画やスライド）の埋め込みは今回できなかったのでそれをどのように扱うかは今後の課題です。 最後にフォーマットの観点から言うとMarkdownから書籍を執筆するための環境は発展途上だと感じました。もともとのMarkdownの記述が弱いというのとHTMLの埋め込みができるというのがいろいろと問題を起こして汎用的なアプローチが難しいのかもしれません。それとMarkdownとは関係ありませんが、PDF化するためにはLaTeXを経由するのは胃が痛くなりそうです。CSS組版がもう少し成熟すればこの辺が解消されるのではないかと個人的には期待しております。 「本」だけではなくて本記事も長くなって来たので、とりあえず書き初めはここまでにして次回は（仮）を外して少なくとも自分が満足のいく本を出版できるように精進したいと思います。 本記事がブログで「本」を作成してみたい方の一助となれば幸いです。1.一応膨らむ要素としてはリンクや画像のURLがありますが、それを除いても多いのは間違いないです。 ↩2.「りびゅー」と発音します。 ↩2.トンボはトリムラインとも呼ばれます。 ↩3.もともとはRubyソースコード完全解説で有名な青木峰郎さんが2002年頃に設計・開発されたものを、現在は有志の方々が引き継いで開発しています。 ↩3.ノンブルは必ず原稿の全ページに連番で振られている必要があります。読者が見るページ番号は目次には振られていないケースもあるのでノンブルの替わりにはならない場合があります。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2020/01/05/blog_to_book/"},{"title":"HexoのoEmbedプラグインをnpmに公開した話(後編)","text":"HexoにYouTubeなどのサイトを記事に埋め込むためのプラグインを作成して、npmに公開しました。この記事はその後編にあたります。前編は以下になります。 HexoのoEmbedプラグインをnpmに公開した話(前編) 目次1. Hexoタグの制作1.1. 完成形をイメージする1.2. モデルを探す1.3. 一番簡単なタグを作ってみる1.4. プラグイン化してみる1.5. ソースコードをGit管理してみる1.6. oEmbedを利用したタグを作ってみる1.7. 答え合わせ2. Hexoタグの公開2.1. GitHubでリポジトリを作成する2.2. README.mdを書く2.3. package.jsonを書く2.4. GitHubに公開する2.5. npmに公開する2.6. READMEにバッジをつける2.7. Hexo本家のプラグイン一覧に載せてもらう2.8. ブログやツイッター等で告知する3. hexo-oembedの今後4. まとめ 1. Hexoタグの制作前回はコンテンツ埋め込みのための業界標準であるoEmbedの説明まで行いました。今回はいよいよoEmbedを利用したHexoタグの制作と公開を行います。ここからはこれから実際にHexoタグを作ってみたい方や、OSS貢献に興味がある方向けに丁寧に書いてみたいと思います。 1.1. 完成形をイメージする制作を始める前に完成形をイメージします。今回Hexoのタグ名はoembedにして、後ろにパーマリンクを渡せるものとします。パーマリンクがスライドの場合、oEmbedのレスポンスとしてはtypeがrichのものが帰ってくるので、パラメータのhtmlの値をそのまま表示すればよいだけです。大抵の場合htmlの中身はiframeタグになっています。あとは外側をdivタグで囲ってクラスを指定しておきます。こうすることで後で簡単にスタイルを適用できるようになります。outerとinnerで二重に囲っているのは要素をセンタリングしたい場合にinnerクラスが指定してあったほうが便利だからです。最初はouterだけだったのですが、htmlパラメータで返ってくるのがiframeかどうか保証はできないので念の為囲うことにしました。 Hexo タグ1{% oembed http://slide.com/slides/123456 %} 実際に展開されるHTML12345 もう一つ例を見てみましょう。今度は写真の場合です。写真の場合はoEmbedのtypeがphotoになるので自分でタグを生成します。といっても、aタグとimgタグを入れ子にするだけの単純なものです。aタグで囲うのは写真をクリックした際にもとのパーマリンク先に飛べるようにするためです。あとはoEmbedにtitleパラメータがあればimgのalt属性に指定するようにしておきます。あと、oEmbedの仕様ではオプションでmaxwidthとmaxheightを渡せるので、これもHexoタグにオプションとして渡せるようにします。以下の例ではmaxwidthに300、maxheightに400を指定しています。 Hexoタグ1{ oembed http://phote.com/photos/456789 300 400 %} 実際に展開されるHTML12345 1.2. モデルを探すある程度仕様がイメージできたら、次はパクリ元リスペクト元を探してきます。ライセンスには十分注意しましょう。公式のサンプルを参考にしたり一般的な構成を真似るくらいであれば特に言及はいらないと思いますが、ソースコードを流用する場合はライセンスに則って処理してください。ソースの流用がなくても独創的な機能を真似る場合は、READMEに謝辞を述べるのが礼儀だと思います。今回は以下を参考にさせて頂きました。 Hexo公式のタグプラグイン 公式のタグプラグインです。本体に取り込まれているためプラグインとしての参考にはなりませんでしたが、タグの作り方の参考にしました linkPreviewプラグイン 本ブログでもお世話になっているタグです。構成の参考にしました hexo-tag-oembed 前編で紹介したoEmbed対応タグです。 今回作ろうとしているタグはこれのパワーアップ版みたいなものです ソースコードの流用はしていませんが、構成の参考にしました node-oembed ライブラリとして利用させて頂きました Discoveryに対応しています 1.3. 一番簡単なタグを作ってみる 公式のドキュメントを参考に一番簡単なタグを作ってみます。以下は単なるhoge と表示するだけのタグです。hoge.jsというファイルに保存して[hexo dir]/themes/scripts配下に配置します。 hoge.js123hexo.extend.tag.register('hoge', function(){ return 'hoge';}); 配置ができたらHexoサーバを再起動して、ブログの記事でhogeタグを使ってみましょう。レンダリングした投稿に「hoge」と表示されていて、HTMLが以下のようになっていれば、成功です。HTMLはChromeのデベロッパーツール等で確認してください。 Hexo タグ1{% hoge %} 実際に展開されるHTML1hoge 1.4. プラグイン化してみるHexoはプラグイン機能を持っています。プラグイン化するとソースコードを外出しできて、npmでインストールすることができるようになります。npmで公開するための必須の手順ですが、単に外出しできるだけでもGitによるソース管理が行いやすくなるので、早めにやったほうがいいと思われます。プラグイン化の方法は 公式に書いてあるとおり、驚くほど簡単です。適当なディレクトリ(hexo-hoge-plugin)を作って、メインのソースファイルであるindex.jsとnpmのメタファイルであるpackage.jsonを配置するだけです。 index.js123hexo.extend.tag.register('hoge', function(){ return 'hoge';}); package.json12345{ \"name\": \"hexo-hoge-plugin\", \"version\": \"0.0.1\", \"main\": \"index\"} さっそく作成したパッケージをnpmでインストールしてみましょう。 1$ npm install --save Hexoサーバを再起動してhogeタグが利用できれば成功です。このとき前節で作成したhoge.jsファイルをscriptsディレクトリから削除しておくことを忘れないでください(笑)。npmは一般的にはネット上のnpmリポジトリからダウンロードしてインストールしますが、このようにローカルのパスを指定してインストールすることも可能です。このときインストール先のnode_modules配下にはパッケージディレクトリへのシンボリックリンクが張られるだけなので、一度インストールをすればパッケージディレクトリのファイルを修正が同期します。したがって更新のたびにインストールするとかは必要ないです1。 1.5. ソースコードをGit管理してみるGitにおけるファイル管理は非常に心強いです。こまめにコミットしておくことで至福の安心感が得られます。特に新しいモノを作ろうとしているときは試行錯誤の連続なのでバージョン管理があるとないとでは効率で大きな差がつくのでなるべく早い段階でGit管理に以降するようにしましょう。以下はおまじないのようなものです。反射的に打てるようになるまで写経しましょう(笑)。ちなみに公開予定のリポジトリのコミットログは英語だけで書くことをオススメします。拙くても日本語で書かれるよりはより多くの人に理解してもらえます2。 123456$ cd # 管理対象のソースコードの一番上のディレクトリに移動$ git init # Gitのレポジトリとして初期化$ echo node_modules > .gitignore # .gitignoreにnode_modules配下を無視するように記述$ git add . # カレントディレクトリ配下のファイルをステージング$ git status # 余計なファイルが入っていないか確認$ git commit -m \"initial commit\" # メッセージ付きでコミット 1.6. oEmbedを利用したタグを作ってみるさて、いよいよoEmbedを利用したタグ作りに取り掛かります。まず、oEmbedのプロトコルをどのように実装するかです。HTTPクライアントを使って素直に実装する手もありますが、Discoveryも含めるとちょっと面倒なので公開されているパッケージの中に良いものがないか探してみたところ、node-oembedパッケージがありました。これを使えば比較的ラクにoEmbedを実装できそうです。 npmoembedhttps://www.npmjs.com/package/oembedoEmbed consumer library and tools あともう一つ決めなければいけないことは、エンドポイントの設定方法です。幸いにもHexoにはディレクトリのトップに_config.ymlという設定ファイルがあり、これに自由に設定を加えていけます。具体的には以下のような設定があったとします。classNameは冒頭で述べたCSSのクラス名の設定です。endpointsが具体的のoEmbedプロバイダを設定する箇所で、matchパラメータがパーマリンクのURLのホスト名に部分一致していたらそのエンドポイントのurlを利用するという仕様にします。正確なスキーマの検査をしなくても多分このレベルで実用になるだろうと判断しました。手を抜いたわけじゃないよ！ _config.yml123456789101112oembed: className: oembed endpoints: instagram: match: instagram url: http://api.instagram.com/oembed/ gyazo: match: gyazo url: https://api.gyazo.com/api/oembed/ flickr: match: flickr url: http://www.flickr.com/services/oembed/ 上記の設定ファイルにはhexo.configでアクセスできます。例えばインスタグラムのエンドポイントのURLが欲しい場合はhexo.config.oembed.endpoints.instagram.urlです。これで、oEmbedのタグを実装する上での主要な情報と仕様が出揃いました。あとは基本的はWebプログラミングの知識があれば解ける問題です。Let’s Try! 1.7. 答え合わせさて、皆様の出来栄えはいかがしょうか？以下のCodePenに自分のソースコードを日本語のコメント付きで載せたので参考にしてみてください。 See the Pen hexo-oembed by hinastory (@hinastory) on CodePen. 2. Hexoタグの公開せっかく作ったので公開して多くの人に使ってもらいたいと思います。そのための手順を簡単に説明したいと思います。 2.1. GitHubでリポジトリを作成するまずは、 GitHubにリポジトリを作成します。もしまだアカウントを作成していない方はSignUpから始めてください3。以前に以下の記事でプライベートリポジトリの作り方を説明しましたが、今回は公開用のパブリックリポジトリです。 cats cats catsGitHubのプライベートリポジトリに移行した話https://hinastory.github.io/cats-cats-cats/2019/01/13/github/新年そうそうビッグニュースが流れてきました。GitHubがプライベートリポジトリをタダで使わせてくれるってよ! ITmedia NEWSGitHub、無料ユーザーもプライベートリポジトリを使い放題にhttp://www.itmedia.co.jp/news/articles/19… リポジトリ名はダイレクトにhexo-oembedにしました。ライセンスは特にこだわりがなければHexoと同じMITライセンスにしておけばいいと思います。.gitignoreファイルはもうすでに作成済みなのでここであえて作成する必要はないです。リポジトリの作成自体は十秒もかからず終わります。以下が今回作成したGitHubのリポジトリです。 リポジトリの作成が終わったらローカルのリポジトリとGitHubのリポジトリをリンクさせます。 1$ git remote set-url origin {new url} 2.2. README.mdを書くREADME.mdは公開したパッケージの説明や利用法をMarkdownで書くファイルです。リポジトリのトップにこのファイルを置いておけばGitHubのリポジトリのトップページに表示してくれるので、公開時にはほぼ必須のファイルです。書き方も大体以下のようにリポジトリ名->リポジトリの簡単な説明 -> 特徴 -> インストール方法 -> 利用方法 -> 設定(あれば) -> 謝辞(利用or参考にしたものがあれば) -> ライセンスの順に書いていけばOKです。 123456789101112131415161718192021222324252627# hexo-oembedEmbed [oEmbed](https://oembed.com/) item on your [Hexo](https://hexo.io/) article.Features--------- Supports A- Supports B## Installation`npm install hexo-oembed --save`## Usage・・・## Configuration...## Thanks## LicenseMIT 実はここが今回一番苦労したところです。oEmbedはともかくoEmbed Discoveryとかエンドポイントの設定方法とか説明しなければいけない部分がそこそこあるので、どうしたものかと悩みました。特に英語が苦手なのでGoogle先生のお力も拝借したのですがイマイチ自信がありません・・・ 2.3. package.jsonを書くすでにnpmでインストールするために最低限のpackage.jsonは記述していると思いますが、npmに公開するためにはさらに追加の記述が必要です。以下がほぼ最低限の公開用package.jsonです。特に気をつけなければ行けないのはバージョン番号です。セマンティック バージョニングになるべく厳密に従うようにしてください。あと、ファーストバージョンは1未満のバージョンから初めるのが慣例です。ある程度成熟したと感じたときにバージョン1をリリースしてください。 12345678910111213141516171819202122{ \"name\": \"hexo-oembed\", \"version\": \"0.1.5\", \"description\": \"embed oEmbed item on your Hexo article.\", \"main\": \"index.js\", \"repository\": { \"type\": \"git\", \"url\": \"git@github.com:hinastory/hexo-oembed.git\" }, \"bugs\": { \"url\": \"https://github.com/hinastory/hexo-oembed/issues\" }, \"keywords\": [ \"hexo\", \"blog\",\"plugin\",\"helper\",\"tag\",\"oembed\",\"youtube\",\"slideshare\",\"speakerdeck\",\"twitter\",\"vimeo\",\"codepen\",\"pixiv\",\"instagram\",\"flickr\",\"gyazo\" ], \"author\": \"hinastory\", \"license\": \"MIT\", \"dependencies\": { \"hexo-util\": \"^0.6.3\", \"oembed\": \"^0.1.2\" }} あと、お気づきだと思いますがキーワード盛り盛りですね(笑)。ここのキーワードはnpmの検索時に参照されるので、嘘偽りがなければできるだけ多く記載したほうが良いです。特に今回のhexo-oembedみたいにパッケージ名だけだと具体的に何をするものか分からない場合はkeywordに力を入れましょう。 2.4. GitHubに公開するGitHubに公開するのは簡単です。git pushするだけです。簡単なんですが、以下のコマンドを打った瞬間に全世界に公開されしまうのでgit diffやgit showで余計なファイルやコミットがないか再確認してからpushしましょう。一番ありがちなのは必要なファイルをgit addし忘れてpushすることです。他にもプライベートアクセスキーとか重要な情報も一緒にコミットしてしまったりとかあります。もし間違ったコミットをしてしまった場合は、早い段階であればforce pushでもみ消せますがマナーとしては最低ですので最後の手段としましょう。 1$ git push origin master ブランチをプッシュしたら忘れずにタグもプッシュしておきましょう。GitHubの場合タグのプッシュが新バージョンのリリースとみなされるので、リリース直前には必ずタグを打つようにします。 12$ git tag v0.1.5$ git push origin v0.1.5 2.5. npmに公開するnpmに公開するのはもっと簡単です。公開するパッケージ直下でnpm publishを打つだけです。npmのアカウントを持っていなければ作成する必要がありますが、GitHubのアカウントでログインできるので手間はそれほどかかりません。アバターの設定にはGravatarが利用できます。これも持っていなければWordPress.comにログインしてすぐに作ることができます。 npm publishに成功したらnpmからページを確認してみます。自分のアバターのメニューのPackagesからも確認できますし、パッケージ名やキーワードで検索をかければ出てきます。 npmhexo-oembedhttps://www.npmjs.com/package/hexo-oembedembed oEmbed item on your Hexo article. ページの確認ができたらnpm installしてパッケージがネットからインストールできるか試してみましょう。このときローカルバージョンをインストールしている場合は一旦npm uninstallしてから実行してください。 2.6. READMEにバッジをつけるGitHubのリポジトリを眺めているとREADME.mdにイカすバッジが着いているのを見かけることがあります。や みたいな奴です。 これらのバッジはカッコいいだけでなく、必要な情報が視覚的にわかりやすいという実用面も大きいのでぜひ貼りましょう。まず一番目に紹介するのはnpmパッケージをかっこよく表示してくれる NodeICOです。ドメイン名にこだわりが感じられます(笑)。npmのパッケージ名を指定するだけですぐに作ってくれるので張らない手はないです。 次に紹介したいのは Shields.ioです。Shields.ioではOSS向けに高いクオリティのバッジを提供しています。種類も豊富でカスタマイズも可能なので、大抵の用途のバッジはここで作成することができます。上記のlicenseのバッジはここで作成しました。 最後に紹介したいのはCode Climiteです。ここはGitHubのリポジトリからOSSの品質を測定するサービスを提供しています。そしてその結果はバッジとして表示可能になっています。上記のこのバッジはCode Climiteを利用しています。hexo-oembedのページには、測定結果の詳細が載っているのでパッケージの改善に役立てる事ができます。 2.7. Hexo本家のプラグイン一覧に載せてもらうようやく公開の最終工程の一歩手前です。実は拡張の公開は以下のとおり大きく分けて4段階あって、今までおこなったのは2段階目までです。3番目は見逃されがちですが利用者の観点からすれば非常に重要です。ここに載せてもらえるか否かでリーチできるターゲット規模が大きく変わるので手を抜かずにやり遂げましょう。 パブリックソースリポジトリに公開する GitHub, GitLab, Bitbucket, Mercurial・・・ パブリックパッケージリポジトリに公開する npm, RubyGems, PyPI CPAN, MvnRepository, NuGet, OS系パッケージ管理システム・・・ 公式の拡張リストに記載してもらう ブログやツイッター等で告知する とはいっても、気負う必要はまったくありません。最近は公式のサイト自体GitHubで管理でしているところが増えてきているので、ドキュメントを修正してプルリクエストを投げるだけです。バグ修正のプルリクエストと変わらないというかむしろそれより敷居は低いといっても過言ではありません。さらにHexoの場合には公式サイト自体がHexoで構築されているため、ドキュメントの修正確認に新しい知識は不要です。具体的には公式の手順を確認してもらう必要がありますが、たった5ステップで通常のバグ修正とほぼ変わりないことがわかると思います。 注意点としては、プルリクエストには各リポジトリにマナーがあるのでコントリビューションガイドにはよく目をとおすことと、過去のクローズ済みのプルリクエストを確認して自分のプルリクエストに問題ないか確認することを忘れないようにしましょう。マナー違反のプルリクは優しく注意してくれる場合もありますが、問題が多ければ放置や強制クローズもありえます。ちなみに今回のプルリクは半日程度でマージしてもらえました4。 マージしてもらったら公式のプラグイン一覧でoembedで検索して、問題なく掲載されているか確認します。自分の1件しか引っかからなかったので、どうやらhexo-tag-oembedの方は公式への登録を行っていなかったみたいです。もったいない・・・ 2.8. ブログやツイッター等で告知するまさしくこの記事です・・・ 3. hexo-oembedの今後npmパッケージとしては結構ニッチかなと思っていましたが、公開から一週間も経たないうちにダウンロード数が200を超えたので意外とニーズはあったみたいです。ということでしっかり開発およびメンテナンスはしていきたいと思います。 まず開発の基本方針として、サイト固有の対応を入れるつもりはないです。それやりだすとキリがないので・・・実はjquery-oembed-allというライブラリも見つけていたのですが、更新が止まっていたし自分がこのようなものを作ろうと思ってもメンテナンスが辛そうなので正直やりたくありません。しかもこのライブラリのフォールバック先のYQL (Yahoo! Query Language)は2019/1/3で終了したみたいです。 ただちょっと面白いなと思ったのはフォールバック先にOpen Graph protocolがあることです。OGP対応は割とメジャーなSEO対策でリンク先のサムネイルや説明の表示でお馴染みだとおもいます。もちろんこのページもOGPには対応しています。hexo-oembedのフォールバック先として一応Embed.lyには対応していますが5、OGPへのフォールバックは今後検討しようかと思います。 もちろんプルリクは大歓迎です。またバグ報告、ご意見・ご要望はGitHubのIssuesにお願いします6。 4. まとめ既存のOSSへの貢献としてメジャーなのはバグ報告したりプルリクを出したりすることだと思いますが、プラグインを作って公開して公式の載せてもらうまでを丁寧に解説した入門記事は意外と少ないと感じました。断片的な記事は多いのですが、それだとどういう流れなのか入門者にわかりづらいかなと思い、本記事ではhexo-oembedをテーマにOSSへの貢献への流れを要点を絞って解説するようにしました。 実際に既存のOSSへの貢献を始めるには、いきなり本体に手をだすよりもプラグインから手をだした方がうまくいく場合が多いです7。プラグインを通して本体を見たほうが理解しやすいですし、プラグインを作ってる最中に本体のバグを見つけることも多いので本体へのプルリクを書く動機にもなります(笑)。そのうちプルリクを送り続けていれば本体側にも十分詳しくなってコラボレーターとして招待されるかもしれません。実際にプラグインから始めてそのOSSへのメンテナーになった人は数多くいるので、まずOSSに貢献してみたいと思ったら気に入ったOSSを見つけてプラグイン機能を探して、実際に作ってみることをオススメします。 最初はHexoタグの作り方を簡単に紹介する記事にする予定でしたが、もしかしたら既存のOSSへの貢献の入門記事としてもニーズがあるのではないかと思ったのが今回の記事を書こうと思った動機です。長くなってしまいましたが本当にここまで読んで頂いてありがとうございました。この記事が読んでくれた方のOSS貢献の一助となれば幸いです。 1.ちなみにnpm linkというコマンドもあってこちらはさらに便利で、複数の非公開パッケージを開発するときに真価を発揮します。興味がある方はぜひ調べてみることをオススメします。 ↩2.自分も頑張れば読めはしますが、書くのは苦手です・・・ 話すのはもっと無理です・・・ ↩3.認証周りの設定とかいろいろ面倒くさいのでそこそこ時間ががかかると思います・・・この記事の余白に書くのは狭すぎるのでGoogle先生のお力をお借りください・・・・ ↩4.このプルリクです。 ↩5.一応利用しているライブラリ(node-oembed)が対応していたので、対応コードは入れてみたけど実際に動かしてはいないです・・・ ↩6.コントリビューションガイドはまだ用意していませんが、いずれ突っ込む予定です・・・ ↩7.プラグインで実装できるものを本体側を修正してプルリク送るとメンテナーに嫌がられることが多いです。それはメンテナーは単に機能を見ればよいのではなく、全体との整合性や今後の開発の障害にならないか等、取り込みの判断に非常に多くの労力を割かなければいけないからです。従ってプラグインでできることはプラグインで実装したほうがよいです。そしてどうにもならない部分だけ最小限に絞って、なるべく汎用的で、互換性に影響ないような形で本体側にプルリクを投げるとうまくいく確率が高まります。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/02/11/hexo-oembed-2/"},{"title":"ScalaMatsuri 2019に参加してきました","text":"ScalaMatsuri 2019に参加してきました。毎年何気に楽しみにしているお祭りです。今年も面白い発表がたくさんあり、お祭り気分も充分に味わえたので満足の行くカンファレンスでした。 ScalaMatsuri 2019｜日本最大級の Scala のカンファレンスScalaMatsuri 2019｜日本最大級の Scala カンファレンスhttps://2019.scalamatsuri.org/Scala をテーマに日本最大級のカンファレンスが開催されます。 目次1. ScalaMatsuriとは2. 会場について2.1. 行動規範3. カンファレンスDAY3.1. Scala 3って、私にはどんな影響があるの? @joang38 (Joan Goyeau)3.2. 再帰:スキーム,代数,finally tagless,データ型. 統合されたビジョン @alexknvl (Alexander Konovalov)3.3. プロジェクトで引き回す型をEffにするメリット @wing_007 (ma2k8)3.4. こんなに違う！ @kaelaela (前川裕一)3.5. DOT計算をやさしく説明する @kmizu (水島宏太)3.6. Making Logic Monad @halcat0x15a (ねこはる)3.7. sbt コア・コンセプト @eed3si9n (Eugene Yokota)3.8. 継続とDI @gakuzzzz (中村学 (がくぞ）)4. アンカンファレンスDAY5. 聞きたかったけど聞けなかったセッションたち6. 食事7. カフェスペース8. まとめ9. 戦利品 1. ScalaMatsuriとはプログラミング言語Scalaをテーマにした日本最大級のカンファレンスです。今年は「Scala Conference in Japan 2013」から数えて６回目の開催になります。ScalaMatsuriの特色としてはセッションの決定がチケット購入者の投票によって決まること、海外スピーカの招待に力を入れておりセッションに日英同時通訳が入っていること、アンカンファレンスがあることなどが挙げられます。また、その名の通り「お祭り」の雰囲気を出すための工夫が随所に見られ、非常に親しみやすいカンファレンスとなっています。 2. 会場について 日時と場所は以下のとおりです。自分は6/28のカンファレンスから参加しました1。場所はお台場です。ビッグサイトの近くと言ったほうが分かる人にはわかるかもしれません（笑）。新橋からゆりかもめに乗って行ったのですが、東京国際クルーズターミナル駅と言う聞き慣れないアナウンスが流れて来て仰天しました。どうやら最寄り駅の駅名が変わっていたみたいです・・・2 日時 6/27(木) - 6/29(土) 10:00〜 場所 東京国際交流館 2.1. 行動規範到着したのは10:00頃で、ちょうど行動規範3のアニメが流れているときでした。このアニメは毎年クマくんのところで笑ってしまいます。アニメが流れ終わったところで、次のオススメ動画に子供向け動画（３匹の子豚？）が出てきて会場が爆笑していました。 3. カンファレンスDAYScalaMatsuriは27日(金)がカンファレンスDAYでした。ScalaMatsuriで凄いと思うのは３本のセッションが並行して走っていてそのうち２つの部屋では同時通訳が行われていたことです。そのため翻訳レシーバを借りれば英語のセッションでも問題なく楽しめました。4。 以下が27日に自分が聴講したセッション一覧です。ちょっとこじらせているので関数型プログラミングに偏っています（笑）。 Scala 3って、私にはどんな影響があるの? @joang38 (Joan Goyeau) 再帰:スキーム,代数,finally tagless,データ型. 統合されたビジョン @alexknvl (Alexander Konovalov) プロジェクトで引き回す型をEffにするメリット @wing_007 (ma2k8) こんなに違う！ @_kaelaela (前川裕一) DOT計算をやさしく説明する @kmizu (水島宏太) Making Logic Monad @halcat0x15a (ねこはる) sbt コア・コンセプト @eed3si9n (Eugene Yokota) 継続とDI @gakuzzzz (中村学（がくぞ）) ここからは少し長くなりますがセッションの感想を一つずつ述べていきます。 3.1. Scala 3って、私にはどんな影響があるの? @joang38 (Joan Goyeau)Scala3(Dotty)の影響についての発表でした。以下が言及があった機能一覧です5。概ね知っていましたが、いくつか新しい発見もあったので有意義なセッションでした。 Enumerations, Algebraic Data Types やっとScalaにまともなEnumurationが来るといった話です。今まではcase objectで書いていたものがenumキーワードで大分シンプルに書けるようになります6。 交差型, 合併型 ようやくTypeScriptでできた型表現がScalaでもできるようになりました 交差型に関してはwithもあったけど交換可能でなかったので、ようやく本物がDottyにも入ったということです NULLは100万ドルの間違い たぶん#5747の話だったと思います 現在でもOptionがありヌルポを見る機会はあまりありませんが、将来的には本当に見る機会がなくなるかもしれません Opaque 型エイリアス AnyValは、いけてないよねという話7。opaqueで本物の型エイリアス（いわゆるゼロコスト抽象化）が手に入ります 拡張メソッド 型が定義された後にメソッドを追加できる機能 拡張メソッドの構文はすごいGo言語チックだと思いました8 デリゲート 今までの型クラスのためのハックがまともな構文として定義されましたというお話 マルチバーサル等価性 トレイトパラメータ givenが使えるのは大きいです Parameter Untupling mapでcaseのあのパターンがいらなくなるお話。これは地味に便利 TASTy コンパイル時の成果物の新しいフォーマットです クロスコンパイルが容易になります 上記の機能のいくつかはこのブログでも言及しているので参考にしてください。 cats cats catsScala3に入るかもしれないContextual Abstractionsを味見してみた(更新・追記あり)https://hinastory.github.io/cats-cats-cats/2019/02/24/scala-dotty-contextual-abstractions/Scala3のリサーチコンパイラであるDottyにImplicitsに代わる「Contextual Abstractions」と呼ばれる一連の機能が実装されていたので一部を味見してみました。 (2019年3月10日追記・更新: 追記内容はここを見てください) (2019年6月22… 3.2. 再帰:スキーム,代数,finally tagless,データ型. 統合されたビジョン @alexknvl (Alexander Konovalov)ScalaMatsuriでは例年話される関数型プログラミングネタです。最初の方はずっと同型（Isomorphism）について話していたので、どこに向かうのかよく分かりませんでしたが終わってみれば関数型プログラミングの流行りのテクニックの総集編みたいな内容でした。結論は以下のような感じです。 1folds(畳み込み) ≈ 再帰スキーム ≈ Final Tagless ≈ Free 「folds(畳み込み) ≈ 再帰スキーム」と「Final Tagless ≈ Free」は予想は付いていましたが全てが同型だとは思っていませんでした。まぁ「代数」というくくりで見ると確かに同じ気がします。 3.3. プロジェクトで引き回す型をEffにするメリット @wing_007 (ma2k8)この話も毎年恒例になってきたEff(Extensible Effects)のお話です。Effを簡単に説明するとモナドの合成と評価を柔軟に行うためのテクニックです。 上記スライドでEffと対比されていた型は以下です。 Future[A] Future[Try[A]] Future[E Either A] EitherT[Future, E, A] Eff[R,A] この中で自分がよく使うのは1です。2,3,4は使ったことはありますが型合わせが面倒なので部分的にしか使ったことはないです。5はEffそのものですが写経したくらいで実践で使ったことはありません。いずれ使って見たいと思いますが、問題はチーム全員が使いこなせるかどうかですね・・・ 3.4. こんなに違う！ @kaelaela (前川裕一)ScalaをコンパイルするとシンプルでKotlinをデコンパイルすると複雑だというのが面白かったです。あと似ている型も注意が必要そうです。デコンパイルしよう。 3.5. DOT計算をやさしく説明する @kmizu (水島宏太) スライド kmizuさん9によるScala3の理論基盤であるDOT計算のお話です。現実のプログラミング言語を厳密に理論基盤に載せるのは難しいので、妥当な言語のサブセットを作ってモデル化したものを「核計算」というらしいです。DOTはScala3の核計算であり、健全性が厳密に証明されているようです。残念ながらDOT計算の詳細は時間がなかったので翌日のアンカンファレンスに持ち越されました。 3.6. Making Logic Monad @halcat0x15a (ねこはる)論理プログラミング言語としてはPrologが有名ですが、その一部の機能をScalaで実現しようというものでした。自分は論理プログラミングはあまり知らなかったのですが、論理的に可能な組み合わせが全て解として求めることができるというのは面白いと思いました。 Making Logic Monad from Sanshiro Yoshida 具体的なモナドの構成はスライドを見たほうがいいと思いますが、印象に残ったパワーワードは以下の２つです。 バックトラッキングはモナドプラス FizzBuzzは実践的アプリケーション あとはType-aligned sequenceというデータ構造も知らなかったので後で調べて見ようと思います。 3.7. sbt コア・コンセプト @eed3si9n (Eugene Yokota)scalaで最もよく使われているビルドツールであるsbtの解説でした。いつもお世話になっているsbtですが、いつもなんとなくというかその場しのぎで調べて動かしていたので、いろいろと勉強になりました。内容はおそらく6/11-23に開催されていた Scala Days 2019で発表されたものと同じではないかと思います。 Player is loading... 以下はメモです。 空でも動く 流れを堰き止めるのがcommand commandよりtaskがオススメ キーは4次元 デリゲーションルール Zero キーは最も広いスコープで定義して、最も狭いスコープで参照 3.8. 継続とDI @gakuzzzz (中村学 (がくぞ）) スライド 継続渡しスタイル(Continuation Passing Style/CPS)とDI(Dependency Injection)を比較する非常に興味深いセッションでした。自分の継続のイメージはSchemeから来ていてどちらかと言うとgotoと比較されるプリミティブだったのですが10、確かに言われて見ればDIだなと納得しました。あとSchemeでCPSを使うとネストが深くなりますが継続モナドを使うとネストを潰せてDSLっぽくなるのも面白かったです。CPSとDIのどちらを使うべきかという問いに対しては、使い分けが必要でCPSはアドホックな場面で使いましょうということでした。個人的に気になった点は似たような使い方ができるリーダーモナドやMinimal Cake PatternやEffなどと比較した場合はどうなんだろうということでした。また時間があったら自分で考察してみたいと思います。そういえば大昔にScala本体にも継続が実装されていていつの間にか標準から切り離されていたけど今も息をしているのだろうか・・・ 4. アンカンファレンスDAYアンカンファレンスはカンファレンスのカウンターパートです。つまり、カンファレンスほど形式張らずにアドホックにセッションを決定しようという比較的軽いノリですが、まさしく「お祭り」の雰囲気に相応しいワイワイした感じがとても印象的です。 アンカンファレンスが具体的にどのように進むかというと、前日から用意されたホワイトボードに付箋で聞きたいネタや喋りたいネタを貼って、セッションに参加したいと思ったら赤いシールを貼ります。そして当日の朝会や昼会で投票が多くて発表者やファシリテータがいるものを優先してセッション枠を埋めていきます。上の写真はホワイトボードの様子と昼会でセッション枠を決めている様子です。 以下参加したセッションです。どれも非常に面白かったのですが長くなるのでメモベースの記載になります。 仕事でScalaを使おう - Arm Treasure DataでのAirframe活用事例 @taroleo (Taro L. Saito ) 最近よく聞く Airframeのお話です AirframeはDIコンテナだと思いこんでいましたが、どうやらDIだけではなく様々な便利ツールの集合体のようです MessagePackがTreasure Data発だと初めて知りました。MessagePackを使いたくなったらAirframe logとlauncherは後で試してみよう Bengal: Dotty Cats @ Walter Chang Bengal: A less than minimal functional library in the spirit of cats in Scala 3. Scala Taiwanから来日された方の発表です Scalaで型クラスを使う場合に広く使われているcatsライブラリをScala 3で実装するとどうなるかという発表でした 個人的に面白かったのは shapelessを使って型クラスを導出する部分でした DOT Calculus I didn’t explain yesterday @kmizu (水島宏太) DOTは難しいということが分かりました 特にサブタイプは鬼門だと・・・ DOTは健全らしいです （参考: dot soundness) Fast & Functional 最初の１時間だけ聴講しました パワーワード「制限は開放し、自由は制限する」 WiFi x Scala: Implementing Captive Portal in Scala and deploy into #ScalaMatsuri @ kuro_m88 Captive Portalという仕組みを初めてしり勉強になりました 今回のWifi事情はおそらく他のカンファレンスと比べても非常に良かったと言えるくらい安定して繋がりやすかったです その裏舞台を聞けて非常によかったです Write stacksafe non-tailrec recursions @jooohn1234 再帰 is cool Stack overflowを避けるためにトランポリンを使おうという話 Fujitask meets Extensible Effects @ y-yu Extensible Effectsでトランザクションモナド“Fujitask”を作る - Qiita トランザクションを型レベルの計算に落とせるというのは面白い kits-effを利用している 進捗大陸05にねこはるさんのkits-effの記事があります。自分も前回の技術書典で購入していました From Go To Scala Easy vs Simple 個人的にはアンカンファレンスのなかで一番熱いプロレスでした KotlinやC++やSwiftも参戦してきて、カオスな雰囲気がいい味を出していました Applicative Functor - Selective Functor - Monad Selective Functorという聞き慣れない言葉に誘われて聴講しました なんとなく仕組みはわかりましたが、どういうときに使うものかイマイチわからなかったので後で勉強します・・・ 5. 聞きたかったけど聞けなかったセッションたちあとで見るために資料のリンクだけ載せておきます。 Using Akka Cluster for a payment service Running Scala on AWS Lambda in a Snappy Way 同僚の登壇資料作成をScalaで手伝った話 How we replaced a 10-year-old Perl product using Scala ScalaのOSSに貢献しよう ~ Phil Bagwell Award記念講演 High Performance Scala/high_performance_scala How to build an Event-Sourcing system using Akka with EKS [Running in PRODUCTION Reactive Systems with cloud services ScalaでGANをスクラッチ開発した話@ScalaMatsuri コードで理解するPlayframeworkの脆弱性 Functional Concurrency in Scala 101 悩める開発者に贈る〜 サービスの継続的な成長を支える分析設計手法 Clean Architecture in Practice Intro to typeclass in Scala Scala Driven Management Case of Ad Delivery System is Implemented by Scala and DDD ピュアなドメインを支える技術/pure domain model and the technology behind it How to test proper{t,l}y (Scala Matsuri edition) 6. 食事ScalaMatsuriの魅力には食事もあります。以下の写真は昼食のお弁当です。お弁当はいくつかの種類があってちゃんとベジタリアン向けのお弁当も用意されていました。多分以下は鳥がメインのお弁当だったと思います。 ScalaMaturiで「祭り」の気分が味わえるのは屋台の存在も大きいです。カンファレンスDAYにはたこ焼き屋が出ていて、アンカンファレンスDAYにはかき氷屋が出ていました。 懇親会も盛り上がりました。途中でLTもありました。 左下はアンカンファレンスDAYの朝食で、右下はデプロイされていたうまい棒です。コーヒーとお菓子は豊富に配備されており、セッションの合間に飲んだり食べたりしていました。 7. カフェスペース今年から1階にカフェスペースが設けられていました。カフェスペースの入り口でScalaのシンボルである螺旋階段がお出迎えをしてくれました（笑）。カフェスペースなのでコーヒーやお菓子がデプロイされていましたが、それ以外にもScala関連書籍が置かれており自由に読めるようになっていました。 8. まとめScalaMatsuriは本格的なカンファレンスでありながらその名の通り「お祭り」気分が味わえる素晴らしいカンファレンスです。もっとScalaMatsuriを知りたい方はScalaMatsuri運営ブログやTwitterのハッシュタグ #ScalaMatsuriもチェックしてみてください。 この記事を読んでくれた方に、少しでも\u001cScalaMatsuriの楽しさをお伝えできたなら幸いです。 参加者、関係者の皆様、お疲れ様でした！ 9. 戦利品たくさんの記念品を頂きました。今年も型安全でありますように・・・ 1.6/27はワークショップDAYです。 お絵かきで学ぶScala教室と OSS ハッカソンが開催されていました。OSSハッカソンはよく見たらチューターがガチですね。どんなコントリビュートがされたのか気になります。 ↩2.2019年3月16日に「船の科学館駅」から「東京国際クルーズターミナル駅」に改称したようです。また「東京国際展示場正門駅」も「東京ビッグサイト駅」に変更されています。こちらはまだ分かりやすい気がしますが、東京国際クルーズターミナルの開業は2020年7月ということらしいので、寝耳に水でした。 ↩3.行動規範(Code of Conduct/Universal Access)はカンファレンス参加者が守るべきルールです。内容はハラスメントフリーで参加者全員を尊重しつつ、みんなが気持ちよくカンファレンスを楽しめるようにするための取り決めになっています。行動規範はイベントごとに微妙に違うので事前に目を通しておくことをおすすめします。 ↩4.この同時通訳は日本語->英語だけではなく、英語->日本語も行われています。したがって英語話者に対して日本語で質問ができたりその逆も可能です。RubyKaigiでも同時通訳はありましたが日本語->英語だけだったので、その点ScalaMatsuriの方が英語が苦手な人でも楽しめると思います。またスライドに日本語訳が付いているのも有難かったです。 ↩5.軽くメモしただけなので間違っているかもしれません。Dottyドキュメントのリンクは自分調べて貼りました。 ↩6.Enumerationクラスは本当に使えない子なので見なかったことにしてあげてください。 ↩7.AnyValがなぜいけていないかは SIP-35を参照してください。 ↩8.拡張メソッドの構文は#6760で変わるかもしれません・・・ マージされずにクローズされました。ただまたいつかこの話題が再燃するかもしれません・・・ ↩9.退職エントリが話題になっていました。他にもドワンゴの退職エントリを見かけたので、本気でやばそうですね・・・ ↩10.Schemeでは継続が第一級オブジェクトで、機能名からcall/ccともよく呼ばれています。goto（またはsetjmp/longjmp）と呼ばれる理由は「Scheme/継続の種類と利用例 - Wikibooks」を参照するとよく分かると思います。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/07/04/scala-matsuri-2019/"},{"title":"暗記が苦手な人間が「AWS認定ソリューションアーキテクト - アソシエイト」に5日間で合格した話","text":"AWS(Amazon Web Service)は言わずと知れたシェアNo.1のクラウドコンピューティングサービス1です。そのAWSの認定の中でも屈指の人気を誇る「AWS認定ソリューションアーキテクト - アソシエイト」を5日間の勉強で合格することができました。本当に月曜日に思い立って試験の申込みをして勉強を開始し、その週の土曜日の朝一の試験で合格しました。本記事はその合格に至るまでの体験談になります。 Amazon Web Services, Inc.AWS 認定ソリューションアーキテクト – アソシエイト | AWShttps://aws.amazon.com/jp/certification/certified-solutions-architect-associate/「AWS 認定ソリューションアーキテクト – アソシエイト」試験は、ソリューションアーキテクトの方を対象としており、AWS 上でセキュアかつ堅牢なアプリケーションを設計および展開する方法についての理解度を評価するものです。 目次1. はじめに2. 「AWS認定」とは2.1. 「AWS認定」の全体像2.2. 「AWS認定ソリューションアーキテクト - アソシエイト」の認定試験について3. スペック4. きっかけと動機と衝動5. 試験の申し込み6. 勉強方法6.1. 一日目(月曜日)6.2. 二日目(火曜日)6.3. 三日目(水曜日)6.4. 四日目(木曜日)6.5. 五日目(金曜日)6.6. 試験当日(土曜日)7. 認定に費やした勉強時間と費用8. 試験結果9. まとめ10. おまけ：手を動かして学んだほうが良いAWSサービス11. 参考文献 1. はじめに最初にお断りしておきますが、AWS認定プログラムアグリーメントの規約により、認定試験を含む試験関連資料の内容については他言してはいけないことになっているので、その辺はAWSが公開している情報に準拠します。そして試験やAWS自体も今後変わっていくと思われるので、実際に受ける際には最新の情報を確認するようお願いします。 また、本記事は筆者の体験談になりますので、以下の条件に合わない人の参考にはならないかもしれないのでご注意ください。 「AWS認定ソリューションアーキテクト - アソシエイト」に短期集中型かつ実践重視で合格したい中堅エンジニア ITの基礎知識(OS、DB、ネットワーク、セキュリティ)があり、ソフトウェアの開発経験あり AWSのサービスに関してEC2、VPC、S3くらいには触ったことはあるが、それ以外は自信がない 書籍よりもハンズオンで実践的にAWSを使う技術を身につけたい ぶっちゃけ暗記が苦手もしくは嫌い 夏休み等のまとまった連休を使って一気に資格を取りたい 2. 「AWS認定」とはまずは簡単にAWS認定についてどういったものなのかを説明したいと思います。すでにご存知の方はこの章は飛ばしてください。AWS認定は簡単に言えば「AWSの専門家」を認定するためのもので、特徴としてはAWSの実務経験を強く意識した試験になっていると思います2。 AWS 認定は、業界で認められている資格情報を使用してクラウドの専門知識を認証することで、学習者が信頼性と自信を築くことと、組織が AWS を使用してクラウドを主導していくスキルのあるプロを識別することに役立ちます。 AWS認定aws.amazon.com/jp/certification 2.1. 「AWS認定」の全体像AWS認定は3つののレベルと専門領域から成り立っています。自分が合格した「AWS認定ソリューションアーキテクト - アソシエイト」は、その3つのレベルの認定のちょうど中間の「アソシエイト」レベルのもので、1年間のAWSを使った実務経験がある方が対象となっています。「ソリューションアーキテクト」とは、実際にAWSを使った安全かつ堅牢なソリューションを設計できる人材を指しています。 *AWS 認定より引用* アソシエイトレベルには他にも「AWS 認定デベロッパー – アソシエイト」と「AWS 認定システムオペレーション (SysOps) アドミニストレーター – アソシエイト」があり、それぞれAWSを使った「開発」と「運用」の分野のプログラムになっています。上位レベルの「プロフェッショナル」には、「ソリューションアーキテクト」はそのまま上位認定がありますが、「開発」と「運用」は統合されて「AWS 認定 DevOps エンジニア – プロフェッショナル」という認定になっています。「プロフェッショナル」レベルは2年間の実務経験たある方が対象になっています。また、アソシエイトの下には「AWS 認定クラウドプラクティショナ」という基礎レベルの認定もあります。このレベルは特に区分けはされておらず半年の実務経験が想定されています。 3つのレベルの外には5つの専門領域があって、それぞれ「セキュリティ–」、「ビッグデータ」、「高度なネットワーク」、「機械学習」、「Alexa スキルビルダー」になります。うーん、何か一つだけとてつもなく浮いている認定があるのですが、これは突っ込みどころなのでしょうか・・・ Alexaのスキル開発を「専門領域」と呼ぶには他の領域と比較してもニッチすぎる気が・・・ どうしてもAmazonが押したいのは理解できますが、せめてネーミングだけでも「音声自動化」とか汎用的なネーミングの方が違和感がなかった気がします(笑)。 2.2. 「AWS認定ソリューションアーキテクト - アソシエイト」の認定試験について「AWS認定ソリューションアーキテクト - アソシエイト」の認定試験の概要は以下のとおりです。以下はアソシエイトレベルの試験では共通しています。\b合格ラインに関しては720固定ですが、スコア自体はTOEICのように試験問題の難易度に応じて調整されるようです。 試験時間 130分間 問題数 65問 回答タイプ 択一選択問題: 選択肢には 1つの正解と3つの不正解 複数選択問題: 5つの選択肢のうち、2つが正解 合格ライン 100～1000点の範囲のスコアでレポート 最低合格スコアは720 受験料金 15,000 円（税別）/ 模擬試験 2,000円（税別） 試験言語 英語、日本語、韓国語、中国語 (簡体字) 認定期間 3年 再認定時は同じ試験を50%割引で受けられる 試験範囲は以下のとおりです。 分野 試験における比重 分野 1 回復性の高いアーキテクチャを設計する 分野 2 パフォーマンスに優れたアーキテクチャを定義する 分野 3 セキュアなアプリケーションおよびアーキテクチャを規定する 分野 4 コスト最適化アーキテクチャを設計する 分野 5 オペレーショナルエクセレンスを備えたアーキテクチャを定義する 合計 100% ご存知の方もいるかも知れませんが上記の分野は「Well-Architectedフレームワーク」に沿って決められているので、まずは「Well-Architectedフレームワーク」がどういったものかをざっくり理解してから試験に取り組んだほうがよいかと思われます。 3. スペックさて、いよいよ本題の体験談に入っていきたいと思います。まずは試験を受ける前の自分の力量を以下に簡単に書き出してみました。 基礎知識 OS、ネットワーク、DB、セキュリティの基本的な知識あり クラウド関連 AWS EC2関連、VPC、S3に関しては「開発」、「テスト」で利用するには問題ないレベル その他の基本的なサービスに関しては名前と機能を知っているか若しくは軽く触った程度で、雑な知識と経験しかない GCP,Azule 少しだけ触ったことがある程度 Docker, Kubernetes 利用経験あり 開発経験 ミドルウェアやWebアプリを中心にいろいろ まぁ、上記のとおりなので「初心者が5日間でとった！」みたいな衝撃的な内容ではないです。また、AWSに関してもEC2関連とVPC、S3といったAWSのキモは押さえており、その点だけでもかなり有利なのであまり参考にならない体験談かもしれません・・・ 4. きっかけと動機と衝動AWS認定試験を実際に受けようと思ったきっかけは「【未経験でも挫折しない】40時間でAWS認定ソリューションアーキテクトアソシエイトを取得する方法 - Qiita」という記事を読んでからでした。実はAWS認定自体は以前から知っていましたが、試験勉強よりも手を動かす方が好みなのでスルーしてきました。しかしこの記事ではUdemy3の以下の講座がオススメされており、しかしちょくちょくセールもやっているということだったので覗いてみたら、見事にセール中だったので釣られてしまいました・・・ これだけでOK！ AWS 認定ソリューションアーキテクト – アソシエイト試験突破講座（初心者向け21時間完全コース） | Udemy とりあえずこのときは買っただけで満足して放っておいたのですが、とある事件が起こりました。夏休みは某イベントの参加を楽しみにしていたのですが、主にTwitter方面から阿鼻叫喚具合が伝わってきたので、行く気が失せてしまったのです。そんな中で急に天からとある言葉が降ってきたのです。「そうだ、AWS認定を受けよう」 5. 試験の申し込み思い立ったが吉日、さっそく試験の申込みをしました。試験日は自分の実力と確保可能な時間から考えて最短で合格できる期日を設定しました。そうすれば、最短の合格に向けてモチベーションを維持しやすくなると思ったからです。自分はちょうど夏休みで勉強時間が確保できる見込みがあったので、試験日を申込日(8/12)から5日後の8/17(土)にしました。ちなみに、試験は24時間以上前なら無料でキャンセル可能であり、試験の延期も可能なので試験を受ける気になったら申し込みを躊躇する理由はありません。 試験の申し込みは以下のページからアカウントを作成して申し込む必要があります。 AWS 認定 | AWS トレーニングと認定 試験のプロバイダーにはPSIとピアソンVUEがあるのですが、まずはピアソンVUEで試験会場を探してみることをオススメします。自分はピアソンVUEでしか受験したことがないので完全に私見になりますが、ピアソンVUEはAWS以外にも試験開催が豊富で安心して受験できます。今回は「武蔵小杉テストセンター」で受験しましたが、申し込みから受験まで特にトラブルはありませんでした。 6. 勉強方法正直言って、前述したQiita記事とほぼ同じです。ただし自分は書籍での勉強は行わず、UdemyのコースとWebで無料で公開されている問題だけを解きました。特にUdemyのコースをのハンズオンを重視して手を動かしながら、重要なサービスについてはきちんと理解するまで触りました。 サンプル問題の確認 このページから「サンプル問題のダウンロード」をクリック Udemyのコースを受ける これだけでOK！ AWS 認定ソリューションアーキテクト – アソシエイト試験突破講座（初心者向け21時間完全コース） | Udemy 公式模試を解く AWS認定のページから、アカウントを作成して申し込む(税抜2000円) Web上の問題を解く AWS SAA/SAP資格 予想問題集 Web版 | 日本語完全無料 基本的には上記の順番で勉強したのですが、せっかく五日間で集中して終わらせたので、ここからは日付ごとにもう少し掘り下げて実施したことや感想、注意点を述べたいと思います。 6.1. 一日目(月曜日)まずは、試験概要の確認とサンプル問題の確認を行いました。どんな試験でもそうですが、合格を目指すならまず最初に試験の形式の把握と具体的な問題の傾向を把握することは一番最初にやるべきことだと思います。これをやらないと見当違いの対策をしてしまい後で後悔する可能性が高くなります。試験の概要とサンプル問題へのポインタは前述したとおりです。サンプル問題は初見で10問中5問しか正解できませんでした。つまりこのままでは確実に落ちるのでしっかりと対策を行う必要がありました(汗)。 次にUdemyのコースを受けました。基本的には動画で講義を観ながら適宜ハンズオンを実施する形式です。講義は時間がもったいないので1.75倍速で観ました。重要なところはブックマークができるようになっているので、セクションを見終わった段階で見直してメモに書き出してました。ハンズオンは動画を観ながらじっくりこなしています。知識と実際のAWSの感触は大分違うので、このハンズオンの内容レベルがきちんと身についていないと実務ではまったく役に立たないと思います。正直言って合格したいだけなら、ハンズオンをやらなくても「模擬試験」をたくさんこなせば合格は可能だと思います。しかし自分は実践的な能力の向上も欲しかったのでハンズオン重視で取り組みました。 セクション1 まずは知ってみる セクション2 Day1対応の実施 セクション3 AWSとアソシエイト試験の概要 セクション4 IAM セクション5 EC2 初日はUdemyのコースをセクション5まで進めました。ある程度知識があった箇所なので動画も理解している箇所はがんがん飛ばしてきいきました3。IAMは知っているようで、意外と理解できていない部分があったので勉強になりました。 ハンズオン後に重要なのEC2を起動したまま放置しないことです。使わないのなら停止して、できれば削除したほうが安く済みます。動画でも注意されますが、気をつけておかないとすぐに料金に跳ね返ってきます。なので忘れないようにCloudWatchの請求でアラームをしかけておくか定期的に「マイ請求ダッシュボード」で料金を確認する習慣をつけることをオススメします。 6.2. 二日目(火曜日)二日目の最初はVPCとS3のハンズオンをしっかりやりました。このふたつはAWSを代表するサービスなので、AWSのコンソールから一通りできることを確認して理解があやふやな箇所がないようにすることが肝要です。「Well-Architected Framework」はAWSが推奨するAWSを使った設計の方法論です。これは非常に重要なのでしっかりと理解すべきで、動画だけでなくホワイトペーパーもしっかり目を通して置く必要があります。クラウドではキャパシティプランニングやセキュリティの考え方が特にオンプレとは異なるので要注意です。 セクション6 VPC セクション7 S3 セクション8 Well Architected Framework 6.3. 三日目(水曜日)三日目はクラウドの強みともいえる、高可用性、信頼の設計です。おなじみのマルチAZ構成のWebサーバをロードバランシングとオートスケールさせるやつです。さらにデータベース(RDS)を定期的にS3にスナップショットでバックアップをとりつつ、マルチAZ構成で自動フェールオーバできるようしてリードレプリカでスケールアウトさせる構成も学びます。 次にRoute53はDNSですが、ただのDNSではありません。基本的にはBINDでDNS運用の経験がある人なら特に問題なく利用可能ですが4、aliasレコードやルーティングポリシーやトラフィックフロー等、新しい機能や概念も出てくるのでしっかりと理解する必要があります。あと、ハンズオンでドメイン名を使うので持っていない人はドメイン名をお名前.comで早めに取得しておいた方が良いです5。DNSの反映には数時間から72時間かかるので、もしドメインを新規取得するなら早めにやっておくことをオススメします。 データベースではDynamo DBやAurora、EFSのハンズオンがあり、よく理解できていないところもあったので助かりました。そしてなぜかデータベースのセクションにIoTでよく用いられるKinesisのハンズオンも含まれていました。いや、ハンズオンは良かったのですが「なぜ、ここにいれたし」と思ってしまいました（笑）。 セクション9 信頼性の設計 セクション10 Route53 セクション11 データベース 6.4. 四日目(木曜日)四日目は一番楽しみにしていました。この日に学べるサービスは「ElastiCache」、「Lambda」、「CloudFormation」です。もちろん一番興味があったのはサーバレスの中核を成す「Lambda」で、FaaS(Function as a Service)をAWSでどのように実現していくのかを学びました。手を動かしてみると今までの知識だけの理解とは違い色々分かって面白かったです。特に「設計図」や「 Serverless Application Repository」\b等で簡単に始められて共有する仕組みがあり、他の人が作ったサーバレスアプリケーションを眺められたので勉強になりました6。「ElastiCache」は使い方が限られているので役割をしっかり押さえておけば大丈夫です。「CloudFormation」は実務では凄い有用ですし実際触ってみると嵌りどころが多い機能ですが、試験ではそこまで突っ込んだ内容は出ないと思われるので、時間がなければ役割とフォーマットだけ押さえておけば大丈夫です。 セクション12 キャッシュの活用 セクション13 サーバレス セクション14 環境の自動化 6.5. 五日目(金曜日)五日目は最後の仕上げです。セキュリティと運用は座学だけかとおもったらCloudWatchのハンズオンがありました。CloudWatchもAWSのなかで重要なサービスなので手を抜かずしっかりとハンズオンして不安な箇所を潰しました。 あとはひたすら模擬試験を解くだけです。Udemyの模擬試験は試験と同じ65問ありますが、本番の130分もかからないです。早ければ1時間もかからないと思うので、見直しはせずさっさと確定して結果を見て、間違った箇所を復習するほうが効率は良いです。ただし、不安な箇所はなぜ最終的にその選択肢を選んだのかをメモしておくと、復習の際に役立ちます。Udemyの模擬試験は２回分あり、さらにおまけでクイズ形式の問題もついてくるので合計3回分あります。これらをしっかりと復習して間違えないようになればかなり自信は付くと思います。 公式模擬試験は問題数が少なく難易度も低めですが、本番とまったく同じUIと操作感なので本番で焦らないためにもぜひ受けておくことをおすすめします。試験終了後は問題を見ることができないので復習に必要な箇所はメモしておいてください。ちなみに自分のスコアは84%でした。残りの時間はひたすらWeb上の問題を解きました。意外と知らない箇所があったので非常に役立ちました。 セクション15 セキュリティと運用 セクション16 模擬試験 公式模擬試験 Web上の問題を解く AWS SAA/SAP資格 予想問題集 Web版 | 日本語完全無料 6.6. 試験当日(土曜日)試験は10:30からだったので、朝７時頃に\b起きて2時間程みっちり復習しました。具体的には模擬試験とWeb上の問題で不安な箇所をもう一度見直しました。 試験会場は武蔵小杉駅の北口正面にある武蔵小杉STMビルの7階のテストセンターでした。会場には15分前に来ることと指示があったので余裕をもって30分前に着きました。早すぎたかなとも思いましたが、とくに待たされることもなく試験を受けることができました。本人確認証には免許証と直筆サイン入りのクレジットカードを用意しました。試験前にはさらに直筆のサインと写真撮影を行い、荷物をポケットの中のものも含めてロッカーにいれて試験会場に入りました。試験はパーティションで区切られたパソコンで行う一般的なものです。集中できるようにイヤーマフや耳栓が置いてありました。 試験問題はだいたい1時間程で解き終わったので最初からもう一度見直して終了しました。合否はその場で表示されるので最後までページを進めてきちんと確認する必要があります。 7. 認定に費やした勉強時間と費用認定に費やした勉強時間と費用は以下のとおりです。月曜から金曜までは6時間集中してやったわけではなく、1時間やったら1時間休憩みたいな感じで疲れすぎないようにやりました。費用は2万以上かかりましたが、5日間AWSで充分遊べて認定も取れたのでいい買い物をしたなと思いました。 勉強時間: 合計32時間 月曜から金曜: 平均6時間 × 5日間 土曜: 2時間(試験前の見直し時間) 費用: 合計21,960円 認定試験: 16,200円 (税込み) 模擬試験: 2,160円 (税込み) Udemy: 1,600円 (セール時の価格) AWS使用料金: 約2000円 8. 試験結果試験結果は無事合格でした。スコアは以下のとおりです。見直しが終わった時点で「満点いけるかも?」と淡い期待を抱いたのですが、その幻想は無惨にも打ち砕かれました。もっと精進します・・・ 9. まとめ「AWS認定ソリューションアーキテクト - アソシエイト」に関してはすでに多くの記事が書かれているので、自分が書く必要があるのか迷ったのですが、短期集中型でハンズオン重視の記事は少なそうだったので書いてみました。 今回の経験で分かったのはAWSの試験問題は実務経験を想定しているので、単なる知識で解ける問題よりも具体的なシチュエーションを提示して、最適解を問うような問題が多いです。このような試験形態の場合は暗記中心の勉強だと足元をすくわれる可能性が高いので、愚直に「理論」と「実践」をバランス良く学ぶことが近道ではないのかと思いました。今回はUdemyのオンライン動画学習サービスを利用して、以下の要領で自分のペースで理解を深めるようにしたので正直暗記が苦手な自分でも試験勉強が苦になりませんでした。 座学で理論を身につける 手を動かして理論と現実のAWSを結びつける 模擬試験の問題のシチュエーションを「具体的なAWSのサービスと操作手順」でイメージをする イメージできなかったら実際にAWSに触りながら理解を深める また、短期集中型の利点ですが、暗記な苦手な人や忘れやすい人でも、短い期間だけ覚悟を決めればいいだけなので大分気が楽になります。試験のストレスやプレッシャーに弱い人でも短期間であれば耐えられるという人も多いとおもうので、そういう方には短期集中型をオススメします。逆に短期集中型の欠点は、当然まとまった休暇（予定なし）が確保しづらいということですね・・・まぁ工夫するしかないです。 なんか結局Udemyの回し者みたいな記事になってしまいましたが、もちろん筆者は単に教材に釣られた購入者に過ぎません(笑)。本記事が、「AWS認定ソリューションアーキテクト - アソシエイト」を受ける方の一助になれば幸いです。 10. おまけ：手を動かして学んだほうが良いAWSサービス「AWS認定ソリューションアーキテクト - アソシエイト」の試験には、ある程度手を動かして学んで「肌感覚」で身につけておいた方が良いサービスが存在すると思っています。特に自分のように暗記が苦手な方や実践に重きを置く方にとっては、AWSの「肌感覚」を身につけることはAWSが絡む様々な場面で強い味方になるはずです。以下は私見ですが手を動かした方がよいサービスを雑に分類してみました7。 絶対に手を動かして学んだほうが良い EC2, VPC, IAM, S3, CloudWatch できれば手を動かして学んだほうが良い RDS, DynamoDB, EFS, SNS, SQS, Route53, Lambda, API Gateway 余裕があれば手を動かして学んだほうが良い ECR, ECS, EKS, Elastic Beanstalk, CloudFormation, CloudFront, ElastiCache 興味があれば手を動かして学んだほうが良い Elasticsearch Service, Codeシリーズ, EMR RedShift, Kinesis, OpsWorks, SES, Step Functions 知識として知っておいたほうが良い Neptune, Storage Gateway, Direct Connect, Glacier, QuickSight, Data Pipeline Key Management Service(KMS), Snowball, Glue, Organization, Batch Service Catalog, System Manager, Trusted Advisor, SageMaker, Config Cloud Trail, Cost Explorer, GuardDuty, Cognito, Athena, Inspector WAF & Shield, Backup, CloudTrail, Lightsail, Directory Service 11. 参考文献 AWS 認定 – AWS クラウドコンピューティング認定プログラム AWS 認定ソリューションアーキテクト – アソシエイト AWS Well-Architected 【未経験でも挫折しない】40時間でAWS認定ソリューションアーキテクトアソシエイトを取得する方法 - Qiita 1.AWSは主にIaaS(Infrastructure as a Service)を提供しています。 ↩2.実際に実務経験の有無を確認されるわけではありません。あくまで実務経験が必要なレベルだということだと思います。AWS認定のQ & Aにも認定に際する前提条件はないことが書かれていました。実際に「AWS認定ソリューションアーキテクト - アソシエイト」を合格された方の中には実務経験の無い方やAWSを触ったことが無い方もおられるようです。 ↩3.Udemy（ユーデミー）は、世界で2,400万人が利用する世界最大級のオンライン動画学習サービスです。 ↩3.動画は5秒単位で飛ばすことも、戻ることもできます。 ↩4.Route53はWindowsサーバのDNSよりもBIND寄りの印象ですが、それをさらにシンプルにして使いやすくした感じです。さらにマネージドサービスでSLA100%を謳っています。すごい・・・ ↩5.動画でも説明されていますが、Route53でドメインを購入すると結構お高いです。お名前.comなら「.work」ドメインを1円で販売していました(2019年8月18日時点)。 ↩6.せっかくなので、自分もいくつかサーバレスアプリケーションを作って「 Serverless Application Repository」に公開してみたいと思います。いつになるかは分かりませんが・・・ ↩7.同じ分類内は順不同です。またこの手の分類に正解はないのでツッコミは不要です(笑)。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/08/20/aws-solution-architect-associate/"},{"title":"Pulumiでプログラマのための「Infrastructure as Code」を実践する","text":"「Infrastructure as Code (IaC)」という言葉が生まれてからしばらく経ちました。IaCは簡単に言えばインフラをコード化するという概念です。この言葉に触れた当時はインフラをプログラミングできる時代がやってくるのだと思い、プログラマとして非常に心が躍りました。しかし残念ながらその気持ちは長くは続きませんでした。Ansible, Chef, Puppet, CloudFormation, AWS SDK, Terraform・・・ これらの技術はどれも素晴らしいものだと思います。Docker Composeやkubectl applyには感動した記憶もあります。しかしプログラマとしての自分が告げるのです。何かが足りない・・・本当に欲しいのは「コレジャナイ」と。そして長い、長い旅路の末にようやく巡り会うことができました。Pulumiという希望の星に。 目次1. はじめに2. 実践 Infrastructure as Code2.1. Pulumiの導入2.2. pulumiログイン2.3. プロジェクトとスタックの作成2.4. index.tsの作成2.5. utils.tsの作成2.6. インフラのデプロイ2.7. デプロイ結果の確認2.8. リソースの後片付け2.9. コードの解説3. Pulumiのいいところ4. Pulumiの仕組み5. まとめ6. 参考文献 1. はじめに本記事ではPulumi1で「Infrastructure as Code」を実践します。具体的にはAWS上に以下の2層構造のWebアプリケーション2のインフラを100行未満のTypeScriptで記述します。 2. 実践 Infrastructure as CodePulumiではインフラの状態が内部で管理されているので、インフラを簡単に作ったり、壊したりすることができます。また、今回は静的型付き言語であるTypeScriptを選択したので、インフラの「型」を簡単に確認でき、IDEのサポートが受けやすいです。そしてTypeScriptは汎用言語でもあるのでインフラを関数やライブラリ化したり、インフラをループで大量に生成するのも簡単です。まさにプログラミング感覚でインフラが構築できて、いらなくなったら簡単に破棄できるのでプログラマのためのIaCを実践するのにPulumiはうってつけです。 2.1. Pulumiの導入PulumiのGet Startedに従って、Pulumi CLI、AWS CLI、Node.jsをインストールしてください。(「Configure AWS」まで進めてください。) 2.2. pulumiログイン以下のコマンドでpulumiにログインしてください。 1$ pulumi login ブラウザでログインする場合は上記のコマンド後に「Enter」キーを押します。するとブラウザ側でサインインできます。自分はGitHubでPulumiにサインインしましたが、他にもGitLabやE-mail等でもサインインできます3。 2.3. プロジェクトとスタックの作成次にプロジェクトとスタックの作成をします。以下のコマンドで実行します。 12$ mkdir aws-ts-twe-tier-web && cd aws-ts-twe-tier-web$ pulumi new aws-typescript pulumi newは基本的にはデフォルトでOKなのでEnterで進めますが以下の質問だけ「ap-northeast-1」にしてEnterを押してください。 aws:region: The AWS region to deploy into: (us-east-1) 2.4. index.tsの作成メインファイルであるindex.tsファイルに以下を記述してください。 index.ts12345678910111213141516import * as pulumi from \"@pulumi/pulumi\"import * as awsx from \"@pulumi/awsx\"import * as utils from \"./utils\"const vpcPrefix = \"custom\"const vpc = new awsx.ec2.Vpc(vpcPrefix)const db = utils.createRDSInstance(vpcPrefix, vpc)const alb = utils.createApplicationLoadBalancer(vpcPrefix, vpc)const targetGroup = alb.createTargetGroup(`${vpcPrefix}-web-tg`, { port: 80, targetType: \"instance\" })const listener = targetGroup.createListener(`${vpcPrefix}-web-listener`, { port: 80 })const autoScalingGroup = utils.createAutoScalingGroup(vpcPrefix, vpc, alb)autoScalingGroup.scaleToTrackAverageCPUUtilization(\"keepAround50Percent\", { targetValue: 50 })export const endpoint = listener.endpoint.hostname 2.5. utils.tsの作成同じフォルダにutils.tsを作成して、ファイルに以下を記述してください。コーディングは以上です。 utils.ts12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import * as aws from \"@pulumi/aws\"import * as awsx from \"@pulumi/awsx\"export function getAmazonLinux(): Promise { return aws.getAmi({ filters: [ { name: \"name\", values: [\"amzn-ami-hvm-*\"] }, { name: \"virtualization-type\", values: [\"hvm\"] }, { name: \"architecture\", values: [\"x86_64\"] }, { name: \"root-device-type\", values: [\"ebs\"] }, { name: \"block-device-mapping.volume-type\", values: [\"gp2\"] } ], mostRecent: true, owners: [\"amazon\"] }).then(ami => ami.id)}export function createRDSInstance(vpcPrefix: string, vpc: awsx.ec2.Vpc): aws.rds.Instance { const dbSg = new awsx.ec2.SecurityGroup(`${vpcPrefix}-db-sg`, { vpc, ingress: [{ protocol: \"tcp\", fromPort: 3306, toPort: 3306, cidrBlocks: [\"0.0.0.0/0\"] }], egress: [{ protocol: \"-1\", fromPort: 0, toPort: 0, cidrBlocks: [\"0.0.0.0/0\"] }], }) const dbSubnets = new aws.rds.SubnetGroup(`${vpcPrefix}-dbsubnets`, { subnetIds: vpc.privateSubnetIds, }) return new aws.rds.Instance(`${vpcPrefix}-db`, { engine: \"mysql\", instanceClass: \"db.t2.micro\", allocatedStorage: 10, dbSubnetGroupName: dbSubnets.id, vpcSecurityGroupIds: [dbSg.id], name: `${vpcPrefix}DbInstance`, username: \"testdb\", password: \"testdb123\", multiAz: true, skipFinalSnapshot: true, })}export function createApplicationLoadBalancer(vpcPrefix: string, vpc: awsx.ec2.Vpc): awsx.elasticloadbalancingv2.ApplicationLoadBalancer { const albSg = new awsx.ec2.SecurityGroup(`${vpcPrefix}-alb-sg`, { vpc, egress: [{ protocol: \"-1\", fromPort: 0, toPort: 0, cidrBlocks: [\"0.0.0.0/0\"] }], }) return new awsx.lb.ApplicationLoadBalancer(`${vpcPrefix}-web-traffic`, { vpc, securityGroups: [albSg] });}function getRunCmd(title: string, content: string): string { return `az=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)echo \"&lt;html&gt;&lt;head&gt;&lt;title&gt;${title}&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;${content} from $az&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\" > index.htmlnohup python -m SimpleHTTPServer 80 &`}export function createAutoScalingGroup(vpcPrefix: string, vpc: awsx.ec2.Vpc, alb: awsx.elasticloadbalancingv2.ApplicationLoadBalancer): awsx.autoscaling.AutoScalingGroup { const userDataLines = getRunCmd(\"My Web Site\", \"Hello World\").split(`\\n`).map(e => ({ contents: ` ${e}`, }) as awsx.autoscaling.UserDataLine) return new awsx.autoscaling.AutoScalingGroup(`${vpcPrefix}-web-asg`, { vpc, subnetIds: vpc.publicSubnets.map(e => e.id), targetGroups: alb.targetGroups, templateParameters: { minSize: 2, maxSize: 4, }, launchConfigurationArgs: { instanceType: \"t2.micro\", imageId: getAmazonLinux(), securityGroups: alb.securityGroups, userData: { extraRuncmdLines: () => userDataLines } } })} これで見事に冒頭で示した構成がindex.tsとutils.tsを合わせて100行弱のコードで作成できました。 2.6. インフラのデプロイ以下を実行してください。途中で本当に実行してよいか聞かれるので「yes」を選択してEnterを押してください。インフラの作成には数分かかります。 1$ pulumi up 2.7. デプロイ結果の確認pulumi upが成功すると最後の出力結果にendpointが表示されるので、そのURLにブラウザからアクセスしてみてください。 「Hello, World! from ap-northeast-1a」が表示されたら成功です。ロードバランサを挟んでいるのでリロードでするごとにAZの部分(from ap-northeast-1a)が変わります。また実際にAWSのコンソールにログインしてEC2やRDSやVPCの構成も確認してみてください4。 2.8. リソースの後片付け確認が終わったら以下のコマンドでリソースを破棄してください。リソースの破棄をしないとAWSの料金が発生し続けるので不要になったらすぐに破棄するようにしてください5。 1$ pulumi destroy 2.9. コードの解説短いのであまり解説する必要もないかもしれませんが、index.tsだけ一応簡単にコメントします。魔法はライブラリの「awsx」にあります。これは「Pulumi Crosswalk for AWS」というライブラリで、AWSのwell-architectedなベストプラクティスを実装しています。以下のコードでは「new awsx.ec2.Vpc(vpcPrefix)」が凄い仕事をしていて、二つのパブリックサブネットと二つのプライベートサブネットとインターネットゲートウェイ、NATゲートウェイやそれに付随するセキュリティグループなど様々なものを生成しています。それ以外はAWSの知識があれば割合素直に読めるのではないかと思います。 index.ts1234567891011121314import * as pulumi from \"@pulumi/pulumi\"import * as awsx from \"@pulumi/awsx\"import * as utils from \"./utils\"const vpcPrefix = \"custom\" // VPCの名前を設定const vpc = new awsx.ec2.Vpc(vpcPrefix) // VPCの作成(二つのパブリックサブネット、二つのプライベートサブネット、インターネットゲートウェイ、NATゲートウェイの作成)const db = utils.createRDSInstance(vpcPrefix, vpc) // RDSインスタンスの作成const alb = utils.createApplicationLoadBalancer(vpcPrefix, vpc) // アプリケーションロードバランサーの作成const targetGroup = alb.createTargetGroup(`${vpcPrefix}-web-tg`, { port: 80, targetType: \"instance\" }) // ターゲットグループの作成const listener = targetGroup.createListener(`${vpcPrefix}-web-listener`, { port: 80 }) // リスナーの作成const autoScalingGroup = utils.createAutoScalingGroup(vpcPrefix, vpc, alb) // オートスケーリンググループの作成autoScalingGroup.scaleToTrackAverageCPUUtilization(\"keepAround50Percent\", { targetValue: 50 }) // スケーリングポリシーの作成export const endpoint = listener.endpoint.hostname // \"endpoint\"の出力 util.tsも含めた全体の処理の流れは以下のとおりです。 VPCの作成 VPCの作成 ２つのパブリックサブネットの作成 NATゲートウェイの作成 ２つのプライベートサブネットの作成 NATゲートウェイの作成 インターネットゲートウェイの作成 必要なルートテーブルやセキュリティグループの作成 RDSインスタンスを作成 セキュリティグループの作成 ふたつのプライベートサブネットを対象にサブネットグループの作成 サブネットグループにRDSインスタンスの生成 アプリケーションロードバランサの作成 セキュリティグループの作成 アプリケーションロードバランサの生成 ターゲットグループの作成 ロードバランサ用のターゲットグループを作成 リスナーを作成 ロードバランサ用のリスナーを作成し、転送先に上記で作成したターゲットグループを設定 オートスケーリンググループの作成 起動設定の作成 最新のAmazonLinuxを検索してAMIのIDを取得 起動設定のユーザデータとして起動コマンドを渡してpythonのSimpleHttpServerが立ち上がるようにする パブリックサブネットとロードバランサを指定してオートスケーリンググループの生成 オートスケーリングは2から4インスタンスの幅に設定 スケーリングポリシーの設定 CPU使用率が50%を基準にスケーリングするように設定 “endpoint”の出力 注意すべきは、実際にPulumiが上記の流れどおりにリソースを作成しているわけではないということです。プログラマはあくまでリソースの依存関係だけを気にしてプログラムを作成すればよく、実際のリソースの作成はPulumiが依存関係を賢く判断して並列に作成できるリソースは並列に作成してくれます。 さらに言えば、Pulumiは上記のプログラムを「実行」して実際に必要なリソースを確定し、現在すでに存在するリソースとの差分を計算して差分のリソースだけをAWS側に作成してくれます。つまり一旦上記のコードをpulumi upで実行したあとにEC2インスタンスを作成するコードを付け足して再実行した場合には、差分であるEC2インスタンスの作成のみが行われるということです。これは例えるならPulumiはキャッシュ付きの自動並列化コンパイラのような役割を果たしていると考えられると思います。 一番最後の「”endpoint”の出力」は分かりづらいかもしれませんが、変数をexportしておくとpulumi upした最後の結果として変数の値を出力してくれます。また「pulumi stack output 」を実行することで変数の値を出力することができるので、外部のプログラムとの連携が容易になります。 3. PulumiのいいところPulumiのいいところは以下のとおりです。 マルチクラウド AWS Azure Google Cloud Platform Kubernetes OpenStack 複数の汎用言語をサポート Node.js - JavaScript, TypeScriptやその他のNode.js互換言語(JSに変換可能) Python 3 - Python 3.6 or greater Go(PREVIEW) インフラのリソースの状態を管理している pulumi upしたとき前回実行時からの差分のリソースだけを作成する pulumi destroyで作成済みのリソースを破棄する 複数のプロジェクト、スタックを使い分けられる スタックごとに変数を定義できる Secretの管理もできる 1つ目はマルチクラウドなところです。AWSやAzureやGCPのようなパブリッククラウドだけではなくOpenStackもサポートしています。またKubernetesのようなコンテナオーケストレーションもPulumiでコード化することができるので、Pulumiを覚えるだけで非常に広範囲のインフラ構築を自動化できることが分かると思います。 2つ目は複数の汎用言語をサポートしていることです。現在はJavaScriptやTypeScriptやPythonがサポートされており、Go言語も仲間入りする予定です。また拡張可能なように作られていて自分のお気に入りの言語を追加することも可能です6。 3つ目はインフラのリソースの状態をPulumi側で管理していることです。このことでプログラマは最終的にあるべき状態だけを意識するだけでインフラをプログラミングできます。もしこれが現在のリソース作成状況を意識しながらプログラムを考えなければ行けないとすると非常に大変です。インフラの最終状態だけを思い浮かべてロジックに集中できるということはプログラマにとってありがたいことです。 もう一つプログラマに取ってありがたいのはリソースの削除がpulumi destroyで簡単にできることです。プログラマは基本的にはトライアル&エラーでプログラムを作成することに慣れています。しかしリソースの破棄が面倒であれば試行錯誤する気にもならないかもしれません。実際のインフラではリソースに複雑な依存関係がついており順番を守らなければリソースの削除に失敗することもよくあります。しかし、Pulumiはリソースの状態を管理して、依存関係を把握することで、大量かつ複雑な依存関係のリソース群を一括で削除できます。 4つ目は複数のプロジェクトおよびスタックを使い分けられることです。プロジェクトは再利用の単位にもなっていて「自分が作成したインフラ」を簡単に公開して共有することができます。今回作成したコードも以下にGitHubで公開してあるので、ぜひ試してみてください。 👇 hinastory/aws-ts-two-tier-web: 2-tier web application hosting example for AWS by Pulumi また、プロジェクトの中でスタックを作成でき、コードの中で利用可能な「設定」を定義できます。例えばリージョン情報やユーザ名やパスワード等を「設定」としてコードから外出しすれば再利用性が高まり、スタックの切り替えで「設定」の切り替えもできるので非常に便利です。一番よくある使い方は開発用のスタックと本番用のスタックを分けるやり方です。その他にもパスワードを暗号化して管理する方法も提供されているので安全にインフラを共有することが可能です。 4. Pulumiの仕組みPulumiの仕組みは以下のとおりです。 *How Pulumi Worksより引用* まず言語ホストがTypeScript等のコードを実行して、その結果をPulumiのエンジンに伝えます。Pulumiのエンジンは最後のデプロイの情報を確認して、必要に応じてクラウド上のリソースの作成、更新、削除を行い、その結果をまたデプロイの最終結果として保存します。最終結果の保存先はデフォルトでは Pulumiのサービスになりますが、ローカルやクラウドストレージ上に保存することも可能です。 5. まとめ本記事ではPulumiを利用して100行未満のTypeScriptでAWS上に高可用な2層構造のWebアプリケーションのインフラを作成しました。Pulumiを利用した利点は以下のとおりであり、プログラマが「Infrastracture as Code」を実践するのに最適なツールだと感じました7。 マルチクラウド AWS, Azure, GCP, OpenStack, Kubernetes 汎用言語で記述できる 汎用言語の表現力や生産性を享受できる JSONやYAMLやその他DSLだと関数やロジックを記述する上での制約が大きく、少し複雑なことをしようとすると生産性が激減してプログラマの力を活かしきれない TypeScript等の静的型付け言語ではさらに型情報もあるので、IDE(VSCode等)から補完や定義の確認等のサポートを受けられる 状態を管理している リソースの差分だけを自動で更新してくれる リソースの依存関係を理解して並列でリソースの作成をするので処理が速い リソースの状態や依存関係を理解しているので関連リソースの安全な削除が可能 インフラのコードの再利用性のための枠組みがある 今回作成したコードは aws-ts-two-tier-web - GitHubで公開中 自分はずっとインフラに苦手意識を感じていました。その理由は失敗したら簡単には元に戻せないことと、物理的な制約により抽象化が難しいからです。プログラマとしての自分はこれらの理由によりずっとインフラは苦手なままだと思っていました。クラウドが現れて、多くが仮想化されてもまだ抽象化に難があり状態管理が面倒だと感じていました。しかし時は流れてようやくインフラに抱いていたコンプレックスが解消されつつあります。 汎用言語の表現力と生産性を身につけ、主要な「インフラ」をカバーし、インフラの煩わしい状態管理からの解放を告げたPulumiの出現により、プログラマのためのIaCがようやく登場したことを確信したのです。 もはやインフラが「ハード」と思われる時代は過ぎ去り、プログラマが柔軟に抽象化し、ライブラリ化し、複雑なインフラ構成をも再利用可能にしていく時代が到来しようとしています。そしてPulumiはその先頭を走るプロダクトであり、自分は多くのPulumiプロジェクトがネット上に公開され、多くのインフラコードのエコシステムが生まれてくることを願って止みません。 本記事がPulumiの普及とプログラマのためのIaCの一助になれば幸いです。 6. 参考文献 Pulumi Documentation Joe Duffy - Hello, Pulumi! Pulumi Advances DevOps on AWS - DevOps.com Pulumi Crosswalk Aims to Simplify Deploying to AWS - The New Stack Infrastructure as Code: Chef, Ansible, Puppet, or Terraform? | IBM Infrastructure as Code - Wikipedia 私は Infrastructure as Code をわかっていなかった - メソッド屋のブログ まだTerraform使ってるの？未来はPulumiだよ | apps-gcp.com Terraform と Pulumiを比較する | apps-gcp.com これが次世代プロビジョニングツールの実力か！？ PulumiでAWSリソースを作成してみた ｜ DevelopersIO Pulumi で AWS Application Load Balancer を構築する - Qiita 昨今話題?の Pulumi を使ってみた - Qiita pulumiのチュートリアルをやってみた - Qiita pulumiのapplyを理解する - Qiita Pulumiの状態管理にクラウドストレージバックエンドを使う - Qiita Pulumi+VSCodeの書き心地が抜群な件 - Qiita PulumiのProviderをTerraformのProviderから実際に作成してみた - Qiita 1.PulumiはモダンなInfrastructure as Codeを実践するためのプロダクトであり、OSSです。そして、それを開発、サポートする会社の名前でもあります。Pulumiの由来はハワイ語の箒(ほうき)になります。もしくはPulumiの創業者でありCEOでもあるJoe Duffyの親友でもあった「Chris Brumme(故人)」の名前の誤発音です。詳細はJoe Duffy's Blogを参照してください。 ↩2.一般的にはAWSのリファレンスアーキテクチャにある通り、Webサーバとアプリケーションサーバを分離した構成をとることが多いです。またDNSであるRoute53やCDNのCloudFrontもWebサーバの構成として入れるのが妥当ですが、今回はなるべく応用可能な「基礎」を提示したかったのでこの構成にしました。 ↩3.Pulumiにサインインせずにローカル\bだけで完結させる方法もあります。 ↩4.AWSのマネジメントコンソール側でPulumiで作成したリソースの変更をしないでください。Pulumi側が管理している状態とAWSの状態がずれるとこの後説明するリソースの後片付けで失敗する可能性があります。 ↩5.特にRDSのマルチAZ構成はお高いので注意してください。 ↩6.言語エンジンは外部プロセスと実行されgRPCを介してPulumiエンジンやリソースプロバイダと通信します。つまり通信プロトコルさえ守れば、Pulumi本体に手を入れることなく簡単に別の言語サポートを追加できるということです。 ↩7.Pulumiが他のIaCツールと何が違うのかを詳しく知りたい方は「Pulumi vs. Other Solutions」を参照してください。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/08/26/infrastructure-as-code-in-pulumi/"},{"title":"Serverless Days Tokyo 2019 に参加してきました","text":"Serverless Days Tokyo 2019に初参加してきました。サーバーレスの息吹を感じられる素晴らしいカンファレンスでした。 ServerlessDays TokyoServerlessDays Tokyo 2019https://tokyo.serverlessdays.io/A community based conference focusing on Serverless based platforms and technologies. Tokyo, October, 2019 目次1. ServerlessDays Tokyoとは2. 会場について3. 午前のセッション3.1. 10x Serverless Product Development for a Startup with Microsoft Azure @ Yutaka Tachibana(EBILAB)3.2. Keynote @ Keisuke Nishitani (AWS)3.3. グローバル展開のコネクティッドカーを支える大規模サーバーレスシステム事例 @ Yuya Urayama (TOYOTA), Takanori Suzuki (Acroquest Technology) and Eiichiro Uchiumi (AWS)3.4. 昼食の様子4. 午後のセッション4.1. All You Need Is JavaScript @ Jensen Hussey / Cloudflare4.2. Zero Scale Abstraction in Knative Serving @ Tsubasa Nagasawa (CyberAgent)4.3. 空調設備向けIoTシステムにおけるクラウドランニングコスト @ 野原 健太 / ダイキン工業株式会社4.4. ISPがサーバレスに手を出した @ 伊藤良哉 & 松田丈司 (NTTコミュニケーションズ)4.5. ドーナツのデプロイの様子4.6. AWS Lake Formation で実現、マイクロサービスのサーバーレスな分散トレーシング @ 江藤武司 & 岩井良和 (Sony Corporation)4.7. Don’t think Serverless Security, think Application Security @ Ido Neeman (Nuweba)4.8. Azure でサーバーレス、 Infrastructure as Code どうしてますか？ @ Kazumi Ohira4.9. The hidden cost and technical debt of running huge Serverless service on production @ James Nguyen / MaaS Global5. スポンサーセッション & Lightning Talk5.1. Oracle も Serverless サービスやっています @ Sugiyama Suguru (スポンサーセッション)5.2. 「サーバーレス」な同人誌の紹介 @ めもおきば(nekoruri) (Lightning Talk)5.3. 目つぶり検証作成期 @ Kana Kitagawa (Lightning Talk)5.4. SERVERLELESSなエンジニアのためのSERVERLESSなオンラインサロンをVUE/NUXT/COGNITO/STRIPEで作った話 @藤本竜之介 (Lightning Talk)6. 懇親会の様子7. まとめ8. おまけ9. 参考文献 1. ServerlessDays TokyoとはServerlessDays Tokyoは、サーバーレス1に関する技術を扱う技術者のためのカンファレンスです。2016年が第一回で今年で4回目ですが、ServerlessDays Tokyoになったのは今年からで去年まではserverlessconf Tokyoという名称で行われていました2。カンファレンスの規模は400名弱であり、1トラック形式です。コーヒーとドーナツがデプロイされており、スポンサーブースを全て回って缶バッジを集めるとイベントTシャツが貰えるなど色々と工夫されたカンファレンスだと感じました。 2. 会場について会場はもともと印刷工場だったものを商業施設に転用した「TABLOID」で行われました。元が印刷工場だけあって独特の味がある建物で「ものづくり」を意識させる工場の造りはエンジニアの魂に訴えかけるものがあり、今回のカンファレンスにはうってつけの会場でした。 日時 10/21(月) ワークショップ、10/22(火・祝) カンファレンス 場所 TABLOID 参加人数 370名(10/22) 3. 午前のセッション会場には8:40頃に到着しました。イベント開始の20分前です。参加者用のリストバンドを渡されたのでそれを手首に巻いてセッションが始まるのを静かに待ちました。 3.1. 10x Serverless Product Development for a Startup with Microsoft Azure @ Yutaka Tachibana(EBILAB)一発目のセッションはゑびやさんのMicrosoft Azureを活用したサーバーレスの事例でした。ゑびやさんはもともとは伊勢の食堂をやっている会社でしたが、社長が変わったのを機にITを経営に取り入れ始めて、たった数年でITソリューションを外販するようになった驚きの企業です。 プロダクトはTOUCH POINT BIで、実店舗経営のためのBIツールでその開発ノウハウの紹介でした。BIツールに関しては一から作り込むのではなく、MicrosoftのPower BIを利用してBI部分をiframeを利用して埋め込み、メニュー部分をNuxt.jsやLaravelを利用して作り込んでいる点です。これによってBI部分の作成にエンジニアは必要なくなり、実際にゑびやさんではカスタマーサクセスチームが作成を行っているそうです。 また全体的にAzureとサーバーレスを全面活用しているのもポイントだと思いました。特にAzure Functionsの利用においてはPythonをGAになる前から利用しており、必要に応じてFunctionをランサーズに外注している話は関心を持ちました。もともと機能の外注は使用のすり合わせでいろいろと面倒な面がありますが、Azure Functionsの制約のもとでの依頼であれば非機能面を含めての仕様の要求、提示がしやすくお互い楽に交渉できそうです。またTOUCH POINT BIはAI活用もAI専門の会社と協力して開発しておりAzure Functionsが役にたったそうです。 結果として本セッションのタイトルにある「10倍の生産性」はAzureとAzure Functionを使って開発全体を疎結合にして、外注できる部分はどんどん外に出して自社のエンジニアは外に出せない経営分析や全体のアーキテクチャに専念することでスピード感のある開発で実現していました。一番印象に残った言葉は「なるべくエンジニアがボトルネックにならないように」であり、まさしくサーバーレスの開発に必要な考え方だと思いました。 3.2. Keynote @ Keisuke Nishitani (AWS)2番目のセッションはAWSさんでした。発表内容はAWSのLambdaの歴史とLambdaのネットワーキングについてとモダンアプリケーションの開発についてでした。Lambdaの歴史に関しては、LambdaはもともとはEvent Drivenなアーキテクチャを実現するためのサービスとして発表されたそうです。今はすっかりサーバーレスの代名詞となったLambdaもRe:Inventを重ねるごとにパワーアップしているのが分かって面白かったです。 Lambdaのネットワーキングについては、LambdaはAWSサービスチームのVPCで稼働しており、ユーザのVPCにつなぐ際には課題があるという内容でした。具体的にはユーザのVPCにつなぐ際にENI(Elastic Netowrk Interface)の作成に時間がかかり、またENIというリソース消費も問題になるという内容でした。しかし、この問題は「AWS Hyperplane」を用いてつい最近大幅に改善されたそうです3。 モダンアプリケーションに関しては、イベントドリブンな開発手法でメッセージングを通してステートを取り除くことが重要だという話には納得しました。パワーワードとして「all you need is code」、「全てはサーバーレスになる」が印象に残りました。 3.3. グローバル展開のコネクティッドカーを支える大規模サーバーレスシステム事例 @ Yuya Urayama (TOYOTA), Takanori Suzuki (Acroquest Technology) and Eiichiro Uchiumi (AWS)本イベントで一番気になっていたTOYOTAさんのセッションです。TOYOTAと言えばカイゼンが文化なのである意味サーバーレスの選択には納得しました。サーバーレスは無駄を低減し、無駄をマネージドする手法としては非常に合理的だからです。事例に関しては「コネクティッドカー」がテーマで、車、ドライバー、保険事業者、環境庁をIoTで結ぶものです。 ただこのプロジェクトが初めてのアジャイル、初めてのサーバーレスで若手主体で行われたというのは驚きました。そしてAWSが直接サポートというのも驚愕で、こういうユーザとクラウドベンダーが組んで、ITベンダーが「中抜き」される案件が今後も増えてくることを予想される案件でした。 面白かったのはアーキテクチャスタイルの解説とテスト関連です。アーキテクチャスタイルに関しては、「Nティアー」と「ウェブキュー・ワーカー/イベント・ドリブン」、「マイクロサービス」の３パターンが主にあり、トヨタはいずれも使っているとのことでした。また、サービスやリクエストの性質に合わせてリアクティブなスケーリング(Lambda)とプロアクティブなスケーリングを組み合わせて使うのも面白いと思いました。基本的にはリアクティブなスケーリングを使い、それでは要件を満たせない場合にプロアクティブなスケーリングを検討します。プロアクティブなスケーリングでは事前にリクエストが大量に来ると分かった時点でコンテナを大量に立ててあらかじめスケールしておく等の手法をとります。 サーバーレスのテストのしずらさにも言及があり、LocalStackを使ってモックの作成を行ったり、Karateで連携テストを行っていると言う話が印象に残りました。サーバーレスは非同期な処理が多いので連携テストは難しいと感じていましたがきちんとツールを使って連携テストを実施している事例は非常に参考になりました。 3.4. 昼食の様子昼食の場所は同会場のカフェでビュッフェ形式で提供され、イベントに来られていた同じ会社の方とご一緒しました。下の写真のように洒落たカフェのような雰囲気で美味しく頂きました。 4. 午後のセッション自分の場合昼食をとった後は強烈な眠気に襲われるので、ある意味ここからが本番です・・・ 4.1. All You Need Is JavaScript @ Jensen Hussey / CloudflareCDN(コンテンツデリバリーネットワーク)の事業者のCloudflareさんの発表です。CDNと言えばAkamaiとAmazon CloudFlontしか知らなかったので、Cloudflareの発表には非常に興味がありました。発表内容に関しては、独自FaaSのCloudflare Workersについてでした。 Cloudflare Workersの特徴的なところは、FaaSの利用言語をJavaScript固定にしているところです。Lambmda等の他のFaaSではコンテナベースで複数の言語をさぽーとしているところが多かったので以外でした。もちろんJavaScriptに固定しているのには十分な理由があって、JavaScriptは世界で最も使われており、JavaScriptのVMであるChrome V8は性能がよく、多くのテストがされ、WebAssemblyをサポートしているからとのことでした。あとコンテナと比較しての一番の利点はコールドスタートが非常に速いということです。具体的にはWorkersの実装はV8のIsolateインスタンスになっていて、数値は忘れてしまいましたが「コールドスタート」というより「ホットスタンバイ」といっても過言でないほどの速さでした。 4.2. Zero Scale Abstraction in Knative Serving @ Tsubasa Nagasawa (CyberAgent)CyberAgentさんの発表は「ゼロスケール」をKnativeで以下に実現しているかでした。KnativeはFaaSやPaaSをKubernetes上で実現するためのOSSですが、「ゼロスケール」という言葉は耳慣れないかもしれません。 ゼロスケールは例えばコンテナで言うと最初はコンテナ数が0から始まり、要求が来ると自動的にスケールアウトしてコンテナ数を増やし、要求が少なくなると自動的にコンテナ数を0まで減らす仕組みです。概念としては単純ですが実装するとなると大変です。基本的にはイベントをキューでためておいて、ローリングアップデート時のルーティングを工夫してスケーリングを実現しているようでした。 Knativeを使うと複雑なYamlを書かなくても簡単にサーバーレスアプリケーションが書けそうなので試してみたいと思いました。 4.3. 空調設備向けIoTシステムにおけるクラウドランニングコスト @ 野原 健太 / ダイキン工業株式会社ダイキンの事例はIoTをサーバーレスで行う事例でした。もともとIoTは需要が読めないのでサーバーレスと相性が良いというのは納得です。このセッションで面白かったのはサーバーレスのランニングコストに言及していた点です。 一般的にはサーバーレスは従量課金がメインなのでコスト最適化の余地は少ないのですが、DynamoDBのコスト最適化という視点は今まであまりなかったので斬新でした。具体的にはDynamoDBはWCU(書き込みキャパシティ単位)の料金が高いので、必要な分だけ個別書き込みできるようにすることと、必要な情報を一括で読み込めるようにキーを工夫する必要があるという内容でした。 余談としてEC2インスタンスを停止し忘れと、ログの出しすぎでCloudWatch Logsの料金が跳ね上がったという、いわゆるクラウド破産に気をつけようという話もされました。 4.4. ISPがサーバレスに手を出した @ 伊藤良哉 & 松田丈司 (NTTコミュニケーションズ)NTTコミュニケーションズさんの話は少し特殊な部類でISP(インターネットサービスプロバイダー)がサーバーレスに取り組んだお話でした。 面白いのはNTTコミュニケーションズはパブリッククラウドの「Cloudn」や企業向けの「Enterprise Cloud」をすでに持っているのにAWSを使うという点です。曰く「自社クラウドがあるとAWSが使いづらい」そうで、なんと今回のプロジェクトが初AWSで与信審査まで経験できたそうです(笑)。 技術的にはISPらしくIPv6に取り組んだ話とか、信頼性を担保するためにマルチクラウド（AWSとAzure）にした話とかいろいろと興味深い話が聞けました。肝心のサーバーレスに関してはserverless framework poweredを利用しており、テストにはScala製のGatlingを使っているとのことでした。Gatlingは自分も愛用しており非常に簡単にテストが書けてレポートも充実しているのでオススメです。 4.5. ドーナツのデプロイの様子会場にはコーヒーとドーナツが大量にデプロイされており、美味しくいただきました。 😄 😋😋😋😋😋😋😋ド ー ナ ッ ツ再 デ プ ロ イ のお 知 ら せ 🍩😋😋😋😋😋🎃😋#serverlessdays#serverlessdtokyo pic.twitter.com/ZGz1QHr2Zq— Serverless(JP) (@serverlessjp) October 22, 2019 4.6. AWS Lake Formation で実現、マイクロサービスのサーバーレスな分散トレーシング @ 江藤武司 & 岩井良和 (Sony Corporation)Sonyさんの発表は分散トレーシングの話でした。AWSで分散トレーシングといえばAWS X-Rayですが、X-Rayは非同期のトレーシングには向いておらず、ログの長期保存や複数アカウントのログ収集といった課題もあったのでそれをいかに解決したかという話でした。 具体的にはAWS Lake Formationを利用したという話です。データレイクの概念は知っていましたが、AWS Lake Formationは使ったことがなかったので興味深い内容でした。面白かったのは非同期のトランザクションが多い中でどうやってトレースIDを伝播させるかという内容でした。具体的にはサービスごとに異なり、例えばAPI GatewayｈではCustom HTTP Headerを用い、SNS/SQSではmessage attributeを使うといった具合です。さすがのSonyで最新のサービスと難しい課題に正面から取り組んだ素晴らしい内容でした。 4.7. Don’t think Serverless Security, think Application Security @ Ido Neeman (Nuweba) PDF Nuwebaさんの発表はサーバーレスのセキュリティについてです。サーバーレスというとセキュリティに漠然とした不安を抱える人も多いけど、基本的には関数は短期間しか実行されないし、異常を見つけやすく制限を適用しやすいので安全というものでした。 確かにサーバーレスは個人で管理する部分が少なく、サーバーの管理自体をクラウド事業者にオフロードしているので普通のサーバー運用が必要なアプリケーションと比較した場合にセキュリティ的に安全というのも分かります。なのでサーバーレス固有のセキュリティ問題は特にないから、アプリケーションのセキュリティに集中しようというのが本セッションの趣旨でした。 4.8. Azure でサーバーレス、 Infrastructure as Code どうしてますか？ @ Kazumi Ohira大平さんの発表はAzureでInfrastructure as Code(IaC)をどのように行うかといった内容でした。IaCはインフラのリソース構成の構成・管理をコードで行うことです。このことによりインフラの自動化やバージョン管理やレビューのしやすさが向上したりなど様々なメリットがあります。 Azure でサーバーレス、 Infrastructure as Code どうしてますか？ from Kazumi OHIRA 基本的な内容はAzureなのでARM Templateでした。ARM(Azure Resource Manager)は冪等性の管理や、リソースの差分デプロイ、並列デプロイができて非常に使い勝手が良さそうです。また日本語ドキュメントが充実しておりVisual Studio Codeの拡張機能が便利なので実用的です。ただ「すでにお使いなら、断然Terraform」だそうです(笑)。しかし、ここは個人的にはPulumiを押しておきたい・・・ 4.9. The hidden cost and technical debt of running huge Serverless service on production @ James Nguyen / MaaS Globalこのセッションは大規模なサーバーレスのシステムにおける隠れたコストと技術的負債の話でした。まずクラウドだから障害が起こらないというのは間違いで、AWSでも何回も障害が起きているので障害への備えは当然必要になるということです。また技術的負債の話でいくと、クラウドだと最新のOSSが利用できるまで待たされるかもしれないという話とサービスの進化に追従しなければ行けないという点から技術的負債が生まれるという話も興味深かったです。 5. スポンサーセッション & Lightning Talkここからは気になったスポンサーセッションとLightningTarkの内容になります 5.1. Oracle も Serverless サービスやっています @ Sugiyama Suguru (スポンサーセッション)OracleさんのスポンサーセッションはOracleのFaaSであるOracle Fanctionsの内容でした。 Oracle も Serverless サービスやっています from SuguruSugiyama Oracle Fanctionsは、OracleがOSSとして公開しているFn ProjectというOSSベースがベースでオンプレでもクラウドでも動くそうです。実装はコンテナベースで関数の公開はDockerイメージをアップロードして行います。 5.2. 「サーバーレス」な同人誌の紹介 @ めもおきば(nekoruri) (Lightning Talk)Lightning Talkで一番気になったのは、めもおきば(nekoruri)さんの、マニアックな同人誌紹介です。個人的には「SERVERLESSを支える技術」を含め以前に何冊か購入させて頂いたので、間違いなく紹介される本は良本だと確信が持てました。 #ServerlessDays Tokyo 2019 「サーバーレス」な同人誌の紹介 from Masahiro NAKAYAMA 「サーバーレス」な同人誌を #ServerlessDays で紹介しました - めもおきば とりあえず、すでに何冊か持っている本もありましたが、気になる本もたくさんあったので後で購入を検討しようと思います。 持っている本 Knativeソースコードリーディング入門 雰囲気でOAuth2.0を使っているエンジニアがOAuth2.0を整理して、理解できる本 Pragmatic Terraform on AWS 欲しい本 Goで学ぶGoogleCloud Functionsの使い方 ゼロから始めるNetlify OAuth、OAuth認証、OpenID Connectの違いを整理して、理解できる本 Microservices architecture よろず本 その三（初版は持っていたが３まで出ていたのか・・・） NATSによるPub/Subメッセージング入門 5.3. 目つぶり検証作成期 @ Kana Kitagawa (Lightning Talk)大学4年生で写真専門の方の発表です。プログラミングの初心者がServerless FrameworkとAmazon Recognitionを使って目つぶり検証を行ったという内容です。普通にすごいというかこういうことにチャレンジできるいい時代になったなと思いました。あと、まとめで「公式ドキュメントが読むこと」という内容があったので、この学びは素晴らしいと思いました。ブログや記事は興味を持つきっかけにはいいですが、手を動かす際に参照するのは圧倒的に公式ドキュメントが正義です。 5.4. SERVERLELESSなエンジニアのためのSERVERLESSなオンラインサロンをVUE/NUXT/COGNITO/STRIPEで作った話 @藤本竜之介 (Lightning Talk)オンラインサービスをSERVERLELESSで立ち上げた話でした。だいたい知っている技術でしたが、STRIPEだけ唯一知りませんでした。STRIPEはインターネットビジネスのためのオンライン決済処理だそうです。 6. 懇親会の様子懇親会も大分盛り上がりました。料理もとても美味しかったです。 🍻🍻🍻#serverlessdays#serverlesstokyo pic.twitter.com/a9FNYnq7pP— Serverless(JP) (@serverlessjp) October 22, 2019 7. まとめ今回ServerlessDays Tokyoは初参加でしたが、Serverlessの息吹を感じられ学びしかない一日でした。またサーバーレスはよりクラウドを使いやすくし、ユーザとクラウド事業者を直接結びつける効果もあり、今後ITベンダーはうかうかしていられないということも感じました。「全てがサーバーレスになる」、この言葉を胸にに今後のことを考えて行きたいと思います。次回は12/14に福岡でやるそうなので、興味がある方はオススメです！ 関係者、参加者の皆様、本当にお疲れさまでした。本記事がサーバーレスに興味がある方の一助になれば幸いです。 #ServerlessDays #serverlesstokyo おつかれさまでした！ pic.twitter.com/rxmlKB3sAM— 真吾 / Shingo (@yoshidashingo) October 22, 2019 8. おまけ入手したものです。目玉はスポンサーブースを全て回って缶バッジを手に入れると貰えるイベントTシャツ(黄色の方)です。 9. 参考文献 ServerlessDays Tokyo 2019 ServerlessDays Tokyo / Fukuoka 2019 を開催します - yoshidashingo ServerlessDays Tokyo 2019を開催しました - yoshidashingo ServerlessDays Tokyoを終えて振り返りというか単なる感想です。 | 技術的な何か。 ServerlessDays Tokyo 2019 - Togetter [発表] Lambda 関数が VPC 環境で改善されます | Amazon Web Services ブログ Serverless Days Tokyo 2019 トヨタ自動車・ダイキン工業 事例覚書 - Qiita1.ここで言う「サーバーレス」とは、サーバーレスアーキテクチャを基軸にしたパラダイムを指しています。サーバーレスアーキテクチャはイベントを契機に「関数」を実行するアーキテクチャで、「関数」を実行するためのリソースはインフラストラクチャが自動的に割り当てることを前提としています。つまり、「関数」側はインフラ(サーバ)を意識しない設計が可能なので「サーバーレス」という命名となっています。Function as a Service(FaaS)も「サーバーレス」と同じ意味合いで使われています。 ↩2.変更理由に関してはこちらのブログに詳細が書いてあったので興味がある方は参考にしてください。 ↩3.この記事の内容のことを話されていました。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/10/27/serverless-days-tokyo-2019/"},{"title":"MinikubeとTensorFlow 2.0でGPUを使ってCIFAR-10","text":"LinuxでGPU環境を構築して暫く経ちました。今回いろいろ古くなった環境を再構築する機会があったので、Linuxにおける機械学習の環境構築とCIFAR-10でディープラーニングに至る道程について簡単に記録に残しておくことにしました。 目次1. 対象読者2. マシンの準備3. OSのインストール4. NVIDIAドライバのインストール5. Docker Engine for Community のインストール6. NVIDIA Container Toolkitのインストール7. kubectlのインストール8. Minikubeのインストール9. Dockerイメージの作成10. デプロイメントの作成11. サービスの作成12. JupyterLabでノートブックを作成13. TensorFlow 2.0 with Kerasで画像分類(CIFAR-10)14. 実行結果15. まとめ16. 参考文献17. おまけ 1. 対象読者本記事は、LinuxでGPUを用いた機械学習の環境を構築してみたい方を対象にしています。また、MLOpsに興味があり、機械学習基盤の構築に興味がある方にもおすすめします。本記事では実際に機械学習の環境を構築してディープラーニングで画像分類をおこなうところまでの手順を説明します。その際以下の知識があったほうがより深く理解が出来ますが、本記事を読むのに必須ではありません。 Kubernetes JupyterLab TensorFlow Keras CNN(Convolutional Neural Network) 2. マシンの準備まずは機械学習用のマシンを調達します。マシンはLinuxが動作してNVIDIAのグラフィックボードを認識できれば問題ないです。グラフィックボードはなるべく新しいものの方が計算速度が速いのでいいと思います1。自分は「NVIDIA GeForce GTX1080 Ti」を利用しています。 3. OSのインストールUbuntu 18.04を利用します。以下からダウンロードしてインストールします。 Ubuntu 18.04 LTS 日本語 Remix リリース | Ubuntu Japanese Team 4. NVIDIAドライバのインストール「ソフトウェアとアップデート」を起動して「追加のドライバー」タブから「プロプライエタリ、検証済み」のドライバを選択して「変更の適用」をします。適用後に再起動してください。 nvidia-smiコマンドが利用できて以下のような感じになればOKです。 123456789101112131415161718192021$ nvidia-smiSun Oct 20 09:55:26 2019+-----------------------------------------------------------------------------+| NVIDIA-SMI 430.26 Driver Version: 430.26 CUDA Version: 10.2 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A || 25% 36C P8 13W / 250W | 261MiB / 11175MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1670 G /usr/lib/xorg/Xorg 18MiB || 0 1704 G /usr/bin/gnome-shell 49MiB || 0 3918 G /usr/lib/xorg/Xorg 87MiB || 0 4031 G /usr/bin/gnome-shell 103MiB |+-----------------------------------------------------------------------------+ 5. Docker Engine for Community のインストール以下の手順どおりに実行します。 Get Docker Engine - Community for Ubuntu | Docker Documentation 以下はコマンドの抜粋です。 123456789101112131415$ sudo apt-get remove docker docker-engine docker.io containerd runc$ sudo apt-get update$ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io インストール後は「docker version」のコマンドで確認します。 12345678910111213141516171819202122232425262728$ sudo docker versionClient: Docker Engine - Community Version: 19.03.3 API version: 1.40 Go version: go1.12.10 Git commit: a872fc2f86 Built: Tue Oct 8 00:59:59 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.3 API version: 1.40 (minimum version 1.12) Go version: go1.12.10 Git commit: a872fc2f86 Built: Tue Oct 8 00:58:31 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 nvidia: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683 6. NVIDIA Container Toolkitのインストール以下に従って行います。 NVIDIA/nvidia-docker: Build and run Docker containers leveraging NVIDIA GPUs 12345$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$ sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit デフォルトのコンテナランタイムを変更するため/etc/docker/daemon.jsonに以下を記載します。 123456789{ \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }} dockerを再起動します。 1$ sudo systemctl restart docker 以下のコマンドでdockerからGPUが見えているか確認します。 12345678910111213141516$ docker run nvidia/cuda:10.0-base nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 430.26 Driver Version: 430.26 CUDA Version: 10.2 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A || 25% 37C P8 13W / 250W | 262MiB / 11175MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|+-----------------------------------------------------------------------------+ 7. kubectlのインストール以下の手順でインストールします。 Install and Set Up kubectl - Kubernetes 1234$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.0/bin/linux/amd64/kubectl$ chmod +x ./kubectl$ sudo mv ./kubectl /usr/local/bin/kubectl 8. Minikubeのインストール以下に従って行います。 Minikubeのインストール - Kubernetes 12$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube$ sudo cp minikube /usr/local/bin && rm minikube 以下のコマンドでMinikubeを実行します。--vm-driver noneがポイントでこれをつけるとホスト上のDockerでKubernetesが実行されます。 1sudo -E minikube start --vm-driver none 9. Dockerイメージの作成まずは機械学習用のDockerfileを作成します。Dockerfileは以下のとおりです。 Dockerfile1234567891011121314151617181920212223242526272829303132333435363738394041424344FROM nvidia/cuda:10.0-cudnn7-develWORKDIR /ENV PYENV_ROOT /.pyenvENV PATH $PYENV_ROOT/shims:$PYENV_ROOT/bin:$PATHRUN apt-get update && apt-get install -y \\ curl \\ git \\ unzip \\ imagemagick \\ bzip2 \\ graphviz \\ vim \\ tree \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/*RUN git clone git://github.com/yyuu/pyenv.git .pyenvRUN pyenv install anaconda3-2019.10RUN pyenv global anaconda3-2019.10RUN pyenv rehashRUN pip install tensorflow-gpu==2.0.0 gymRUN conda install -c conda-forge jupyterlabRUN conda install -c anaconda pandas-datareaderRUN conda install -c anaconda py-xgboostRUN conda install -c anaconda graphvizRUN conda install -c anaconda h5pyRUN conda install -c conda-forge tqdmRUN mkdir /jupyterWORKDIR /jupyterENV HOME /jupyterENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64ENV SHELL /bin/bashEXPOSE 8888ENTRYPOINT [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--no-browser\", \"--allow-root\", \"--NotebookApp.token=''\"] ポイントは以下のとおりです。 ベースイメージとしてnvidia/cuda:10.0-cudnn7-develを指定 TensorFlow 2.0が動作可能なCUDAとcuDNNを指定 pyenvでAnacondaをインストール Anacondaでデータサイエンスに必要なパッケージの全部入りをざっくり入れる minicondaで細かく指定して入れる方法もある condaでJupyterLabをインストール pipでtensorflow-gpu(2.0.0)をインストール まだAnacondaに最新パッケージがなかったのでpipでインストール LD_LIBRARY_PATHにCUPTIのライブラリのパスを指定 TensorFlowで利用されるのでパスを通しておく ENTRYPOINTでJupyterLabが起動するように指定する 次に以下のコマンドでイメージのビルドを行います。 1docker build -t ml/all:v0.1 . 以下のコマンドでイメージが作成できているかどうか確認します。 123$ docker image ls ml/allREPOSITORY TAG IMAGE ID CREATED SIZEml/all v0.1 e23280bc3856 18 hours ago 11.3GB 10. デプロイメントの作成以下のようなyamlファイルを作成し、上記で作成したイメージをMinikubeにデプロイします。 ml-deploy.yaml1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: ml-deployment labels: app: ml-deployspec: replicas: 1 selector: matchLabels: app: ml-deploy template: metadata: annotations: kubernetes.io/change-cause: \"modified at 2019-10-20 05:50:40 +0900\" labels: app: ml-deploy spec: containers: - name: ml-deploy image: ml/all:v1.0 ports: - containerPort: 8888 volumeMounts: - name: notebook mountPath: /jupyter imagePullPolicy: IfNotPresent volumes: - name: notebook hostPath: path: /home/jupyter/ml-all type: Directory 以下のコマンドでデプロイメントの作成を行います。 1kubectl apply -f ml-deploy.yml 以下のコマンドでデプロイメントが作成されていることを確認します。 123$ kubectl get deploymentsNAME READY UP-TO-DATE AVAILABLE AGEml-deployment 1/1 1 1 36h 11. サービスの作成以下のようなyamlファイルを作成し、上記で作成したイメージをデプロイメントをサービスとして公開します。 ml-svc.yaml12345678910111213kind: ServiceapiVersion: v1metadata: name: ml-svcspec: selector: app: ml-deploy ports: - protocol: TCP port: 8888 targetPort: 8888 nodePort: 30001 type: NodePort 以下のコマンドでサービスの作成を行います。 1$ kubectl apply -f ml-svc.yml 以下のコマンドでサービスが作成されていることを確認します。 1234$ kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 443/TCP 37hml-svc NodePort 10.103.99.171 8888:30001/TCP 36h サービスの作成はkubectlのexposeコマンドを用いても行うことができます。ここまででようやく機械学習の環境が整いました。 12. JupyterLabでノートブックを作成ブラウザで以下のアドレスにアクセスします。 1http://(minikubeが起動しているマシンのIPアドレス):30001/ +ボタンを押して、Lancherを起動し、Python3のノートブックを作成します。 最初は「Untitled.ipynb」というファイル名で作成されますが、ファイルを右クリックで「Rename」を選択してファイル名を変更できます。今回は「cifar10.ipynb」に変更します。 13. TensorFlow 2.0 with Kerasで画像分類(CIFAR-10)ようやくお待ちかねのディープラーニングのターンです。今回はようやく最近正式リリースされたTensorFlow 2.0に密に統合されたKerasのAPIを利用してCIFAR-10の画像セットを用いて画像分類を行います。 ソースコードはkeras/cifar10_cnn.pyをベースにtensorflow対応や可視化表示のコードを加えたものになります。先程作成したノートブックに貼り付けて実行してください。適当にセルに分割して実行したほうが良いと思います。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import tensorflow as tfimport numpy as npimport os# GPUをオフにする場合# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"from tensorflow import kerasfrom tensorflow.keras.datasets import cifar10from tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flattenfrom tensorflow.keras.layers import Conv2D, MaxPooling2Dimport matplotlibmatplotlib.use(\"Agg\")import matplotlib.pyplot as plt%matplotlib inline# GPUのメモリを必要な量だけ使うようにするphysical_devices = tf.config.experimental.list_physical_devices('GPU')[0]tf.config.experimental.set_memory_growth(physical_devices, True)# パラメータの宣言batch_size = 32num_classes = 10epochs = 15num_predictions = 20save_dir = os.path.join(os.getcwd(), 'saved_models')model_name = 'keras_cifar10_trained_model.h5'# CIFAR-10のデータロード(x_train, y_train), (x_test, y_test) = cifar10.load_data()print('x_train shape:', x_train.shape)print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')# 画像の表示LABELS = ('airplane', 'mobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck')def to_label(v): idx = np.argmax(v) if idx < len(LABELS): return LABELS[idx] else: return Noneplt.clf()for i in range(0, 40): plt.subplot(5, 8, i+1) plt.tight_layout() pixels = x_train[i,:,:,:] plt.title(to_label(y_train[i]), fontsize=8) fig = plt.imshow(pixels) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False)# One-Hot Vectorに変換y_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)# モデルの構築model = Sequential()model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))model.add(Activation('relu'))model.add(Conv2D(32, (3, 3)))model.add(Activation('relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Conv2D(64, (3, 3), padding='same'))model.add(Activation('relu'))model.add(Conv2D(64, (3, 3)))model.add(Activation('relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(512))model.add(Activation('relu'))model.add(Dropout(0.5))model.add(Dense(num_classes))model.add(Activation('softmax'))# モデルのコンパイルmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])model.summary()# データの正規化x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255x_test /= 255# モデルの訓練history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)# 訓練したモデルの保存if not os.path.isdir(save_dir): os.makedirs(save_dir)model_path = os.path.join(save_dir, model_name)model.save(model_path)print('Saved trained model at %s ' % model_path)# モデルの評価scores = model.evaluate(x_test, y_test, verbose=0)print('Test loss:', scores[0])print('Test accuracy:', scores[1])# モデルの訓練仮定の可視化loss = history.history['loss']val_loss = history.history['val_loss']nb_epoch = len(loss)plt.plot(range(nb_epoch), loss, marker='.', label='loss')plt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')plt.legend(loc='best', fontsize=10)plt.grid()plt.xlabel('epoch')plt.ylabel('loss')plt.show() 14. 実行結果分類対象の画像です。CIFAR-10では32x32のサイズの画像を10種類に分類します。 モデルの要約です。畳み込み層、プーリング層、ドロップアウト層、活性化層(relu)を利用した典型的なCNNになっています。最後の出力には全結合層と活性化層(SoftMax)を利用しています。 12345678910111213141516171819202122232425262728293031323334353637383940414243Model: \"sequential\"_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________activation (Activation) (None, 32, 32, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 30, 30, 32) 9248 _________________________________________________________________activation_1 (Activation) (None, 30, 30, 32) 0 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 15, 15, 32) 0 _________________________________________________________________dropout (Dropout) (None, 15, 15, 32) 0 _________________________________________________________________conv2d_2 (Conv2D) (None, 15, 15, 64) 18496 _________________________________________________________________activation_2 (Activation) (None, 15, 15, 64) 0 _________________________________________________________________conv2d_3 (Conv2D) (None, 13, 13, 64) 36928 _________________________________________________________________activation_3 (Activation) (None, 13, 13, 64) 0 _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64) 0 _________________________________________________________________dropout_1 (Dropout) (None, 6, 6, 64) 0 _________________________________________________________________flatten (Flatten) (None, 2304) 0 _________________________________________________________________dense (Dense) (None, 512) 1180160 _________________________________________________________________activation_4 (Activation) (None, 512) 0 _________________________________________________________________dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________dense_1 (Dense) (None, 10) 5130 _________________________________________________________________activation_5 (Activation) (None, 10) 0 =================================================================Total params: 1,250,858Trainable params: 1,250,858Non-trainable params: 0 訓練経過です。だいたい1エポック9秒程度で終わっています。また検証データの正解率(val_accuracy)も77%程度になっています。ちなみに下記の結果はGPUを使用したものですが、CPUの場合は1エポックで43秒程かかりました。GPUは偉大です・・・ 12345678910111213141516171819202122232425262728293031Train on 50000 samples, validate on 10000 samplesEpoch 1/1550000/50000 [==============================] - 10s 199us/sample - loss: 1.5545 - accuracy: 0.4320 - val_loss: 1.1938 - val_accuracy: 0.5681Epoch 2/1550000/50000 [==============================] - 9s 177us/sample - loss: 1.1665 - accuracy: 0.5840 - val_loss: 0.9679 - val_accuracy: 0.6565Epoch 3/1550000/50000 [==============================] - 8s 162us/sample - loss: 1.0074 - accuracy: 0.6419 - val_loss: 0.8711 - val_accuracy: 0.6919Epoch 4/1550000/50000 [==============================] - 8s 163us/sample - loss: 0.9178 - accuracy: 0.6771 - val_loss: 0.8321 - val_accuracy: 0.7100Epoch 5/1550000/50000 [==============================] - 9s 187us/sample - loss: 0.8527 - accuracy: 0.6999 - val_loss: 0.7774 - val_accuracy: 0.7309Epoch 6/1550000/50000 [==============================] - 9s 186us/sample - loss: 0.8061 - accuracy: 0.7175 - val_loss: 0.7574 - val_accuracy: 0.7349Epoch 7/1550000/50000 [==============================] - 9s 173us/sample - loss: 0.7724 - accuracy: 0.7287 - val_loss: 0.7334 - val_accuracy: 0.7482Epoch 8/1550000/50000 [==============================] - 8s 168us/sample - loss: 0.7343 - accuracy: 0.7417 - val_loss: 0.7231 - val_accuracy: 0.7535Epoch 9/1550000/50000 [==============================] - 9s 185us/sample - loss: 0.7121 - accuracy: 0.7495 - val_loss: 0.7061 - val_accuracy: 0.7586Epoch 10/1550000/50000 [==============================] - 8s 166us/sample - loss: 0.6878 - accuracy: 0.7577 - val_loss: 0.6602 - val_accuracy: 0.7716Epoch 11/1550000/50000 [==============================] - 8s 162us/sample - loss: 0.6672 - accuracy: 0.7675 - val_loss: 0.6990 - val_accuracy: 0.7614Epoch 12/1550000/50000 [==============================] - 8s 163us/sample - loss: 0.6462 - accuracy: 0.7729 - val_loss: 0.6837 - val_accuracy: 0.7645Epoch 13/1550000/50000 [==============================] - 8s 169us/sample - loss: 0.6322 - accuracy: 0.7777 - val_loss: 0.6593 - val_accuracy: 0.7787Epoch 14/1550000/50000 [==============================] - 9s 172us/sample - loss: 0.6197 - accuracy: 0.7826 - val_loss: 0.6672 - val_accuracy: 0.7761Epoch 15/1550000/50000 [==============================] - 8s 159us/sample - loss: 0.6035 - accuracy: 0.7887 - val_loss: 0.6811 - val_accuracy: 0.7682 損失の経過を表したグラフです。見ての通り10エポック以降から過学習をおこしています。 15. まとめ本記事ではLinuxでGPUを用いた機械学習環境を構築する手順を紹介しました。特に初学者から中級者レベルの方が自宅に機械学習環境をシンプルに構築したいシチュエーションを想定して以下の環境構築を一気通貫で実施しました。 Minikube(Kubernetes) JupyterLab TensorFlow Keras CNN(Convolutional Neural Network) 今回なぜLinux+Minikube+JupyterLabの構成にしたかというと、まず機械学習の環境は依存関係が複雑な上に開発のスピードが非常に速いという問題があるからです。特にGPUのドライバのバージョンとCUDAとcuDNNとフレームワーク(TensorFlow等)のバージョンの関係は非常にセンシティブなため、コンテナとして管理した方が非常に安心してバージョンアップができます。特にMinikube(Kubernetes)で管理すれば一つ前のデプロイメントに戻すのも簡単なので環境構築の試行錯誤とノウハウの蓄積も簡単になります。そして、本記事ではMinikubeで構築しましたが、複数マシンのKubernetesクラスタで構築すれば複数人でも利用可能な機械学習基盤になり、MLOpsにも繋がっています。 またJupyterLabはいわゆるノートブックの環境で機械学習を環境としては、試行錯誤が容易でコーディングと結果の可視化が両立されており非常に使い勝手が良いのでおすすめです。ノートブックに関しては以前に「全プログラマに捧ぐ！図解「ノートブック」」という記事を書いたのでそちらを参照してください。 本記事は自分が一番最初にLinuxで機械学習環境を構築しようとした時に、多くの手順に苛まれてなかなかお目当てのディープラーニングまで辿り着けなくてもどかしい思いをした経験から、環境構築からディープラーニングまで一気通貫で記事を構成してみました。 駆け足での説明になってしまいましたが、機械学習に興味がある方の一助になれば幸いです。 16. 参考文献 Minikubeを使ってローカルにkubernetes環境を構築 - Qiita tensorflow2.0 + kerasでGPUメモリの使用量を抑える方法 - Qiita Keras+CNNでCIFAR-10の画像分類 CIFAR-10のデータセットを用いてCNNの画像認識を行ってみる - AI人工知能テクノロジー 17. おまけDockerfileはGitで管理しておくと便利です。また、Dockerfileのビルドからデプロイおよびコミットまで自動化しておくと間違いがありません。以下はRubyスクリプトですが自分が使っているものです。ポイントはsedでkubernetes.io/change-causeを書換えていることです。こうすることでデプロイメントを更新可能にしています。イメージのバージョンは大きな変更をした場合だけ上げるようにして、ちょっとした変更(機械学習用のライブラリ追加等)はDockerfileをちょっと修正してこのスクリプトを実行するだけで環境のアップデートが終わるので非常に楽です。 12345678910111213#!/usr/bin/env ruby$VERSION = \"v1.13\"system(\"docker build -t ml/all:#{$VERSION} .\") || raise('Failed to docker build')msg = \"\\\"modified at #{Time.now}\\\"\"cmd = \"sed -i -e 's/\\\\(kubernetes.io\\\\/change-cause: \\\\).*$/\\\\1#{msg}/' ml-deploy.yml\"system(cmd) || raise('Failed to sed')system(\"kubectl apply -f ml-deploy.yml\") || raise('Failed to sed')system(\"git add ml-deploy.yml\") || raise('Failed to add')system(\"git add Dockerfile\") || raise('Failed to add')system(\"git commit -m 'modify ml/all image'\") || raise('Failed to commit')1.ただし新しすぎるとドライバがなかったり、バグがあったりするのでリサーチは十分行ってください。 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/10/20/minikube_tensorflow_mnist/"},{"title":"🎉🎉祝Ruby2.7リリース🎉🎉 クリスマスなのでRubyの22年に渡るコミットの歴史を可視化してみた","text":"この記事はRuby Advent Calendar 2019の25日目の記事です。 本日はクリスマスということで、例年ならRubyの新バージョンがリリースされる日になります。新バージョンのRuby 2.7はRC2までやってきたので、リリースに向けて着実に進んでいるようです。🎉🎉 そして無事に本日リリースされました!! おめでとうございます!!! 🎉🎉(本記事投稿時点ではまだリリースはされていません。) そこでRuby2.7のリリースのお祝いとコミッターのみなさんのハードワークに感謝の気持ちを込めて、Rubyの22年に渡るコミットの歴史を可視化してみたいと思います。 目次1. はじめに2. コミットログを収集する2.1. Rubyのリポジトリを取得する2.2. Rubyのリポジトリを覗いてみる2.3. 作者別にコミット数をカウントしてみる2.4. 年単位でコミット数をカウントしてみる3. データの前処理のための基盤を整える3.1. 環境構築方法を選択する3.2. Dockerfileの準備3.3. JupyterLabの起動画面4. JupyterLabとRubyでデータの前処理を行ってみる4.1. Rubyコミッタの名寄せを行ってみる4.2. 最終的な集計テーブルを作成する5. Flourishで可視化してみる6. 完成した可視化7. まとめ 1. はじめに一番最初の動機はコミッターのみなさんが日々どれだけのコミットを積み重ねているのかを過去から遡って見てみたいというものでした。しかしRubyの誕生は1993年と言われており、27年の開発の歴史の中で関わっているコミッターの数は200人を超えるので単純な棒グラフや線グラフでは可視化が破綻するのは目に見えていました。 そこで「Flourish」というサービスを使い、時間軸を加えた棒グラフのアニメーションを利用することで、常にトップ20のコミッターの様子を捉えられるようにします。 完成イメージは上記のようになります。上の画像を2019年までアニメーションさせるための作業を行うことが本記事の趣旨になります。また、せっかくなのでなるべくRubyを使ってこの作業を行ってみたいと思います。 それでは行ってみましょう。 2. コミットログを収集する何はともあれ、コミットログを収集しないと可視化ができません。そこでRubyのリポジトリを取得することから始めたいと思います。 2.1. Rubyのリポジトリを取得するまずはGitHubのRubyリポジトリクローンしてきます。 1git clone https://github.com/ruby/ruby.git ただし、上記のリポジトリのページにはタイトルに以下のように[mirror]と付いています。 The Ruby Programming Language [mirror] これはどういうことかというと、実はRubyは正式なRubyのGitリポジトリはGitHubとは別のGitリポジトリで管理されています。また、それ以前にRubyの開発は2019年4月22日までSVNリポジトリで管理されており、一部のブランチはまだそちらで開発が続いているという事実もあります1。 その辺の経緯は「令和時代のRubyコア開発」に書いてありました。歴史の長いプロダクトはバージョン管理システムを変えるのに大きな労力を伴うという一例だと思います。 話は逸れますがこのURLにメールアドレスが載っていないと2020年1月1日以降pushができなくなるみたいなのでコミッタの方はお気を付けください。 2.2. Rubyのリポジトリを覗いてみるリポジトリのクローン後にやることと言えば、一番最初のコミットと一番最後のコミットを見ることだと思います。まずは最後のコミットをgit logコマンドで見てみます。 12345commit 16fddfe352828d26aaa6cdbce696e62de04511ce (HEAD -> master, origin/trunk, origin/master, origin/HEAD)Author: Marcus Stollsteimer Date: Mon Dec 23 15:02:59 2019 +0100 [DOC] Improve readability of requirements for 最後のコミットは12/23に行われています。次に一番最初のコミットを見てみます。git logコマンドに--reverseオプションをつけることで先頭からコミットを見ることができます。 1git log --reverse 一番最初のコミットは1998/1/16に行われたようです。ログにby cvs2svnとあるのでバージョン管理システムをCVSからSubversionに移行するためにcvs2svnコマンドを用いたようです。 12345678910111213141516commit 392296c12de9d7f9be03a8205250ba0844cb9d38Author: (no author) Date: Fri Jan 16 12:13:05 1998 +0000 New repository initialized by cvs2svn. git-svn-id: svn+ssh://ci.ruby-lang.org/ruby/trunk@1 b2dd03c8-39d4-4d8f-98ff-823fe69b080ecommit 3db12e8b236ac8f88db8eb4690d10e4a3b8dbcd4 (tag: v1_0_r2)Author: matz Date: Fri Jan 16 12:13:05 1998 +0000 Initial revision git-svn-id: svn+ssh://ci.ruby-lang.org/ruby/trunk@2 b2dd03c8-39d4-4d8f-98ff-823fe69b080e また、コミットログにgit-svn-idが残っているので、このリポジトリはSubversion時代にgit-svnコマンドを用いてGitHubと同期されていたことも分かります。このへんの経緯をまとめたものがないかなとネットを検索したらるびま0052号に書かれていました。Gitに移行した現在の状況も加味すると以下のようになります。 年代 バージョン管理システム 1993〜1998/1 RCS/tarボール? 1998/1〜2006/12 CVS 2006/12 〜 2019/4 Suvbversion 2019/4 〜 現在 Git コミットログに残っているのはCVSで管理された1998年以降なので、可視化できるのはこの約22年間分のコミットになります。残念ながらRuby誕生から約5年間の歴史は可視化できないことをご了承ください。 2.3. 作者別にコミット数をカウントしてみる次に年単位で作者別にコミット数をカウントしてみます。原理上はコミットログさえあればコミットの日付とコミットの作者とコミット数が分かるので、最初はコミットログを自力でパースしてカウントしようと思っていましたが、git shortlogという便利なコマンドがあることに気づきました。以下のコマンドで1998年から2019年までの作者別のコミット数を見ることができます。 1git shortlog -sne --no-merges --since='1998-01-01' --until='2019-12-31' オプションは以下のとおりです。 オプション 説明 -n 作者ごとのコミット数でソート -s コミット数の概要のみ表示 -e Eメールアドレスを表示 --no-merges マージコミットを除外 −−since 開始日時 --until 終了日時 トップ10は以下のとおりです。１位はご覧の通りnobuさんで圧巻の1万6千コミット。2位にトリプルスコア以上の差をつけて圧倒的な戦闘力を誇っています。 1234567891016566 nobu 4746 akr 4338 svn 2728 naruse 2562 matz 2357 ko1 2050 usa 1414 eban 1176 Nobuyoshi Nakada 1168 kazu ただ上記のコミット数の表示には大きな問題があります。分かる人には分かると思うのですが、実は1位のnobuさんと9位のNobuyoshi Nakadaさんは同一人物です。これは作者名やEメールアドレスが異なると異なるものとしてカウントされてしまうためです。 この問題を解決するためには名寄せといって同一人物と思われるコミットを集約しないといけません。実はgit shortlogには.mailmapという集約の仕組みがあるのですが、これを利用するためにはそもそも同一人物のEメールアドレスがどれかという情報を持っている必要があります。 今回はどのEメールアドレスが同一人物かを推測するところから始めるので.mailmapの仕組みは利用せずRubyを用いて集約を頑張ってみたいと思います。 2.4. 年単位でコミット数をカウントしてみる前節で1998年から2019年までのコミット数を集計しましたが、年単位で可視化を行いためスクリプトで年ごとのコミット数のログを作成します。作成するログは２種類あって1998年からの累積のコミット数を年単位で集計するtotalログと各年のコミット数を集計するtrendログです。以下のRubyスクリプトでは1998年から2019年までループでsystem関数でgit shortlogを呼び出してリダイレクトで各年ごとのコミット数のログファイルを作成しています。 1234(1998..2019).each do |e| system(\"git shortlog -sne --no-merges --since='#{e}-01-01' --until='#{e}-12-31' > trend/#{e}.log\") system(\"git shortlog -sne --no-merges --since='1998-01-01' --until='#{e}-12-31' > total/#{e}.log\")end totalログを見れば総合的に活躍したコミッターの変化を可視化でき、trendログを可視化すればその年に活躍したコミッターを可視化することができます。 3. データの前処理のための基盤を整えるさて、年単位のコミット数を取得してログに出力したので次に行うべきは、データの可視化を行うFlourishが読み込めるデータ形式にログを変換することです。このような処理は一般的に「前処理」と呼ばれます。前述のリポジトリからログを抽出する処理と前処理とデータ登録の作業を合わせてETL(Extract/Transrom/Load)処理と呼ばれることも多いです。 前処理のツールとして最もよく利用されているのはExcelだと思われます。データサイエンティストの方ならJupyter NotebookとPythonの組み合わせが多いかもしれません。その他にも専用のETLツールは数多く存在します。しかし今回はなるべくRubyを使って作業を行うという趣旨なので、JupyterLab(Jupyter Notebookの後継)とRubyを利用して前処理を行ってみたいと思います。 3.1. 環境構築方法を選択するJupyterLabとRubyの環境を構築するには以下のような様々な方法があります。どれも一長一短ありますが、今回はDockerを使って構築してみます。 ローカルに環境を構築する Anacondaを使って手っ取り早く構築できる ローカル環境が汚れる VM上に環境を構築する Anacondaを使って手っ取り早く構築できる ローカル環境は汚れないがOSインストールからなので手間がかかる コンテナを使って環境を構築する 既存のコンテナイメージをベースに手っ取り早く構築できる Docker環境の構築とDockerfileの準備が必要 クラウド上のマネージド・サービスを利用する クラウド上のノートブックを使って気軽に始められる 環境の自由度が低い 3.2. Dockerfileの準備以下が作成したDockerfileです。ベースにしたコンテナイメージは「14言語をぶち込んだJupyter LabのDockerイメージを作ってみた」で公開されていたベースイメージを元にRubyだけを残して、JupyterLabの設定や拡張を入れたり、必要なRubyGemsを入れたものになります。 Dockerfileのポイントは「install Ruby」のコメントで始まる一連の処理になります。ここでRubyをruby-buildでビルドして、JupyterLabからRubyを選択して起動できるようにRubyカーネルをgemでインストールしています。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546FROM hero/jupyter-langs:pythonRUN apt-get update && apt-get install -y curl vimRUN conda install -c conda-forge nodejs# install RubyENV RUBY_VERSION=2.6.5 \\ RUBY_HOME=/opt/rubyRUN git clone https://github.com/rbenv/ruby-build.git \\ && PREFIX=/usr/local ./ruby-build/install.sh \\ && mkdir -p ${RUBY_HOME} \\ && ruby-build ${RUBY_VERSION} ${RUBY_HOME}/${RUBY_VERSION}ENV PATH=${RUBY_HOME}/${RUBY_VERSION}/bin:$PATHRUN gem install --no-document \\ benchmark_driver \\ cztop \\ iruby \\ && iruby register --force# copy JupyterLab SettingsRUN mkdir -p /root/.jupyter/lab/user-settingsCOPY user-settings/ /root/.jupyter/lab/user-settings/# install favorite jupyter lab extensionsRUN jupyter labextension install @lckr/jupyterlab_variableinspectorRUN jupyter labextension install @jupyterlab/tocRUN jupyter labextension install @jupyterlab/gitRUN pip install jupyterlab-gitRUN jupyter serverextension enable --py jupyterlab_gitRUN jupyter labextension install jupyterlab-drawio# for JupyterLab TerminalENV SHELL /bin/bashRUN echo \"alias ls='ls --color=auto'\" >> /root/.bashrcRUN echo \"export PATH=/root/anaconda3/bin:$PATH\" >> /root/.bashrcRUN echo \"export PS1='\\u:\\W# '\" >> /root/.bashrc# install favorite gemsRUN gem install nokogiriRUN gem install daruRUN gem install daru-viewRUN gem install --pre pycallRUN gem install --pre matplotlibRUN gem install numpyRUN gem install pandas 見ての通りRubyは2.6.5を利用しています。2.7.0-rc2も試してみたのですが、うまく動作しなかったので断念しました。DockerファイルはGitHubにpushしてあるのでご利用ください。 3.3. JupyterLabの起動画面以下が実際の起動画面になります。テーマは自分の趣味でダークにしてあります。 4. JupyterLabとRubyでデータの前処理を行ってみる基本的にはPyCallとpandasを用いて作業します。PyCallやRubyからPythonを呼び出せるライブラリで、pandasはPythonで主にデータフレームを扱うためのライブラリです。 データフレームを用いるとExcelのように表形式のデータが扱いやすくなります。 4.1. Rubyコミッタの名寄せを行ってみるここからの作業はノートブックを用いて作業しますが、ノートブックを直接表示はできないので抜粋して説明します。 まずは、ライブラリを読み込みます。またPyCallのよく使う変数はショートカットを定義しておくと便利です。以下ではPythonの組み込み関数はPyCall::builtinsに定義されているのでpybltに格納しています。 123456789require 'open-uri'require 'pycall/import'include PyCall::Importpyimport :pandas, as: :pdpyimport :numpy, as: :nppyblt = PyCall::builtinsDict = PyCall::DictList = PyCall::List 次にコミッターログ(前述のgit shortlogで作成した1998年から2019年までの作者別コミット数)を読み込んでデータフレームを作成します。 read_committers_log関数はコミッターログをパースして、コミット数と作者とEメールアドレスに分割してデータフレームを作成しています。 12345678910def read_committers_log(file) committers = File.read(file).split(\"\\n\").map do |e| commits, id = e.split(\"\\t\") user, addr = id.split(\"","link":"/cats-cats-cats/2019/12/25/visualize_ruby_development/"},{"title":"【続】Rubyの22年に渡るコミットの歴史を可視化してみた(ファイル編)","text":"この記事はRuby Advent Calendar 2019の19日目の記事です。(19日目が投稿されなかったので代理投稿します。) 目次1. はじめに2. ファイル一覧を作成する3. ファイル単位のコミット回数のテーブルを作成する4. Flourishで可視化してみる5. 完成した可視化6. まとめ 1. はじめに本記事は19日目の代理投稿ですが、25日を過ぎて書かれています。そして25日目の続きです。25日目では作者単位で可視化を行いましたが今回はファイル単位で行ってみたいと思います。 Rubyの開発の主なファイルはトップディレクトリ直下に置かれているので、それらのファイルを対象に集計してみたいと思います。 2. ファイル一覧を作成するまずは22年間の間に存在したファイル一覧を作成したいと思います。これは以下のようなワンライナーでできました。 1git log --pretty=\"format:\" --name-only --since=\"23 years ago\"|grep -v '/'|grep -v -e '^\\.' -e '^\\s*$'|sort|uniq > ruby_topdir_files.txt 簡単に説明すると--pretty=\"format:\"の部分でコミットログの表示の余計な部分を消します。--name--onlyでファイル名だけを表示します。これによって（余計な空行は入りますが）コミットログに現れる全ファイルが取得できました。 次に余計な行の削除です。まずトップディレクトリのみ対象にしたいのでディレクトリのセパレータの/が現れている行は除外します。grep -v '/'の-vは除外オプションです。さらにgrep -v -e '^\\.' -e '^\\s*$'でドットファイルと空行を取り除きます。-eオプションを用いることで正規表現を用いてファイルを指定できます。 最後にソート(sort)して重複行の削除(uniq)をすれば、過去22年間で現れた全ファイル一覧(ruby_topdir_files.txt)が作成できます。 できたファイルは以下になります。 ファイル名一覧はこちら。BSDLCONTRIBUTING.mdCOPYINGCOPYING.LIBCOPYING.jaCOPYING.jpChangeLogDoxyfile.inGPLKNOWNBUGS.rbLEGALLGPLMANIFESTMakefile.inNEWSREADMEREADME.EXTREADME.EXT.jaREADME.EXT.jpREADME.jaREADME.ja.mdREADME.jpREADME.mdSECURITY.mdToDoacinclude.m4aclocal.m4addr2line.caddr2line.happveyor.ymlarray.cascii.cast.cast.rbatomic.hazure-pipelines.ymlbignum.cblockinlining.cbuiltin.cbuiltin.hcall_cfunc.cichange.log1class.ccommon.mkcompar.ccompile.ccompile.hcomplex.cconfig.djconfig.guessconfig.subconfig_h.djconfig_s.djconfigureconfigure.acconfigure.batconfigure.inconstant.hcont.ccrypt.hdebug.cdebug.hdebug_counter.cdebug_counter.hdefines.hdiffdir.cdistruby.rbdln.cdln.hdln_find.cdmydln.cdmyenc.cdmyencoding.cdmyext.cdmyloadpath.cdmytranscode.cdmyversion.cencindex.hencoding.cenum.cenumerator.cenv.herror.ceuc_jp.ceval.ceval_error.ceval_error.cieval_error.heval_intern.heval_jump.ceval_jump.cieval_jump.heval_load.ceval_method.ceval_method.cieval_method.heval_proc.ceval_safe.ceval_safe.cieval_safe.heval_thread.cfiber_benchmark.rbfile.cfnmatch.cfnmatch.hgc.cgc.hgc.rbgem_prelude.rbglob.cgolf_prelude.rbgoruby.chash.chrtime.hia64.Sia64.sid.cid.hid_table.cid_table.himp.loginits.cinsn_send.ciinsnhelper.ciinsnhelper.hinsns.definstall-shinstruby.rbintern.hinternal.hio.cio.hio.rbiseq.ciseq.hkeywordslex.clex.c.bltlex.c.srcload.cloadpath.clocaleinit.cm17n.cm17n.hmain.cmarshal.cmath.cmdoc2man.rbmethod.hmini_builtin.cminiinit.cmissing.hmjit.cmjit.hmjit_compile.cmjit_internal.hmjit_worker.cmkconfig.rbnode.cnode.hnumeric.cobject.coniggnu.honiguruma.hopt_insn_unif.defopt_operand.defpack.cpack.rbparse.cparse.yprec.cprelude.rbprobes.dprobes_helper.hproc.cprocess.crandom.crange.crational.cre.cre.hregcomp.cregenc.cregenc.hregerror.cregex.cregex.hregexec.creggnu.cregint.hregparse.cregparse.hregsyntax.cruby-runner.cruby.1ruby.cruby.hruby_assert.hruby_atomic.hrubyio.hrubysig.hrubystub.crubytest.rbrunruby.rbsafe.csig.hsignal.csiphash.csiphash.hsizes.csjis.csparc.csprintf.cst.cst.hstrftime.cstring.cstruct.csymbol.csymbol.ht.rbtest_knownbug.rbtest_symbol.rbthread.cthread_native.hthread_pthread.cthread_pthread.cithread_pthread.hthread_sync.cthread_tools.cthread_win32.cthread_win32.cithread_win32.htime.ctimev.htop.sedtrace_point.rbtranscode.ctranscode_data.htranscode_data_iso_8859.ctranscode_data_japanese.ctranscode_data_one_byte.ctransient_heap.ctransient_heap.hunicode.cutf8.cutil.cutil.hvariable.cvariable.hversion.cversion.hvm.cvm.hvm_args.cvm_backtrace.cvm_core.hvm_debug.hvm_dump.cvm_eval.cvm_evalbody.cvm_evalbody.civm_exec.cvm_exec.hvm_insnhelper.cvm_insnhelper.hvm_macro.defvm_method.cvm_opts.hvm_opts.h.basevm_trace.cvsnprintf.cwarning.rbwercker.ymlyarv.hyarv_version.hyarvcore.cyarvcore.h 3. ファイル単位のコミット回数のテーブルを作成するコードを見て察してください（笑）。今回はカテゴリに拡張子を選択しています。 make_files_activity.rb123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env rubyrequire 'csv'span = (1998..2019)header = [\"file\", \"ext\", *span] # ヘッダーファイルの作成table = [header]# ファイル一覧の読み込みfiles = File.open('ruby_topdir_files.txt').readlines.map{|e| e.chomp}activity = files.map do |file| puts file commits_history = span.map do |year| # ファイル単位でコミットログを絞って行数を`wc-l`で数える commits = `git log --since=1998-01-01 --until=#{year}-12-31 --oneline --no-merges #{file} 2>/dev/null | wc -l`.to_i end # 拡張子の列を作る（トップ30に現れるものだけを表示） ext = File.extname(file) ext = case ext when '.h','.c','.y', '.mk', '.def' then ext when '' then 'no_ext' end [file, ext, *commits_history]endtable.push *activity# CSVファイルに書き込みCSV.open('ruby_files_activity.csv','w') do |csv| table.each do |line| csv < line { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/12/29/visualize_ruby_development_by_file/"},{"title":"Scalaプログラマが圏論を学ぶためのオススメ文献 - 3選","text":"圏論は数学の一分野です。これを学ぶのには「数学書」を手に取るのが王道なのですが、残念ながらこれは大部分のプログラマに理解できる言葉では書かれていません。「定義・命題・証明」の積み重ねで書かれています1。ここで大半のScalaプログラマは苦い顔をすると思います。もう少し分かりやすいScalaプログラマ向けの圏論入門がないかと探してみると「Haskell」向けの記事が大量に引っかかるでしょう。ここで多くのScalaプログラマは心を折られてしまいます。「圏論」はまだScalaプログラマには早すぎたんだと・・・ 本記事ではそんなあなたに贈る3つの文献をご紹介したいと思います。 目次1. はじめに2. プログラマが圏論を学ぶべき理由3. オンラインで読めるオススメ文献 - 3選3.1. Category Theory for Programmers Scala Edition3.2. Scala with Cats3.3. 猫番4. プログラマが圏論で学んでおいたほうがよい概念5. まとめ6. もっと圏論を学びたい人向けのオンラインで読めるオススメ資料 1. はじめに本記事は圏論に興味があるScalaプログラマを対象にしています。特にプログラマにとって実用的な圏論の知識をScalaを通して身につけたい方にオススメします。 2. プログラマが圏論を学ぶべき理由プログラマが圏論を学ぶべき理由に関しては圏論入門レベルの自分があまり大きなことは言えないので、「Category Theory for Programmers Scala Edition」の序文から３つの文章を引用しようと思います。 First, category theory is a treasure trove of extremely useful programming ideas. Category Theory for Programmers Scala Edition (意訳) 最初に、圏論はめちゃくちゃ役立つプログラミングのアイデアの宝庫です。 I would go as far as to argue that category theory is the kind of math that is particularly well suited for the minds of programmers.That’s because category theory — rather than dealing with particulars — deals with structure. It deals with the kind of structure that makes programs composable. Category Theory for Programmers Scala Edition (意訳) 圏論はプログラマの心理に特に適した数学の分野であるといっても過言ではないと思います。それは圏論が個々の詳細よりもむしろ構造を扱うからです。圏論はプログラムを合成可能にする特定の構造を扱います。 Composition is at the very root of category theory — it’s part of the definition of the category itself. And I will argue strongly that composition is the essence of programming. Category Theory for Programmers Scala Edition (意訳) 合成は圏論の重要な根幹を成しており、圏自体の定義の一部でもあります。そして合成はプログラミングの本質であると、はっきり述べておこうと思います。 3. オンラインで読めるオススメ文献 - 3選本記事は「Scalaプログラマに適した圏論の文献紹介」という趣旨なので、以下の条件に当てはまる文献を３つに絞って紹介したいと思います。 圏論の概念(特にモノイド、関手、モナド)に触れている Scalaで解説がしてある 定理の証明が極力載っていない オンラインで無料で読める 3.1. Category Theory for Programmers Scala Editionまずは前述した「Category Theory for Programmers Scala Edition」を紹介します。 Category Theory for Programmers: The Preface | Bartosz Milewski's Programming Cafe 元のHaskellで書かれたページ Category Theory for Programmers Scala Edition Scalaエディションのダウンロードページ 上記のページから「category-theory-for-programmers-scala.pdf」をダウンロード この本はもともとHaskell(と若干のC++)で書かれていた例に、後でScalaの例を付け加えたものになっています。この本の特徴のひとつは豊富な図解とスニペットです。これは具体的に引用して見てもらった方が早いと思います。以下は10章の自然変換で使われている4つの図になります。 番号と赤い矢印は自分が書き足したものです。一般の数学書による自然変換の説明だと最後の4番目の図のしかも右側の可換図式しか描かれていない場合がほとんどだと思います。しかし本書では1番目の図で同じ圏に移す2つの関手FとGが示されて図式が犬と豚に変換されているイメージが描かれています。2番目の図では、自然変換が対象を移すことを示し、3番目の図で対象だけでなく射も移すことを示しています。そして3番目を簡略化したものが4番目の図になることが分かります2。 このように本書では圏論の概念が豊富な図によって解説されています。またソースコードもHaskellとScalaのコードが一緒に載っていて非常にわかりやすいです。以下の引用はListの長さを示すlength関数が自然変換であることの説明に使われているコードです。 length関数は一般的にはList[A] => Intの関数でList関手をIntに変換するものですが、Intを定値関手であるConst[E, A]に埋め込まれたConst[Int, A]と見做すことでlengthを関手間の変換、つまり自然変換になることを示しています。本書では関手間のパラメトリックな多相関数は常に自然変換になることを述べています。 上記のように本書では一貫してHaskellのコードが青、Scalaのコードが赤で示されており非常にわかりやすくなっています。一般的な圏論のプログラミングへの応用ではHaskellを例に出されることが多いので、このように併記してある文献はHaskellを学びたいScalaプログラマにとっても嬉しいと思われます。 最後に本書の内容と構成に関してですが、自分は圏論を学びたいプログラマにとっては非常に秀逸だと思いました。少なくとも数学にはあまり自身がないけどプログラミングに圏論を活かしたいプログラマにとっては必要な概念はほぼ本書で触れられていると思います。 以下に本書の目次(一部抜粋)を載せておきます3。また、自分の日本語訳とプログラマとして読んだ方がいい章を5段階評価で★印を付けています。本書を読む参考にしてください。 目次(日本語訳付き) Preface (序文) ★★★★★ 1 Category: The Essence of Composition (圏: 合成の本質) ★★★★★ 1.1 Arrows as Functions (関数としての矢) 1.2 Properties of Composition (合成の性質) 1.3 Composition is the Essence of Programming (合成はプログラミングの本質) 2 Types and Functions (型と関数) ★★★★ 2.1 Who Needs Types? (なぜ型が必要なのか?) 2.2 Types Are About Composability (型は合成可能性に関係する) 2.3 What Are Types? (型とは何か？) 2.4 Why Do We Need a Mathematical Model? (なぜ数学モデルが必要なのか？) 2.5 Pure and Dirty Functions (純粋な関数と汚れた関数) 2.6 Examples of Types (型の例) 3 Categories Great and Small (圏 大から小まで) ★★ 3.1 No Objects (対象なし) 3.2 Simple Graphs (単純なグラフ) 3.3 Orders (順序) 3.4 Monoid as Set (集合としてのモノイド) 3.5 Monoid as Category (圏としてのモノイド) 4 Kleisli Categories (クライスリ圏) ★★★ 4.1 The Writer Category (Writerの圏) 4.2 Writer in Haskell (HaskellにおけるWriter) 4.3 Kleisli Categories (クライスリ圏) 5 Products and Coproducts (積と余積) ★★★ 5.1 Initial Object (始対象) 5.2 Terminal Object (終対象) 5.3 Duality (双対) 5.4 Isomorphisms (同型) 5.5 Products (積) 5.6 Coproduct (余積) 5.7 Asymmetry (非対称) 6 Simple Algebraic Data Types (単純な代数的データ型) ★★★ 6.1 Product Types (積型) 6.2 Records (レコード型) 6.3 Sum Types (和型) 6.4 Algebra of Types (型の代数) 7 Functors (関手) ★★★★ 7.1 Functors in Programming (プログラミングにおける関手) 7.1.1 The Maybe Functor (Maybe関手) 7.1.2 Equational Reasoning (等式的推論) 7.1.3 Optional (Optional) 7.1.4 Typeclasses (型クラス) 7.1.5 Functor in C++ (C++における関手) 7.1.6 The List Functor (List関手) 7.1.7 The Reader Functor (Reader関手) 7.2 Functors as Containers (コンテナとしての関手) 7.3 Functor Composition (関手の合成) 8 Functoriality (関手っぽいもの) ★★ 8.1 Bifunctors (双関手) 8.2 Product and Coproduct Bifunctors (積と余積双関手) 8.3 Functorial Algebraic Data Types (関手的代数的データ型) 8.4 Functors in C++ (C++における関手) 8.5 The Writer Functor (Writer関手) 8.6 Covariant and Contravariant Functors (共変と反変関手) 8.7 Profunctors (プロ関手) 8.8 The Hom-Functor (ホム関手) 9 Function Types (関数型) ★★★★ 9.1 Universal Construction (普遍的構成) 9.2 Currying (カリー化) 9.3 Exponentials (冪) 9.4 Cartesian Closed Categories (デカルト閉圏/カルテシアン閉圏) 9.5 Exponentials and Algebraic Data Types (冪と代数的データ型) 9.5.1 Zeroth Power (0乗) 9.5.2 Powers of One (1の冪乗) 9.5.3 First Power (1乗) 9.5.4 Exponentials of Sums (和の指数) 9.5.5 Exponentials of Exponentials (冪の指数) 9.5.6 Exponentials over Products (積の指数) 9.6 Curry-Howard Isomorphism (カリー・ハワード同型) 10 Natural Transformations (自然変換) ★★★ 10.1 Polymorphic Functions (多相関数) 10.2 Beyond Naturality (自然性を超えて) 10.3 Functor Category (関手圏) 10.4 2-Categories (2圏) 11 Declarative Programming (宣言的プログラミング) ★★ 12 Limits and Colimits (極限と余極限) ★★ 12.1 Limit as a Natural Isomorphism (極限と自然同型) 12.2 Examples of Limits (極限の例) 12.3 Colimits (余極限) 12.4 Continuity (連続性) 13 Free Monoids (自由モナド) ★★★ 13.1 Free Monoid in Haskell (Haskellにおける自由モノイド) 13.2 Free Monoid Universal Construction (自由モノイドの普遍的構成) 14 Representable Functors (表現可能関手) ★★ 14.1 The Hom Functor (ホム関手) 14.2 Representable Functors (表現可能関手) 15 The Yoneda Lemma (米田の補題) ★★ 15.1 Yoneda in Haskell (Haskellにおける米田) 15.2 Co-Yoneda (余米田) 16 Yoneda Embedding (米田埋め込み) ★★ 16.1 The Embedding (埋め込み) 16.2 Application to Haskell (Haskellへの応用) 16.3 Preorder Example (前順序の例) 16.4 Naturality (自然性) 17 It’s All About Morphisms (結局は射が全て) ★★★ 17.1 Functors (関手) 17.2 Commuting Diagrams (可換図式) 17.3 Natural Transformations (自然変換) 17.4 Natural Isomorphisms (自然同型) 17.5 Hom-Sets (ホム集合) 17.6 Hom-Set Isomorphisms (ホム集合同型) 17.7 Asymmetry of Hom-Sets (ホム集合の非対称) 18 Adjunctions (随伴) ★★ 18.1 Adjunction and Unit/Counit Pair (随伴と単位/余単位) 18.2 Adjunctions and Hom-Sets (随伴とホム集合) 18.3 Product from Adjunction (随伴から積へ) 18.4 Exponential from Adjunction (随伴から冪へ) 19 Free/Forgetful Adjunctions (自由/忘却随伴) ★★ 20 Monads: Programmer’s Definition (モナド: プログラマーの定義) ★★★★ 20.1 The Kleisli Category (クライスリ圏) 20.2 Fish Anatomy (魚の解剖学) 20.3 The do Notation (do記法) 21 Monads and Effects (モナドと作用) ★★★ 21.1 The Problem (問題) 21.2 The Solution (解決策) 21.2.1 Partiality (部分性) 21.2.2 Nondeterminism (非決定性) 21.2.3 Read-Only State (読み取りのみの状態) 21.2.4 Write-Only State (書き取りのみの状態) 21.2.5 State (状態) 21.2.6 Exceptions (例外) 21.2.7 Continuations (継続) 21.2.8 Interactive Input (対話型の入力) 21.2.9 Interactive Output (対話型の出力) 22 Monads Categorically (圏論的なモナド) ★★ 22.1 Monoidal Categories (モノイダル圏) 22.2 Monoid in a Monoidal Category (モノイダル圏におけるモノイド) 22.3 Monads as Monoids (モノイドとしてのモナド) 22.4 Monads from Adjunctions (随伴としてのモナド) 23 Comonads (余モナド) ★ 23.1 Programming with Comonads (余モナドでプログラミング) 23.2 The Product Comonad (積余モナド) 23.3 Dissecting the Composition (合成の解剖) 23.4 The Stream Comonad (ストリーム余モナド) 23.5 Comonad Categorically (圏論的な余モナド) 23.6 The Store Comonad (ストア余モナド) 24 F-Algebras (F代数) ★★★ 24.1 Recursion (再帰) 24.2 Category of F-Algebras (F代数の圏) 24.3 Natural Numbers (自然数) 24.4 Catamorphisms (カタモーフィズム) 24.5 Folds (畳み込み) 24.6 Coalgebras (余代数) 25 Algebras for Monads (モナドの代数) ★★ 25.1 T-algebras (T代数) 25.2 The Kleisli Category (クライスリ圏) 25.3 Coalgebras for Comonads (余モナドの余代数) 25.4 Lenses (レンズ) 26 Ends and Coends (エンドと余エンド) ★ 26.1 Dinatural Transformations (対角化自然変換) 26.2 Ends (エンド) 26.3 Ends as Equalizers (等価子としてのエンド) 26.4 Natural Transformations as Ends (エンドとしての自然変換) 26.5 Coends (余エンド) 26.6 Ninja Yoneda Lemma (忍者米田の補題) 26.7 Profunctor Composition (プロ関手の合成) 27 Kan Extensions (カン拡張) ★ 27.1 Right Kan Extension (右カン拡張) 27.2 Kan Extension as Adjunction (随伴としてのカン拡張) 27.3 Left Kan Extension (左カン拡張) 27.4 Kan Extensions as Ends (エンドとしてのカン拡張) 27.5 Kan Extensions in Haskell (Haskellにおけるカン拡張) 27.6 Free Functor (自由関手) 28 Enriched Categories (豊穣圏) ★ 28.1 Why Monoidal Category? (なぜ豊穣圏なのか？) 28.2 Monoidal Category (モノイダル圏) 28.3 Enriched Category (豊穣圏) 28.4 Preorders (前順序) 28.5 Metric Spaces (距離空間) 28.6 Enriched Functors (豊穣関手) 28.7 Self Enrichment (自己豊穣化) 28.8 Relation to 𝟐-Categories (2圏との関係) 29 Topoi (トポス) ★ 29.1 Subobject Classifier (部分対象分類子) 29.2 Topos (トポス) 29.3 Topoi and Logic (トポスと論理) 30 Lawvere Theories (ローヴェア理論) ★ 30.1 Universal Algebra (普遍代数) 30.2 Lawvere Theories (ローヴェア理論) 30.3 Models of Lawvere Theories (ローヴェア理論のモデル) 30.4 The Theory of Monoids (モノイドの理論) 30.5 Lawvere Theories and Monads (ローヴェア理論とモナド) 30.6 Monads as Coends (余エンドとしてのモナド) 30.7 Lawvere Theory of Side Effects(副作用のローヴェア理論) 31 Monads, Monoids, and Categories (モナドとモノイドと圏) ★★ 31.1 Bicategories (双圏) 31.2 Monads (モナド) 3.2. Scala with Cats次に紹介したいのは「Scala with Cats」です4。CatsはScalaで関数型プログラミングをサポートするためのライブラリで、主に型クラスを提供しています。この型クラスにはモナド(Monad)や関手(Functor)も含まれており、圏論をプログラミングに応用する上で重要な役割を果たしています。 Scala with Cats この本の特色は「型チャート」が豊富に載っていることです。Scalaの型は圏論においては対象や関手やモナドだったり様々ですが、それらの変換の様子が図に表されているので非常に分かりやすくなっています。以下の引用は反変関手の型チャートになります。 本書の構成で秀逸なのは、型クラスの説明に留まらず「Case Study(事例)」と「Solution(答え)」が載っていることです。Case Studyには、具体的のどのようなケースで型クラスを使えばいいかが載っています。「Solution」には、各章に豊富に散りばめられた「Excercise」の答えが載っています。従って本書を読むことで圏論の一部を「実務」でも応用できるようになると思います。 以下に本書の目次(一部抜粋)を載せておきます5。また、自分の日本語訳付けていますが・・・途中で力尽きました。本書を読む参考にしてください。 目次(日本語訳付き) 1 Introduction (はじめに) 1.1 Anatomy of a Type Class (型クラスの解剖学) 1.1.1 The Type Class (型クラス) 1.1.2 Type Class Instances (型クラスインスタンス) 1.1.3 Type Class Interfaces (型クラスインタフェース) 1.2 Working with Implicits (暗黙と働く) 1.2.1 Packaging Implicits (暗黙のパッケージ) 1.2.2 Implicit Scope (暗黙のスコープ) 1.2.3 Recursive Implicit Resolution (再帰的な暗黙の解決) 1.3 Exercise: Printable Library (練習: 印字可能ライブラリ) 1.4 Meet Cats (Catsとの邂逅) 1.4.1 Importing Type Classes (型クラスのインポート) 1.4.2 Importing Default Instances (デフォルトインスタンスのインポート) 1.4.3 Importing Interface Syntax (インタフェース構文のインポート) 1.4.4 Importing All The Things! (全てをインポート!) 1.4.5 Defining Custom Instances (カスタムインスタンスを定義する) 1.5 Example: Eq (例: Eq) 1.5.1 Equality, Liberty, and Fraternity (等値性、自由、友愛) 1.5.2 Comparing Ints (Intの比較) 1.5.3 Comparing Options (Optionの比較) 1.5.4 Comparing Custom Types (カスタム型の比較) 1.6 Controlling Instance Selection (インスタンス選択の制御) 1.6.1 Variance (変位) 2 Monoids and Semigroups (モノイドと半群) 2.1 Definition of a Monoid (モノイドの定義) 2.2 Definition of a Semigroup (半群の定義) 2.3 Exercise: The Truth About Monoids (モナドの真実) 2.4 Exercise: All Set for Monoids\u001d (モノイドの全ての集合) 2.5 Monoids in Cats (Catsにおけるモノイド) 2.5.1 The Monoid Type Class (モノイド型クラス) 2.5.2 Monoid Instances (モノイドインタンス) 2.5.3 Monoid Syntax (モノイド構文) 2.6 Applications of Monoids (モノイドの応用) 2.6.1 Big Data (ビッグデータ) 2.6.2 Distributed Systems (分散システム) 2.6.3 Monoids in the Small (小さな世界におけるモノイド) 3 Functors (関手) 3.1 Examples of Functors (関手の例) 3.2 More Examples of Functors (関手のさらなる例) 3.3 Definition of a Functor (関手の定義) 3.4 Aside: Higher Kinds and Type Constructors (寄り道: 高カインドと型コンストラクタ) 3.5 Functors in Cats (Catsにおける関手) 3.5.1 The Functor Type Class (関手型クラス) 3.5.2 Functor Syntax (関手構文) 3.5.3 Instances for Custom Types (カスタム型のインスタンス) 3.6 Contravariant and Invariant Functors (反変・不変関手) 3.6.1 Contravariant Functors and the contramap Method (反変関手とcontramapメソッド) 3.6.2 Invariant functors and the imap method(不変関手とimap) 3.7 Contravariant and Invariant in Cats (CatsにおけるContravariantとInvariant) 3.7.1 Contravariant in Cats (CatsにおけるContravariant) 3.7.2 Invariant in Cats (CatsにおけるInvariant) 3.8 Aside: Partial Unification (寄り道: 部分的ユニフィケーション) 3.8.1 Unifying Type Constructors (型コンストラクタの結合) 3.8.2 Left-to-Right Elimination (左から右への削除) 4 Monads (モナド) 4.1 What is a Monad? (モナドとは何か) 4.1.1 Definition of a Monad (モナドの定義) 4.2 Monads in Cats (Catsにおけるモナド) 4.2.1 The Monad Type Class(モナド型クラス) 4.2.2 Default Instances(デフォルトインスタンス) 4.2.3 Monad Syntax(モナド構文) 4.3 The Identity Monad(恒等モナド) 4.4 Either (Either) 4.4.1 Left and Right Bias (左右バイアス) 4.4.2 Creating Instances (インスタンスの作成) 4.4.3 Transforming Eithers (Eitherへの変換) 4.4.4 Error Handling (エラーハンドリング) 4.5 Aside: Error Handling and MonadError (寄り道: エラーハンドリングとMonadError) 4.5.1 The MonadError Type Class (MonadError型クラス) 4.5.2 Raising and Handling Errors (エラーの投げ方とハンドリングの仕方) 4.5.3 Instances of MonadError (MonadErrorのインスタンス) 4.6 The Eval Monad (Evalモナド) 4.6.1 Eager, Lazy, Memoized, Oh My! (熱心、怠惰、メモ化、オッ!) 4.6.2 Eval’s Models of Evaluation (Evalの評価モデル) 4.6.3 Eval as a Monad (モナドとしてのEval) 4.6.4 Trampolining and Eval.defer (トランポリンとEval.defer) 4.7 The Writer Monad (Writerモナド) 4.7.1 Creating and Unpacking Writers (Writerの作成と開封) 4.7.2 Composing and Transforming Writers ((Writerの合成と変換)) 4.8 The Reader Monad (Readerモナド) 4.8.1 Creating and Unpacking Readers (Readerの作成と開封) 4.8.2 Composing Readers (Readerの合成) 4.8.3 Exercise: Hacking on Readers (練習: Readerでハッキング) 4.8.4 When to Use Readers? (いつReaderを使うか？) 4.9 The State Monad (Stateモナド) 4.9.1 Creating and Unpacking State (Stateの作成と開封) 4.9.2 Composing and Transforming State (Stateの合成と変換) 4.10 Defining Custom Monads (カスタムモナドの定義) 5 Monad Transformers (モナド変換子) 5.1 Exercise: Composing Monads (練習: モナドの合成) 5.2 A Transformative Example (変換的な例) 5.3 Monad Transformers in Cats (Catsにおけるモナド変換子) 5.3.1 The Monad Transformer Classes (モナド変換子クラス) 5.3.2 Building Monad Stacks (モナドスタックの構築) 5.3.3 Constructing and Unpacking Instances (インスタンスの構成と開封) 5.3.4 Default Instances (デフォルトインスタンス) 5.3.5 Usage Patterns (利用パターン) 6 Semigroupal and Applicative (半群とアプリカティブ) 6.1 Semigroupal (Semigroupal) 6.1.1 Joining Two Contexts (2つのコンテキストの結合) 6.1.2 Joining Three or More Contexts (3つ以上のコンテキストの結合) 6.2 Apply Syntax (Apply構文) 6.2.1 Fancy Functors and Apply Syntax (面白いFunctorとApply構文) 6.3 Semigroupal Applied to Different Types (Semigroupalの異なる型への適用) 6.3.1 Semigroupal Applied to Monads (Semigroupalのモナドへの適用) 6.4 Validated (Validated) 6.4.1 Creating Instances of Validated (Validatedのインスタンス作成) 6.4.2 Combining Instances of Validated (Validatedのインスタンス結合) 6.4.3 Methods of Validated (Validatedのメソッド) 6.5 Apply and Applicative (ApplyとApplicative) 6.5.1 The Hierarchy of Sequencing Type Classes (列型クラスの階層) 7 Foldable and Traverse (FoldableとTraverse) 7.1 Foldable (Foldable) 7.1.1 Folds and Folding 7.1.2 Exercise: Reflecting on Folds 7.1.3 Exercise: Scaf-fold-ing Other Methods 7.1.4 Foldable in Cats 7.2 Traverse 7.2.1 Traversing with Futures 7.2.2 Traversing with Applicatives 7.2.3 Traverse in Cats 8 Case Study: Testing Asynchronous Code (事例: 非同期コードのテスト) 8.1 Abstracting over Type Constructors 8.2 Abstracting over Monads 9 Case Study: Map-Reduce (事例: Map-Reduce) 9.1 Parallelizing map and fold 9.2 Implementing foldMap 9.3 Parallelising foldMap 9.3.1 Futures, Thread Pools, and Execu􀦞onContexts 9.3.2 Dividing Work 9.3.3 Implementing parallelFoldMap 9.3.4 parallelFoldMap with more Cats 10 Case Study: Data Validation (事例: データバリデーション) 10.1 Sketching the Library Structure 10.2 The Check Datatype 10.3 Basic Combinators 10.4 Transforming Data 10.4.1 Predicates 10.4.2 Checks 10.5 Kleislis 11 Case Study: CRDTs (事例: CRDT) 11.1 Eventual Consistency 11.2 The GCounter 11.2.1 Simple Counters 11.2.2 GCounters 11.3 Generalisation 11.3.1 Implementation 11.4 Abstracting GCounter to a Type Class 11.5 Abstracting a Key Value Store 3.3. 猫番最後に紹介したいのが「猫番」です。紹介する中では唯一の日本語で読める文献です。現在は「O日目」から「17日目」まで公開されており、著者が「Cats」を使って理解していく過程が記録されています。後半はより「圏論」の説明に移っています。 猫番 「猫番」は前二つの文献と比べ非常に自由に書かれていて、独特な構成になっています。ただそれが不思議と読みにくいという訳でもなく、著者と一緒に「Cats」や「圏論」を旅をしている気分になれるところがこの文献の面白いところです。もっと気楽に圏論に触れてみたい人や圏論の雰囲気を味わってみたい方はこの文献から読むといいかもしれません。 4. プログラマが圏論で学んでおいたほうがよい概念とりあえず「Category Theory for Programmers Scala Edition」に出てきた概念の中で、プログラマが学んでおいた方が良いと思うものを以下に分類してみました6。これはあくまで数学が苦手な圏論入門者である自分の私見です。 必ず学んでおきたい 圏、関手、自然変換 集合の圏(Set)、圏の圏(Cat)、関手圏 半群、モノイド モナド、クライスリ圏 普遍的構成（普遍性） できれば学んでおきたい 積、余積 同型 双対 冪、デカルト閉圏 モノイダル圏 自由モノイド、自由モナド F代数、T代数 余力があれば学んでおきたい ホム関手、表現可能関手 米田の補題、米田埋め込み 極限と余極限 随伴 カリー＝ハワード同型 興味があれば学んだ方が良い エンド カン拡張 ← 全ての概念 豊穣圏 トポス ローヴェア理論 圏論は非常に多くの概念が出てくるので無理せず少しずつ消化していくのが良いと思われます。自分が圏論に興味を持ち始めたのは「モナド」に出会ってからでした。以下の言葉の意味を知りたくて圏論を始めたのがきっかけです。 モナドは単なる自己関手の圏におけるモノイド対象だよ。何か問題でも？ フィリップ・ワドラー この言葉の意味は恐らく「必ず学んでおきたい」まで理解できればなんとなく意味が理解できるようになると思われます。さらに圏論にはパワーワード「全ての概念はKan拡張である」7があって、いつか理解できればいいなと思っています。 5. まとめ本記事ではプログラマがなぜ圏論を学ぶべきかを説明し、Scalaプログラマが圏論を学ぶ上で有用な以下の３つの文献を紹介しました。 Category Theory for Programmers Scala Edition Scala with Cats 猫番 本記事がScalaで圏論を学んでみたい方の一助になれば幸いです。 6. もっと圏論を学びたい人向けのオンラインで読めるオススメ資料残念ながら本記事の趣旨には合いませんでしたが、プログラマが圏論を学ぶ上でぜひオススメしたい資料です。 プログラマーのための圏論 説明がHaskellベースですが非常に丁寧で分かりやすいです 上、中、下 物理学者のための圏論入門 物理学者ではなくても圏論の基本的な概念を理解できる非常にオススメの資料です 特に普遍射の説明が秀逸で、会社組織に擬えての説明がツボりました 圏論によるプログラミングと論理 灘校パソコン研究部の部誌(2013年)に掲載されていたものです 普通に書店に並んでいてもおかしくないボリュームとクオリティです 圏論だけでなく数学やコンピュータサイエンスの基礎も補完しています 圏論 | 壱大整域 圏論の概念を本気で理解したくなったらここに駆け込んでください ただしストイックな数学スタイルで書かれているのでプログラマには少し辛いかもしれません1.ときどき証明をつけずに「簡単(自明)なので証明は読者に委ねる」というパワーワードが記載されています。もちろんプログラマにとって「簡単」ではありません・・・ ↩2.実際には3番と4番の図の間に逆射を持っていた場合の図が挟まっています。 ↩3.掲載している目次は、「Conclusion」、「Challenges」、「Bibliography」等の見出しは削っております。これは本書の概要を知る手がかりにはならないと考えたからです。正しい目次は直接文献をご確認ください。 ↩4.この本はもともと「Scalaz」という別のライブラリ向けに書かれていたものが、Cats向けに書き直されたものです。書き直された当初は「Advanced Scala with Cats」という名前で有償でしたが、無償化されるにあたって「Scala with Cats」という名称に変更され可愛らしい猫の表紙が付きました。 ↩5.掲載している目次は、「Summary」、「Excercise」、「Solution」等の見出しは削っております。これは本書の概要を知る手がかりにはならないと考えたからです。正しい目次は直接文献をご確認ください。 ↩6.ここで列挙する概念は一般的な圏論に登場する概念から選択しています。プログラミングの文脈で登場する代数的データ型や型クラスは含まれていません。 ↩7.参考文献: 全ての概念はKan拡張である、とは何か - algebraic dialy | 壱大整域 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/cats-cats-cats/2019/07/20/learning-category-theory/"}],"tags":[{"name":"Git","slug":"Git","link":"/cats-cats-cats/tags/Git/"},{"name":"Keyboard","slug":"Keyboard","link":"/cats-cats-cats/tags/Keyboard/"},{"name":"Keymap","slug":"Keymap","link":"/cats-cats-cats/tags/Keymap/"},{"name":"Hexo","slug":"Hexo","link":"/cats-cats-cats/tags/Hexo/"},{"name":"Web","slug":"Web","link":"/cats-cats-cats/tags/Web/"},{"name":"npm","slug":"npm","link":"/cats-cats-cats/tags/npm/"},{"name":"oEmbed","slug":"oEmbed","link":"/cats-cats-cats/tags/oEmbed/"},{"name":"html5","slug":"html5","link":"/cats-cats-cats/tags/html5/"},{"name":"Book","slug":"Book","link":"/cats-cats-cats/tags/Book/"},{"name":"Event","slug":"Event","link":"/cats-cats-cats/tags/Event/"},{"name":"Qiita","slug":"Qiita","link":"/cats-cats-cats/tags/Qiita/"},{"name":"Container","slug":"Container","link":"/cats-cats-cats/tags/Container/"},{"name":"k8s","slug":"k8s","link":"/cats-cats-cats/tags/k8s/"},{"name":"AdventCalendar","slug":"AdventCalendar","link":"/cats-cats-cats/tags/AdventCalendar/"},{"name":"ErgoDash","slug":"ErgoDash","link":"/cats-cats-cats/tags/ErgoDash/"},{"name":"GCP","slug":"GCP","link":"/cats-cats-cats/tags/GCP/"},{"name":"Go","slug":"Go","link":"/cats-cats-cats/tags/Go/"},{"name":"Ruby","slug":"Ruby","link":"/cats-cats-cats/tags/Ruby/"},{"name":"Compiler","slug":"Compiler","link":"/cats-cats-cats/tags/Compiler/"},{"name":"Interpreter","slug":"Interpreter","link":"/cats-cats-cats/tags/Interpreter/"},{"name":"OGP","slug":"OGP","link":"/cats-cats-cats/tags/OGP/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/cats-cats-cats/tags/MachineLearning/"},{"name":"Scala","slug":"Scala","link":"/cats-cats-cats/tags/Scala/"},{"name":"Dotty","slug":"Dotty","link":"/cats-cats-cats/tags/Dotty/"},{"name":"Jupyter","slug":"Jupyter","link":"/cats-cats-cats/tags/Jupyter/"},{"name":"Mind","slug":"Mind","link":"/cats-cats-cats/tags/Mind/"},{"name":"Algorithm","slug":"Algorithm","link":"/cats-cats-cats/tags/Algorithm/"},{"name":"gRPC","slug":"gRPC","link":"/cats-cats-cats/tags/gRPC/"},{"name":"MakerFaire","slug":"MakerFaire","link":"/cats-cats-cats/tags/MakerFaire/"},{"name":"Blog","slug":"Blog","link":"/cats-cats-cats/tags/Blog/"},{"name":"Exam","slug":"Exam","link":"/cats-cats-cats/tags/Exam/"},{"name":"ReView","slug":"ReView","link":"/cats-cats-cats/tags/ReView/"},{"name":"AWS","slug":"AWS","link":"/cats-cats-cats/tags/AWS/"},{"name":"pulumi","slug":"pulumi","link":"/cats-cats-cats/tags/pulumi/"},{"name":"IaC","slug":"IaC","link":"/cats-cats-cats/tags/IaC/"},{"name":"Serverless","slug":"Serverless","link":"/cats-cats-cats/tags/Serverless/"},{"name":"Azure","slug":"Azure","link":"/cats-cats-cats/tags/Azure/"},{"name":"Minikube","slug":"Minikube","link":"/cats-cats-cats/tags/Minikube/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/cats-cats-cats/tags/TensorFlow/"},{"name":"Keras","slug":"Keras","link":"/cats-cats-cats/tags/Keras/"},{"name":"MLOps","slug":"MLOps","link":"/cats-cats-cats/tags/MLOps/"},{"name":"Flourish","slug":"Flourish","link":"/cats-cats-cats/tags/Flourish/"},{"name":"CategoryTheory","slug":"CategoryTheory","link":"/cats-cats-cats/tags/CategoryTheory/"},{"name":"Cats","slug":"Cats","link":"/cats-cats-cats/tags/Cats/"}],"categories":[{"name":"Tech","slug":"Tech","link":"/cats-cats-cats/categories/Tech/"},{"name":"DIY","slug":"DIY","link":"/cats-cats-cats/categories/DIY/"},{"name":"Web","slug":"Tech/Web","link":"/cats-cats-cats/categories/Tech/Web/"},{"name":"Miscellaneous","slug":"Tech/Miscellaneous","link":"/cats-cats-cats/categories/Tech/Miscellaneous/"},{"name":"Event","slug":"Tech/Event","link":"/cats-cats-cats/categories/Tech/Event/"},{"name":"Keyboard","slug":"DIY/Keyboard","link":"/cats-cats-cats/categories/DIY/Keyboard/"},{"name":"CloudNative","slug":"Tech/CloudNative","link":"/cats-cats-cats/categories/Tech/CloudNative/"},{"name":"Language","slug":"Tech/Language","link":"/cats-cats-cats/categories/Tech/Language/"},{"name":"ComputerScience","slug":"Tech/ComputerScience","link":"/cats-cats-cats/categories/Tech/ComputerScience/"},{"name":"MachineLearning","slug":"Tech/MachineLearning","link":"/cats-cats-cats/categories/Tech/MachineLearning/"},{"name":"Book","slug":"Tech/Book","link":"/cats-cats-cats/categories/Tech/Book/"}]}